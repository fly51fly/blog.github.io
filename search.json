[{"title":"科学思维的能力是否是智慧的核心本质？","path":"/2023/08/06/cargo-cult-ai/","content":"原文：Cargo Cult AI 摘要：文章探讨了人工智能（AI）与真正的科学思维之间的差异。讨论了科学思维作为智能的核心要素，以及人类倾向于轻易相信与科学不符的神奇和幻想。作者指出类似GPT-4这样的大型语言模型被认为是早期版本的AGI（人工通用智能），但这种观点令人不安。现代AI模型虽然可以提问并回答问题，但无法复制人类科学思维，尤其是在扩展可验证知识边界方面的能力。对于物理学等领域，神经网络模型相对于牛顿的万有引力理论在普适性上存在局限性。 文章还指出，人们在使用AI进行因果推断时需要谨慎，因为神经网络在识别数据相关性方面非常擅长，但却不擅长建立因果关系。在医学诊断等领域，AI在建立因果关系方面应用已经越来越多，但这也会导致类似于人类货物崇拜（Cargo Cult）的情况出现。 作者认为，AI和AGI研究提供了了解人类思维这一未解决科学问题的机会，但目前的AI模型在理解人类思维方面还有很多工作要做。文章最后表示对AI和AGI的未来持乐观态度，并指出需要采用新的算法方法来超越纯经验推理的界限，以实现更强大的AGI。 总之，该文探讨了AI在科学思维和因果推断方面的局限性，以及对未来AI和AGI发展的展望。 启发： 科学思维与AI：尽管AI在许多任务上表现出色，但它是否真正拥有科学的思维方式仍然是一个问题。真正的科学思维需要对假设进行严格的调查和验证，而不是仅仅基于数据进行预测。 AI的局限性：大型语言模型如GPT-4虽然在许多任务上表现出色，但它们缺乏真正的科学探究的能力。这意味着，尽管它们可以生成看似合理的答案，但这些答案可能缺乏真正的科学依据。 人与AI的互动：当我们依赖AI为我们提供答案时，我们需要意识到它的局限性。AI的输出应该被视为一个可能有用的相关指标，而不是作为因果关系的决定性证据。","tags":["Research"],"categories":["文摘"]},{"title":"AI不会取代人类，但使用AI的人将取代不使用AI的人","path":"/2023/08/06/ai-wont-replace-humans-but-humans-with-ai-will-replace-humans-without-ai/","content":"原文：AI Won’t Replace Humans — But Humans With AI Will Replace Humans Without AI 内容要点： 与互联网大大降低了信息传输的成本一样，AI将降低认知的成本。 虽然很多人担心AI会取代人类的工作，但真正的趋势是，拥有AI技能的人类将会取代那些没有AI技能的人类。这意味着AI不是一个威胁，而是一个工具，可以增强人类的能力。 AI不仅仅是为技术工作者设计的，所有员工都应该了解和利用AI。这意味着企业需要为所有员工提供AI培训和资源，而不仅仅是技术团队。 AI的应用不仅仅是在高技术领域，它可以应用在任何需要思考的地方，AI的潜在应用是无限的。","tags":["Research"],"categories":["文摘"]},{"title":"创新研究的原则和启示","path":"/2023/08/01/quick-thoughts-on-research/","content":"原文：Quick thoughts on research 创新研究的原则和启示引言 作者分享了自己在理论物理、计算机科学、思维工具和元科学领域的经验，提出创新研究工作的原则和启示。 强调分享研究思考对整个研究社区的益处。 警惕普适原则 研究原则不是普适法则，因人而异，因情况而异。 需要一系列灵活的启示，适应不同的情况和变化。 在事情进展顺利时，继续保持，而在陷入困境时，尝试其他启示。 坚持和决心是好研究者的重要品质，但同时要具备灵活和重新调整方向的能力。 了解自己的工作方式 理解自己的独特工作方式，可能与他人不同。 伟大的研究者通常有非传统的工作方法。 警惕建议 建议可能有价值，也可能不适合个人情况。 尊重对自己有共鸣和激励的建议，但如果不适用，不要灰心。 避免被不适合自己的建议束缚。 动力(Momentum) 动力(Momentum)在研究中至关重要。 保持动力，持续自我激励。 合作就像是相互指教 最佳合作往往涉及相互指教，帮助彼此成长。 从不寻常的个体中学习可能比从技术上有价值的知识更有价值。 寻找比你年轻和年长的导师 向比自己年轻和年长得多的人学习。 学会与不同年龄段的人交流，这是一种竞争优势。 赋能和激励 以赋能的态度接触他人，培养这种能力。 培养这种能力(a power of enablement)是一种技巧。 研究前沿的动态性 研究前沿可能会突然转移到其他领域，如波前比粒子移动得更快。 不同的人会以不同的方式应对这种变化。 避免追逐潮流，更好的方式是寻找自己独特的研究方向。 所做问题的重要性 研究问题的重要性有很大影响，专注于发现重要问题。 选择在丰饶的领域工作，并培养对基础研究的良好品味。 点滴收获 从经验丰富的研究者那里学到的无形知识非常有价值。 避免愤世嫉俗，避免疲劳，如果有必要，做出改变。 日常工作和动力 日常工作为深入洞察打下基础。 行动会带来动力，避免拖延。 了解和接纳自己 诚实地了解自己的动机，作出相应的工作。 勇气、想象力和决心通常比智力更重要。 教授和学习 教授能帮助加深对知识的理解。 通过努力提高教学水平可以学到更多。 尝试愚蠢的工作 进行愚蠢的实验，有时会带来重要发现。 寻找重要问题 选择有意义的项目，能够作出独特贡献的项目。 向有经验的研究者寻求指导，找到有意义的项目。 勇气和信念 勇气是做出重大研究的关键。 构建自己的信念，超越现有观念。 分享你的理解 与支持你的同行分享你的想法。 分享你的工作有助于完善和发展想法。","tags":["Research"],"categories":["文摘"]},{"title":"是什么让Stack Overflow日渐衰落？","path":"/2023/08/01/the-fall-of-stack-overflow-explained/","content":"原文：The Fall of Stack Overflow, Explained Stack Overflow 的衰落一篇名为《Stack Overflow的衰落》的文章迅速传播，用数据详细描述了 Stack Overflow 在过去一年半流量下降了35-50%。 1. 原因一：Google Analytics 的变化2022年5月，由于隐私法律的原因，Google Analytics 改变了 cookie 的存储方式，导致流量报告出现了15%的下降。实际上，Stack Overflow 并未失去50%的流量，而是约35%。 2. 原因二：Stack Overflow 对用户不友好Stack Overflow 是一个用于提问的网站，但令人惊讶的是，它却是网络上最具有毒性和敌对性的地方之一，表现出被动攻击的方式。十多年来，我们见证了成千上万的有关 Stack Overflow 敌对性的抱怨，因此，Stack Overflow 的敌对性和衰落并不是新鲜事。 3. 原因三：Google 搜索排名下降Stack Overflow 的搜索结果在 Google 上排名下降，不再总是第一，有时甚至不会出现在第一页。 4. 原因四：AI 的影响AI(人工智能)也对 Stack Overflow 的衰落产生了影响。ChatGPT 特别适用于编程，它的发布加速了 Stack Overflow 的衰落。然而，AI 的兴起并非完全是最主要的原因，Stack Overflow 的衰落由来已久。 总结Stack Overflow 的流量衰退由多个因素共同导致。Google Analytics 的变化、对用户不友好以及 Google 搜索排名下降是其流量下降的原因之一。同时，AI 的兴起也对其产生了影响，但这并非唯一原因。Stack Overflow 作为一个免费提供数据的平台，为人工智能的训练做出了贡献，但AI的快速发展可能会导致未来训练数据的减少。 AI在解决问题的速度、友好程度和跟进上具有优势，但它仍然是工具，而非替代品。Stack Overflow 可能会继续衰落，特别是在 Google 的 Search Labs 推出后。Stack Overflow 已经推出了 OverflowAI 作为对衰落的回应，AI 和 Stack Overflow 可能在未来相互竞争。 然而，Stack Overflow 仍然是一个宝贵的资源，它对于许多开发者来说仍然非常有用。AI 和 Stack Overflow 在不同场景下可能会互补，而不是替代彼此。开发者可以根据具体情况选择使用合适的工具来提高生产力和解决问题。","tags":["ChatGPT","StackOverflow"],"categories":["文摘"]},{"title":"ChatGPT时代怎么教编程","path":"/2023/07/24/teaching-programming-in-the-age-of-chatgpt/","content":"原文：Teaching Programming in the Age of ChatGPT 在ChatGPT时代教授编程：计算机教师的适应计划主要信息 ChatGPT等AI编程助手的出现引发了计算机编程教师的关注，他们面临如何调整教学策略的挑战。 短期计划：教师希望阻止学生作弊，避免依赖AI工具，采取一些应对措施，如限制工具使用、强化考试方式等。 长期计划（抵制AI工具）：教师担心学生可能不掌握编程基础，提出设计“AI-proof”作业和评估方法的想法。 长期计划（拥抱AI工具）：教师认为AI编程工具是未来的趋势，可在教学中利用这些工具来帮助学生更好地学习编程。 详细重点内容短期计划：阻止学生作弊 计算机教师普遍认为AI编程助手可能带来学生作弊的问题，并采取一些措施来阻止学生过度依赖这些工具。 教师担心学生不会深入思考问题，而只是依赖AI工具的答案。 一些教师采取限制AI工具使用、调整考试方式等短期措施来解决这个问题。 长期计划（抵制AI工具）：保障学生学习编程基础 一些教师认为使用AI工具可能导致学生不掌握编程基础，提出设计“AI-proof”作业和评估方法的想法。 通过使用定制库或增加本地文化和语言背景等方法，设计作业和评估以减少AI工具的影响。 教师认为这些方法不仅有助于保障学生学习编程基础，还能够让学生更深入地思考编程问题。 长期计划（拥抱AI工具）：帮学生为未来职业需求做准备 许多教师认为AI编程工具将成为程序员的标准工具，希望通过教学中利用这些工具来帮助学生为未来的职业需求做准备。 教师认为AI工具有助于学生更快地学习编程语法，并能够更深入地学习程序设计和工程。 教师还认为AI工具有助于提供个性化帮助，例如解释为什么某段代码出错等。 研究展望：有效、公平和道德地使用AI编程工具 这些发现是早期的研究成果，计算机教师对AI编程工具的使用还没有形成共识的最佳实践。 研究提出了一些关于如何开发、应用和评估AI编程工具的问题，包括学习者对AI生成代码的理解、AI工具的应用和评估等。 结论 计算机编程教师面临着调整教学策略的挑战，需要平衡抵制和拥抱AI编码工具的使用。 在长期计划中，教师需要关注学生学习编程基础的同时，也要充分利用AI工具提供的优势，为学生未来的职业需求做好准备。 进一步研究和探讨如何有效、公平和道德地使用AI编程工具对教育领域具有重要意义。","tags":["ChatGPT","Programming","Teaching"],"categories":["文摘"]},{"title":"让LLM总是用事实而不是虚构来回答问题","path":"/2023/07/24/teach-your-llm-vector-sql/","content":"原文：Teach your LLM to always answer with facts not fiction LLM幻觉与使用Vector SQL减少幻觉主要信息 LLM（Large Language Model）是一种高级AI系统，可以回答广泛范围的问题，但在陌生话题上可能出现幻觉现象。 幻觉是指在缺乏外部刺激的情况下，产生具有真实感知质量的感知错误。 增加外部知识可以减少LLM幻觉的出现。 使用Vector SQL可以实现精细粒度的向量搜索，从而提高LLM回答问题的准确性和效率。 详细重点内容LLM幻觉与外部知识 LLM并不是无懈可击的，可能在陌生话题上产生幻觉。为了减少幻觉，应加入外部知识，以指导LLM向准确和正确的回答迈进。 外部知识可以来自搜索引擎、数字图书馆等多个来源，并应与问题相关。 向量SQL与复杂搜索查询 向量SQL是一种强大的工具，用于构建复杂的搜索查询，它支持多种数据类型和函数。 向量SQL可以与SQL数据库集成，通过精细粒度的向量搜索提供准确的答案。 向量SQL的优势包括增加灵活性、提高效率、易于学习以及对LLM友好。 使用Vector SQL自动化整个过程 LLM可以学习从其数据源中查询数据，并使用SQL查询来自动化整个回答过程。 向量SQL的使用将为复杂搜索查询带来许多好处，并为LLM提供高效准确的回答。 MyScale和其他数据库解决方案 MyScale等数据库解决方案正在将向量搜索集成到其功能中，从而提供更强大的搜索和回答能力。 同时，越来越多的应用开发人员开始在他们的应用程序中使用向量搜索和SQL，以利用LLM的优势。 结论 LLM幻觉在现实中是普遍存在的。减少幻觉的最实用方法是在问题中添加外部知识，以实现高效准确的回答。 向量SQL是一种强大的工具，可以帮助构建复杂的搜索查询，提高LLM回答问题的准确性和效率。 通过使用MyScale等数据库解决方案，向量搜索正在成为越来越受欢迎的工具，为LLM系统提供更强大的搜索和回答能力。","tags":["LLM","SQL"],"categories":["文摘"]},{"title":"人和大模型之间最大的差距","path":"/2023/07/23/differences-between-human-and-ai/","content":"非常认同Andrej Kalpathy在“State of GPT”报告里分享的观点（以下为宝玉搬运字幕版，中文翻译质量很高），结合我的思考进一步阐述如下： 微软2003年Build大会演讲：如何训练和应用GPT 人和大模型之间最大的差距，在于思考过程和追求答案的方向。 人有内心独白，会调动经验，拆解问题，进行复杂的思考过程，追求合理、准确的答案，注重答案从内容到形式的“优雅”，即我们常说的”信·达·雅“。人类思维涉及更深层次的推理和判断，结合自身人格，做出对道德、情感和价值的独立考量。更会对答案进行反思和批判，从内外得到的反馈中，得到精神享受或改进经验。 相比之下，大模型（典型如LLM）没有内心独白，背靠超级广泛的事实知识，通过上下文关联调度工作记忆，追求全面综合已知语料达成的内容和形式上的最佳外推，或模仿。大模型的目标，往往在于以大量数据为基础，给出”不跑偏“的延拓，而非像人类一样进行复杂的思考过程。至于思维链，也是在上下文引导下对思考过程描述数据的模仿。最新的研究表明，大模型有一定的进行隐性”反思“的能力，即对于质量不高的回答，”心“里往往是”有数“的，但机制如何、是否鲁棒、如何利用尚无定论。 提示(Prompt)的出现，一定程度上弥补了这两种认知架构之间的差异。通过设计有效的提示，人们可以引导大模型的输出，使其更接近人类的思考方式和期望的答案。提示可以提供上下文、约束和指导，使大模型更好地理解问题，并生成更准确、相关和符合人类期望的回答。本质上，就是用心智模型，引导模仿过程，达成符合我们对”智能“预期的答案。 因此，提示的使用，在人与大模型之间架起了一座桥梁，帮助弥合了两者认知架构的差异。这使得大模型能够更好地满足人类需求，并在特定任务中展现出更高的准确性和可用性。","tags":["LLM","AI"],"categories":["随笔"]},{"title":"大模型参数规模越大越好吗？","path":"/2023/07/23/effects-of-more-params/","content":"先说结论：规模是把双刃剑，平衡点需具体权衡。 从大模型记忆论的观点来看，模型规模越大，参数越多，记忆容量越高，对整体数据分布的把握就越全面，可以增加模型在推理时的工作记忆，生成更具创新性、更多样的结果。但与此同时，随着熵增，高概率候选结果的多样化会呈指数级爆发，这就带来了一个挑战：如何在这些结果中进行优选，使得模型的输出与人类的价值观对齐。 从大模型压缩论的观点来看，大模型的目标是通过压缩世界知识来实现智能。压缩率越高，模型对核心规律的理解就越深入，因此并不需要过大的参数规模。过多的参数可能导致资源浪费，盲目扩容并不能带来更高的性能提升，这也符合“广记不如巧记”的直觉。 从大模型数据中心论的观点看，数据的质和量是决定模型能力的核心因素。更大的模型需要更多的数据来进行训练。目前，大多大模型还处在“半饥饿”状态，即它们无法得到足够多的优质数据来满足其训练需求。且世界上可用于训练大模型的优质数据已经接近极限，进一步获取优质数据，目前可见有两个来源：一是从相对质量不高的数据来源攫取数据，清洗、优化的成本巨大；二是靠大模型自己生成，且不说这种自激强化&#x2F;近亲演化过程对模型可能造成的负面影响，生成内容的合理性、事实性目前都是大问题，如何分辨和用好这些数据在未来很长一段时间都会是个待解的难题。 从模型效能的角度来看，过大的模型规模可能会导致能源浪费，实现效果和成本的平衡是一个重要考虑因素。在这种情况下，深度挖掘中小规模模型潜力，使用专家模型优化路由的方式，通过分布式集成提高总体能力可能是更优的选择。 总的来说，虽然大模型具有较高的处理和学习能力，但是我们也需要考虑到参数量、数据的质与量、模型压缩和效能等多方面的因素。这需要我们在实践中进行权衡，找到最优的解决方案。模型规模和参数量的平衡点并不是个固定的数值，甚至没有经验可以指导，需要根据具体的应用场景、数据环境、计算资源和目标进行动态权衡和调整。","tags":["LLM","AI"],"categories":["随笔"]},{"title":"医疗AI会取代人类医生吗？","path":"/2023/07/23/can-ai-displace-doctor/","content":"医疗除了技术问题以外，也许更重要的，会涉及对人类情感的理解和关怀。尽管AI能处理大量数据，但其在理解人类情绪和个体经历方面有限。这对处理如慢性病管理和心理健康问题等多元健康问题显得尤为重要。 每个病人都是独特的，需要全面、个性化和富有同情心的医疗服务，也就是个性化的诊疗服务。尽管AI能有效处理和分析大量信息，但其决策基于已有数据和已知规则，对新颖和独特的情况可能无法有效应对。 AI系统的构建需要包含更多多样化的经验、观点和专业知识。AI在识别模式和提供预测方面有巨大潜力，但也可能受到训练数据中固有偏见的影响，这可能导致不准确的诊断。 医疗决策不仅需要疾病和治疗的专业知识，还需要理解和考虑病人的需求和偏好，以及其他诸多因素。这一决策过程中的灵活性和人性化是AI难以实现的。 尽管AI在医学考试等基准测试上的表现已经超过了人类医生，但其在处理真实世界的复杂和模糊问题方面可能存在局限。然而，作为医生的辅助工具，AI具有巨大的潜力，可以帮助医生提高效率，减少错误。 在医疗领域，建立患者对医生的信任是至关重要的，这是AI可能需要投入更多时间和努力去实现的。此外，当出现医疗纠纷时，AI系统如何定责也是一个复杂且未解决的问题。 总而言之，现阶段的医疗AI更适合作为医生的辅助工具，而不是完全替代他们。它们可以帮助处理大量的数据和信息，提高医生的效率，尤其是在我国医疗资源下沉、社区化医疗的大背景下，但在理解和处理病人的全人性需求，以及在人性化的医疗决策方面，人类医生的作用仍然不可替代。","tags":["LLM","AI"],"categories":["随笔"]},{"title":"最重要的编程习惯","path":"/2023/07/23/healthy-coding-habits/","content":"原文：The Most Important Coding Habits 作者在康复期躺着写下这篇文章，因为腰椎间盘突出导致了滑脱症状。他通过自己的痛苦体会认识到，最重要的编程习惯并不是代码的可读性、一致性、组织结构等方面，而是那些让我们能在未来数十年继续享受编程乐趣的习惯。 文章提到了几个重要的编程习惯，以避免因长时间久坐在键盘前而导致的健康问题： 每日伸展：定期进行腹部和大腿肌肉的伸展运动，即使不是瑜伽爱好者，也可以在早晨或热水浴后进行，使肌肉更柔软，更有助于支撑身体。 定期休息：至少每隔一个小时起身走一走，或离开屏幕进行其他活动，不仅对身体有好处，而且对编程也有帮助。经常会有这样的情况，当你卡在一个问题上时，换下思路，回来重新尝试，往往可能会有新的灵感。 不要在深夜编程：避免熬夜编程，疲劳时编写的代码会质量较差，甚至有害，而且容易导致长时间低头弯腰，对身体造成不利影响。设定一个截止时间，坚持下来。 改善编程环境：优化编程环境，例如使用笔记本电脑支架和人体工学椅子，但即使有这些设备，仍然可能出现腰背疼痛。作者在文章中提到听说很多推荐站立式办公桌，现在也开始尝试。他认为，站立式办公桌不仅可以提高活动度，还能促进更多的休息，是一个双重受益的习惯。 总之，这篇文章提醒了程序员要重视健康，特别是在长期编程的过程中，不良的习惯可能导致身体健康问题。通过每日伸展、定期休息、避免深夜编程和改善编程环境等习惯，可以减少对身体的伤害，享受健康的编程生涯。","tags":["Programming"],"categories":["文摘"]},{"title":"用AI生成数据训练AI会有问题吗？","path":"/2023/07/22/use-ai-to-train-ai/","content":"让我们从一个不那么恰当的类比开始——一个人，通过读自己原创的书稿，能受到新的启发、获得能力的提升吗？不必急着回答，因为AI大模型的学习机制，和人类的学习机制，是截然不同的。但可以肯定，至少有一点人比AI靠谱——人不会因为看自己的作品失忆、变坏、变笨，目前的AI呢，还真不一定…… 首先，AI生成的数据良莠不齐，且难以分辨。以目前的大型语言模型(LLM)为例，在生成文本时，通常会在每个步骤中做出高概率候选结果的随机选择，即使在相同的上下文中，模型也可能生成不同的续写。这种随机性可以增加模型的创新性和多样性，但也可能导致质量的不稳定。虽然这些模型在处理大规模的文本数据方面表现出色，但它们并不能理解文本的含义。模型没有人类的常识、情感和道德观念，因此可能会生成不准确、不合逻辑或者不适当的内容。大语言模型的训练数据来自于网络，这意味着它们接触到的信息范围极广，包括高质量的事实表述、学术文章和低质量的网络评论及谣言甚至其它各种不良内容。如果模型在训练过程中接触到大量的低质量内容，那么它可能会学习到这些内容的不良风格和模式。如果能“去其糟粕”，将生成的数据清洗干净，那当然再好不过，可遗憾的是，尽管目前存在一些自动化的文本质量评估方法，用来评估生成文本的流畅性、一致性等，但这些方法可能无法全面评估生成文本的质量。例如，一个句子可能在语法上完全正确，但在语义上完全没有意义，或在道德层面消极甚至反社会、反人性。这使得区分AI生成的高质量和低质量文本变得非常困难。 用人工反馈强化学习(RLHF)能保证生成质量吗？RLHF是一种常用的策略，通过人工评估和反馈来调整AI模型的行为，以使其与人类价值观对齐，避免生成低质量内容。但这种方法通常需要进行大量的试错，以找到能够最大化奖励的策略。对于复杂的任务，如文本生成，这个过程可能非常复杂和耗时。RLHF虽然可以解决一些明显的问题，但可能无法从根本上解决质量问题。有些问题可能源于模型的基本架构或训练数据，通过微调很难根本解决。更进一步，在RLHF训练过程中，模型可能会忘记之前学到的一些知识，这被称为灾难性遗忘。例如，当模型在人工反馈强化学习过程中过度优化某一特定任务时，可能会忘记其他任务的知识。这可能导致模型在某些方面的能力退化。人工反馈强化学习需要大量的人工评估和反馈，这可能会消耗大量的人力和时间。而且，人的评估可能存在一定的主观性和不一致性，也可能会影响训练的效果。 综上所述，AI生成的数据质量难以保证。如果不加以区分，将包括高质量的结果和不合理、不符合逻辑，甚至反社会、反人性的结果在内的所有数据都用于训练，那么可能会对模型质量产生不利影响：模型可能会从不合理、不符合逻辑的数据中学习到不适当的规律，从而在未来的预测中产生错误的、甚至可能带来不良后果的输出；包含大量质量低下的训练样本，可能会降低整体模型的预测质量和准确性，高质量的数据可能被大量低质量数据淹没，导致模型的性能下降；如果训练数据中包含反社会、反人性的内容，这些内容可能会被模型学习并在未来的预测中体现出来，这可能导致模型的输出存在严重的偏见和歧视；模型可能过度拟合这些不合理、不符合逻辑的数据，或者具有某种特定的偏见或者偏斜的数据，从而遗忘它在更广泛、更均衡的数据上学习到的知识，导致在面对真实、合理的数据时，泛化能力受损——即使AI生成数据不包含低质量内容，如果其不能准确地反映真实世界的总体分布，那么这些数据也可能会降低模型在面对未见过任务时的泛化能力。 那么，AI模型有可能“涌现”出对生成内容质量的“品味”，找到对真实世界分布的“感觉”吗？作为一种计算模型，AI模型是通过数学运算和大量数据的训练来进行预测和决策的，并不具有真实的“感觉”或“品味”。然而，从某种程度上，AI模型可以通过学习和优化来逼近(模仿)这些功能。AI模型可以通过学习评价函数或损失函数来优化它们生成的内容质量。例如，对于语言模型，可以通过学习评估语法正确性、信息完整性、创新性等因素的评价函数来优化生成的文本质量。然而，这需要大量的标注数据和精心设计的评价函数，否则模型可能会过度优化某些容易量化的指标，而忽视其他重要的质量因素。AI模型可以通过学习真实世界数据的分布来优化其泛化能力。包括使用更大规模、更多样化的训练数据，以及使用正则化技术来防止过拟合。然而，由于真实世界的复杂性，模型可能很难完全捕捉到所有的数据分布特征，尤其是在任务不明确、数据超级稀缺的情况下。AI模型确实可以通过自监督学习方法来实现自我进化。在这种方法中，模型在没有人工标注的数据上进行训练，通过预测数据的某些部分来学习数据的结构和模式。然而，这种方法在实际应用中仍面临很多挑战，包括如何设计有效的自监督任务，如何解决模型的过拟合问题，以及如何确保模型的学习符合我们的期望和价值观等。总的来说，虽然AI模型可以在一定程度上模拟出对内容质量的“品味”和对真实世界分布的“感觉”，但它们依然依赖于我们人类设计的算法、损失函数和训练策略。未来的研究可能会发现更有效的方法来提高模型的质量判断和泛化能力，以及实现模型的自我进化。 最后，类比基因多样性的概念，也许可以更深入地理解AI训练数据的多样性和相似性对大模型质量的影响。基因多样性在生物学上是至关重要的，因为这意味着一个物种能更好地适应环境变化，增加物种的生存和繁衍能力。在AI训练中，数据多样性也同样重要。多样性丰富的数据可以提供更全面的信息，帮助AI模型学习和理解更广泛的模式和关系，提高模型的泛化能力。如果训练数据只来自AI生成的一部分，那么这些数据可能具有相似的风格和偏见，这会限制AI模型的学习和理解能力，降低其在处理新颖、未见过的任务时的表现。近亲繁殖可能导致基因的同质化，增加了有害基因的表达和累积，从而影响个体的健康和生存能力。在AI训练中，如果数据过于相似或重复，也会引起类似的问题。如果一个AI模型主要或完全使用由自身或类似模型生成的数据进行训练，那么这种“数据近亲繁殖”可能导致模型的学习过程中出现过拟合，使模型在面对新的、与训练数据不同的数据时表现不佳。此外，这种方式还可能导致模型的偏见和错误被放大，从而降低输出内容的质量。 为了避免使用AI生成数据进行训练对大模型质量的不利影响，可以从以下几个方面进行考虑：对AI生成的数据通过立法等手段在生成、分发、使用等各环节与人工数据有效区分，这是一种可能的策略，以管理和控制AI生成内容的质量和公平性，有助于提高透明度，使用户在使用这些数据时能做出知情的决定。对AI生成的数据进行筛选和质量控制，去除低质量、错误信息或者偏离真实分布的数据，确保训练数据的质量。可以采用人工或半自动的方式进行数据清洗和筛选。尽可能使用多元、多样性的数据进行训练，避免数据单一导致的过拟合，保证训练数据能够覆盖真实世界的多种情况。对模型的架构进行优化，如引入多头多层次的注意力机制，使得模型更能注意到重要的信息；对训练策略进行调整，如采用迁移学习、元学习等方法，使得模型能更好地学习和泛化；对训练过程进行监控，及时发现并纠正模型的过拟合等问题。建立有效的模型评估和反馈机制，对模型生成的结果进行质量评估，及时反馈并调整模型，形成高质量的正反馈过程，使其更好地满足质量要求。遵守相关的法规和伦理指南，保证AI的发展在可接受的道德和社会范围内。这也可以帮助确保AI生成的数据和其结果不会产生不利的影响。","tags":["LLM"],"categories":["随笔"]},{"title":"好的代码就像一封情书","path":"/2023/07/22/good-code-like-love-letter/","content":"原文：Good code is like a love letter to the next developer who will maintain it 编程的真谛：好的代码是一封情书我们常常将编程理想化，将其描述为抽象的艺术、科学，甚至是魔法。然而，实际情况要更加务实和踏实。代码，本质上是一种沟通方式。在作者编著的《学习JavaScript设计模式》一书开篇，曾说过：“优秀的代码就像是写给将来维护它的开发者的情书。”这是一种亲密的联系，由一个开发者写给另一个开发者，跨越时间和空间。 爱的语言情书是个人的、真诚的、体贴的，是对感情的诗意见证，往往经过精心打磨，以准确地传达感情。好的代码也是如此。它是个人的，因为它反映了编写者的逻辑和方法。好的代码是真诚的，没有不必要的复杂性。它是体贴的，关心下一个开发者将如何解读它。最重要的是，好的代码经过精心设计，以最高的效率解决问题。 模式和原则就像用语法规则和语言结构可以将词语和感情组成可理解的句子，我们也有设计模式和原则来塑造代码。模式不仅使代码具有可伸缩、可维护且高效，还使其易读易懂。它们为开发者提供了共享的术语，使他们能用普遍认可的结构表达复杂的软件设计。 因此，好的代码巧妙地运用这些模式，就像熟练的诗人使用修辞手法创造共鸣。不是为了滥用模式，而是因为它们为解决方案增值，使代码更易理解，并确保代码库的持久性。 SOLID、DRY、KISS和YAGNI不仅是原则，而且是打造优秀代码的基石。它们引导开发者做出明智的决策，在过度和过少工程化之间取得平衡，最终写下让后来者珍视的“情书”。 最佳实践好的代码也遵循既定的最佳实践，就像情书会遵循某些社交礼仪一样。适当的命名约定、模块化和详尽的注释都是其中的一部分。它们不仅仅是规则，需要遵循，而且是规范，用来表明代码(或编写者)对下一个开发者是多么体贴。确保编写者的意图不会在传递中失去。 拥抱测试就像作家校对他们的信件一样，开发者也应该对他们的代码进行校对。严格的测试和测试驱动开发(TDD)的实践是精心打磨的“情书”的标志。测试验证代码在各种场景下的表现，发现潜在的缺陷和盲点。强大的测试框架的存在往往证明了代码的质量。 共情和尊重最重要的是，一篇情书的核心是对读者的共情和尊重，好的代码也是如此。编写其他人可以阅读、理解和维护的代码，是一种职业尊重。这表明编写者理解他们的工作是一个更大的、持续不断的努力，软件是一个不断演进的生命体，将有许多人在未来继续塑造它，续写它的命运。 结论最后，编程是一种创作行为，类似于写一首诗或画一幅画。然而，我们的创作之美不仅仅取决于我们算法的优雅或代码的高效，更取决于其他人可以如何快乐、轻松地在我们打下的基础上继续展开工作。作为开发者，我们的任务不仅是解决今天的问题，也是确保我们不会成为明天的问题。 因此，好的代码不仅是一封情书，也是我们留给后来者永恒的遗产。","tags":["Programming"],"categories":["文摘"]},{"title":"LLaMA2不是真正意义上的“开源”","path":"/2023/07/22/llama2-isnt-open-source/","content":"原文：LLaMA2 isn’t “Open Source” - and why it doesn’t matter LLaMA2 并非真正的“开源”，但这并不重要作者是一位开源公司创始人，多年来一直参与开源社区，对开源项目的贡献、演讲和投资充满热情。他认为，互联网之所以成为现在的样子，很大程度上归功于那些支撑着数字基础设施的优秀开源项目，因此开源始终是他心中的重要话题。 然而，当 LLaMA2 出现时，许多他尊敬的社区成员对该模型误用“开源”一词感到不满。 LLaMA2 虽然在很大程度上是开源的，但其中有限制条件，例如：如果在发布日期时月活跃用户超过7亿，就不能以商业目的使用该模型；同时也不能使用该模型的输出结果来训练其他大型语言模型。这些限制与开源精神不太相符。但是，尽管作者同意 LLaMA2 在传统意义上不能称为开源，但他认为这并不重要。在人工智能模型的世界中，“开源”一词需要再次演变。 从自由到开源在文章中，作者回顾了自由软件和开源运动的历史。自1976年“给业余爱好者的公开信”以来，软件公司的商业利益与想要绕过限制的黑客的好奇心之间一直存在紧张关系。70年代，自由软件运动在麻省理工学院的人工智能实验室起源，由 Richard Stallman 创立，最终于1983年发展成 GNU 项目。GPL “copyleft” 许可证诞生，并被 Red Hat、MySQL、Git 和 Ubuntu 等项目采用。 “开源”这个词在1998年得以确立，归功于麻省理工学院的 Christine Peterson。在“免费软件高峰会”上，“自由软件”一词正式被“开源软件”取代。随着时间的推移，“自由软件”和“开源软件”社区出现了分歧，因为它们对“自由”和“开源”的理解不同。自由软件，如自由软件基金会所规定，只是开源软件的一个子集，采用非常宽松的许可证，如 GPL 和 Apache。 在过去十年里，由于商业开源公司和云超大规模企业之间的紧张关系，出现了另一种分歧。Elastic 和 MongoDB 将其开源项目转换为“服务器端公共许可证”（SSPL），允许开发者在商业用途下使用产品，前提是所提供的不是产品的托管版本。其目标是阻止 AWS 将它们的产品作为云服务重新托管并从中获利。然而，SSPL 也侵犯了开源理念，并未获得开源倡议组织的认可。尽管如此，大多数开发者仍然认为 MongoDB 是开源的。逐渐地，“开源”一词正在失去其自由的涵义，在开发者心目中几乎成为“源码可用”的同义词。 从源码到权重随着像 Dolly、MPT、LLaMA 等开放模型的崛起，社区中出现了类似的分歧。对于大多数 AI 工程师来说，如今的“开源”意味着“可下载权重”，仅此而已。Heather Meeker 提出了“开放权重”的定义，但目前还没有社区共识。问题在于，开放权重是否足以使一个模型被称为开源；软件的类比是项目发布其二进制文件而不提供源代码以供重新构建。 要使模型真正成为开源且可从头开始重新训练，创建者需要分享所有的训练代码、预训练数据集、微调偏好、RLHF 示例等。然而，这些训练过程的成本非常高，即使有人愿意全部公开，对于大多数开发者和公司来说，从头训练模型是不可行的，因此能够获得最终权重更加实用。 开放模型在大型语言模型（LLMs）领域，术语“开源”用于定义多种开放程度： 开放模型：如 RedPajama 和 MPT-7B，它们的权重对商业用途是开放的（使用 Apache 2.0 许可证），而且可以从头开始重新训练，因为数据集是开源的。 开放权重：StableLM 是 StabilityAI 训练的开放模型。权重是开放的，使用 Apache 2.0 许可证，但用于训练的数据集对公众是不可用的。 受限权重：这是指 LLaMA2。预训练数据集也不可用，尽管权重据称对商业用途是开放的，但存在上述特定限制。 受污染(Contaminated)权重：Dolly 1.0 和 LLaMA1 属于这一类别。权重是公开的，但用于训练它们的数据集不允许商业用途，这使得它在技术上是开放的，但实际上是无用的。 在可预见的未来，开源和开放权重将被互换使用，而作者认为这没问题。重要的是，越来越多的工作以尽可能开放的方式进行。对于 LLaMA2 的许可证，人们可能感到失望，但是 Meta 刚刚将价值约200万美元的浮点运算放进了 Github 库，作者认为这对该领域的进展将产生积极的影响。","tags":["LLM"],"categories":["文摘"]},{"title":"Meta为何选择开源Llama 2？","path":"/2023/07/22/why-did-meta-open-source-llama/","content":"原文：Why Did Meta Open-Source Llama 2? 对Meta开源Llama 2可能原因的一些猜想削弱竞争对手的优势 Llama 2 对拥有专有模型的竞争对手，如Google和OpenAI(以及Microsoft的相关服务)，构成了挑战。通过开源大模型，可以削弱这些大公司在语言模型领域的优势。 作为身处服务栈需要持续吸引用户的公司，Meta坐拥数十亿固定用户，这使他们在竞争中占据优势。 市场推广策略的考虑 Llama 2提供不同参数大小的模型，包括7b、13b和70b。通过将较小的模型作为“免费版”自助选项，Meta可以吸引用户，并鼓励他们在未来选择Meta平台的更大或更新的版本。 Meta可能推出一系列与Llama 2相关的扩展产品，包括为Instagram、Threads或Facebook提供支持Llama的功能，特殊设计的硬件芯片和数据中心，以及在PyTorch中优化Llama衍生模型的机器学习框架。 此外，Meta未来可能推出托管服务作为商业化产品，其中将Llama 2作为基础组件。 市场营销和声誉建设 通过开源Llama 2并将自己定位为技术前沿的公司，Meta旨在建立强大的声誉。这样的声誉可以吸引开发者、用户和媒体的关注，类似于Google多年来的积极影响。 总体而言，开源Llama 2的决定使得Meta可以挑战和削弱竞争对手，用自助免费模型吸引用户，在语言模型领域树立创新领先的公司形象。","tags":["LLM"],"categories":["文摘"]},{"title":"沉迷工具不会让你成为大师","path":"/2023/07/10/Amateurs-obsess-over-tools-pros-over-mastery/","content":"原文：Amateurs obsess over tools, pros over mastery 文章主要信息 文章强调了业余爱好者过度迷恋新工具，而专业人士更关注技艺的精湛。 专业人士知道工具并不能决定成就，真正重要的是个人运用它们的心态和技能。 文章通过例子阐述了技能的重要性，指出工具本身并非关键，个人的精湛运用才是决定成就的关键。 文章提倡专注于培养基本技能和永恒的原则，摒弃追求新奇工具的心态。 详细重点内容 业余爱好者迷恋新工具：现代科技发展迅速，新的工具和应用层出不穷。很多人容易陷入追逐新工具的陷阱，相信使用这些工具会带来高效率、更多产出和成功。然而，过度迷恋新工具只是短暂的热情，很快就会被其他新工具取代，而这些追逐也是徒劳的。 专业人士关注技艺：与业余爱好者不同，真正的专业人士知道工具并不能代表他们的成就。他们理解重要的是运用这些工具的个人心态和技能。类似于音乐家运用吉他演奏，吉他本身可能并不是最先进的工具，但在熟练的音乐家手中，它能产生动人的旋律和感人的和声。 精湛的技艺超越工具：文章举例说明吉他手通过对技艺的深入研究和练习，使得吉他成为创作灵感的源泉。这表明真正的精湛技艺超越了工具本身，是在不受技术潮流影响的基础上持续存在的。 专注培养基本技能：专业人士理解重要的是不断磨练自己的技艺，无论手中有怎样的工具。文章引用了武术家李小龙的名言，强调通过深入、持续的练习，掌握基本技能才是成为专家的关键。 不盲目追求新工具：文章警示不要盲目追求新工具，而是专注于打磨自己的技能。新工具可能只是短暂的潮流，而真正持久的是对基本技能和原则的坚持。 超越人工智能的创造力：文章提到人工智能的发展，但也指出真正的创造力和技艺仍然超越人工智能的能力。专注于个人的技艺将使人们摆脱对人工智能创造力的过度依赖，实现真正意义上的创新。 反思和自省：文章最后呼吁读者在追逐新工具时停下来反思，问自己是否真正在提升自己的技艺，是否被他人的意见所左右，是否在做真正有意义的事情，而不是只是追逐表面的诱惑。 结论文章强调了专业人士不是盲目追求新工具，而是注重培养自己的技艺和技能。真正的精湛技艺超越了工具本身，并且在不受技术潮流影响的基础上持续存在。专注于培养基本技能和原则，而不是追逐新奇工具，才能成为真正的专业人士。文章还提醒我们要超越人工智能的创造力，通过反思和自省，避免陷入盲目追逐的状态。","categories":["文摘"]},{"title":"AI权重开\"源\"怎么论","path":"/2023/07/06/AI_weights_are_not_open_source/","content":"原文：AI weights are not open “source” 文章主要信息总结 AI的许可证复杂多样，不同于传统软件的开源或专有授权。 AI有多个组成部分，如源码、权重、数据等，每个部分的许可方式可能不同。 为了标准化对AI许可证的讨论，文章提出了一套许可证类型的分类，包括专有、合作者、可用、伦理和开源。 提倡区分AI的源代码和权重，认识到权重不是源代码，需要特定的许可证类型。 引用了“开放权重”（Open Weights）和“伦理权重”（Ethical Weights）等术语，以便更准确地描述不同类型的AI权重许可证，避免误用“开源”这个术语。 详细重点内容 AI许可证复杂性：AI许可证不同于传统的软件许可证，因为AI有多个组成部分，如源码、权重、数据等，每个部分可能有不同的许可方式。此外，AI的使用还涉及到社会伦理层面的考量，需要更多的限制和约束。 标准化许可证分类：为了更好地讨论AI许可证，文章提出了一套分类，包括专有、合作者、可用、伦理和开源。每个分类针对不同的用途和组成部分，以更清晰地描述许可证的属性。 “开放权重”和“伦理权重”：文章指出权重和源代码是两个不同的概念，不能将权重称为“开源”，因为它们并不是源代码。为了避免混淆，文章提出了“开放权重”和“伦理权重”等术语，以更准确地描述权重的开放程度和许可限制。 权重的真正开放性：许多标注为“开源”的AI权重实际上并不是真正的开源，因为它们不是源代码，而是特定的组件。文章强调使用正确的术语，如“开放权重”和“伦理权重”，有助于推动行业发展，并在不同类型的权重上建立标准。 避免“开源洗白”：由于AI面临与传统软件不同的挑战，可能需要限制某些潜在的有害用途。文章提倡使用“伦理”来描述允许类似开源的自由使用的许可证，避免“开源洗白”，为用户提供更清晰的信息。 结论文章着重强调AI许可证的复杂性，提出了一套分类以标准化对AI许可证的讨论。作者建议使用“开放权重”和“伦理权重”等术语，以更准确地描述不同类型的AI权重许可证，避免混淆和误用“开源”这个术语。同时，文章强调区分AI的源代码和权重，认识到权重不是源代码，需要特定的许可证类型。通过正确的命名和分类，有助于推动AI领域的标准化和发展。","tags":["LLM"],"categories":["文摘"]},{"title":"AIGC时代程序员如何保持领先","path":"/2023/07/05/4_tips_for_programmers_to_stay_ahead_of_generative_AI/","content":"原文：How Coders Can Survive—and Thrive—in a ChatGPT World &gt; 4 tips for programmers to stay ahead of generative AI 文章主要信息总结 人工智能，特别是由大型语言模型（LLM）驱动的生成式人工智能，可能会改变许多编程人员的生计，但一些专家认为AI不会立即取代人类程序员。 软件开发人员可以通过遵循基本原则和最佳实践，以及找到适合自己需求的AI工具来在生成式AI时代生存和发展。 保持对编程基础和问题解决技能的重视，同时加强软件工程实践，规划系统设计和软件架构。 在使用AI编程助手时，清晰明确的交流是关键，需要详细说明需求，使用合适的提示工程方法，对AI生成的代码进行审查和验证。 开发人员应对大型语言模型的输出持批判态度，了解其潜在的风险和局限性，同时注意版权和安全问题。 详细重点内容 保持基本原则和最佳实践：虽然AI辅助编码工具可以帮助完成代码和生成代码，但编程的基本原则和最佳实践依然重要，如阅读和理解自己和他人的代码，以及了解编写的代码如何适应更大的系统。 问题解决技能：解决问题仍然是程序员最重要的技能之一。分析问题并找到优雅的解决方案在编程领域中依然备受推崇。 选择适合自己需求的AI工具：找到合适的AI工具至关重要。不同的工具有不同的交互方式和整合方式，可以用于自动化单元测试的创建、生成测试数据、或编写文档等。 清晰明确的交流：使用AI编程助手时，需要详细说明需求，对生成的代码进行审查和验证。合理的提示工程方法可以帮助与AI模型进行更有效的交流，以获取满足需求的代码。 对AI输出持批判态度：大型语言模型往往会产生错误或不准确的代码，因此软件工程师需要对生成的代码进行审查和验证。了解模型的训练数据和版本等信息有助于理解结果并提供更多上下文。 注意安全和版权问题：AI生成的代码可能存在漏洞，软件工程师需要注意安全问题，并采取代码审查和强有力的测试流程来防范风险。此外，版权问题也需要考虑，尤其是使用私有代码时。 结论在生成式AI时代，软件开发人员需要认识到AI是一种工具，将其纳入工作流程，同时了解这些工具的机会和限制，并继续依靠自己的人类编码能力来取得成功。重视问题解决能力、保持软件工程实践和审查AI输出是软件开发人员在AI时代生存和发展的关键。","tags":["AIGC"],"categories":["文摘"]},{"title":"科研数据分享最佳实践","path":"/2023/06/29/How-to-make-your-scientific-data-accessible-discoverable-and-useful/","content":"原文：How to make your scientific data accessible, discoverable and useful 这篇文章讨论了在开放科学和可重复性的背景下发布可用和高质量数据的最佳实践。越来越多的研究人员被鼓励在发表论文的同时提交数据，但在处理来自不同来源和格式的数据时可能会遇到挑战。以下是一些数据科学家建议的关键做法： 制定元数据: 元数据描述数据，对于使数据符合FAIR（可找到、可访问、可互操作、可重用）原则至关重要。科学家应提供详细的数据收集、处理和变量信息，以及表格或文件之间相互关联的解释。 多分享: 最好能同时分享原始数据和派生数据。原始数据允许其他研究人员测试假设和处理策略，而派生数据则是分析的基础。 采纳标准: 科学家应该寻求更广泛社区的指导，了解数据存储库和文件格式。推荐使用开放、非专有的文件格式，如CSV，以确保数据长期可读。 包含代码: 当数据分析涉及代码时，研究人员应将代码与数据一起分享。代码应有良好的文档记录，清除特定计算机元素，并经过可重现性测试。 考虑可访问性: 考虑潜在数据用户的技术基础设施和要求。咨询相关组织，以获取有关数据标准和假设的反馈，并为不同条件下的用户开发低技术解决方案。 迈出第一步: 开放科学不必是非此即彼的。即使分享部分数据也能增加价值和促进合作机会。 通过遵循这些最佳实践，研究人员可以促进科学的发展，推动合作，并确保其工作的可重复性。","tags":["Academic","Data"],"categories":["文摘"]},{"title":"图解：私有知识库LLM聊天机器人","path":"/2023/06/28/LLM-based-Chatbot-to-query-Private-Knowledge-Base/","content":"原文：LLM based Chatbot to query Private Knowledge Base LLM based Chatbot to query Private Knowledge Base 将整个知识库的文本语料分割成多个块——每个块表示一个可查询的上下文片段，知识数据可以来自多个源； 用嵌入(Embedding)模型将每个块转换为一个向量； 将所有向量存储在向量数据库； 分别保存表示每个嵌入向量的文本，同时保存指向该向量的指针。 使用与嵌入知识库本身所使用的相同的嵌入模型，将要提问的问题&#x2F;查询进行嵌入，转换成向量； 使用生成的向量在向量数据库的索引中运行一个查询。选择要从向量数据库中检索多少个向量 - 这将等于您将要检索和最终用于回答查询问题的上下文数量； 向量数据库对所提供的向量执行近似最近邻(ANN)搜索，并返回之前选择的上下文向量的数量。该过程返回在给定的嵌入&#x2F;潜空间中最相似的向量； 将返回的向量嵌入映射到对应的文本块； 将问题与检索到的上下文文本块一起传给LLM(大语言模型)，通过提示指示LLM仅使用提供的上下文来回答给定的问题。这并不意味着不需要进行提示工程 - 需要确保LLM返回的答案符合预期的范围，例如，如果在检索到的上下文中没有可用的数据，则确保不提供虚构的答案。","tags":["LLM","ChatGPT","Chatbot"],"categories":["文摘"]},{"title":"LLM赋能的自主智能体(Agent)","path":"/2023/06/28/LLM-Powered-Autonomous-Agents/","content":"原文：LLM Powered Autonomous Agents 以LLM（大型语言模型）作为核心控制器构建智能体的概念很酷。一些概念验证演示，如AutoGPT、GPT-Engineer和BabAGI，都是令人鼓舞的示例。LLM的潜力不仅限于生成书面副本、故事、论文和程序，它可以被视为强大的通用问题求解器。LLM驱动的自主智能体系统概述：LLM作为智能体的大脑，配合几个关键组件：子目标规划和分解、反思和改进、短期记忆、长期记忆和工具使用。该系统还包括记忆类型、最大内积搜索(MIPS)算法、工具使用能力的案例研究，以及AutoGPT等概念验证示例。 要点： 建立以大型语言模型(LLM)为核心控制器的智能体系统是一个很酷的概念。 LLM的潜力不仅限于生成文本、故事、论文和程序，它还可以被视为一个强大的通用问题求解器。 智能体系统的核心组成部分包括规划、反思、记忆和工具使用。 规划：将复杂任务分解为可管理的子目标，实现对复杂任务的高效处理。 反思和改进：智能体可以对过去的行动进行自我批评和反思，从错误中学习并改进，提高最终结果的质量。 记忆：短期记忆用于模型的上下文学习，长期记忆通过外部向量存储和快速检索提供了无限的信息存储和回忆能力。 工具使用：智能体学习调用外部API获取模型权重中缺失的额外信息，包括当前信息、代码执行能力、专有信息源等。 LLM+P方法利用外部经典规划器进行长程规划，通过PDDL语言描述规划问题，将规划步骤外包给外部工具。 自反思：智能体通过结合任务特定的离散动作和语言空间来进行自反思，从而改进决策和行动，提高推理能力。 CoH通过向模型提供过去输出的历史序列和反馈来改进模型的输出质量。 算法蒸馏利用交叉轮次历史训练神经网络，学习强化学习算法的过程而非任务特定的策略。 记忆分为感觉记忆、短期记忆和长期记忆，长期记忆可以通过外部向量存储和快速检索进行扩展。 外部记忆通常使用最大内积搜索(MIPS)算法进行快速检索，常用的算法包括LSH、ANNOY、HNSW、FAISS和ScaNN。 工具使用可以显著扩展模型的能力，如调用API、使用外部模块等。 MRKL是一个神经符号化架构，将LLM与多个专家模块结合起来，根据任务选择合适的模块。 HuggingGPT是一个使用LLM作为任务规划器的框架，根据模型描述选择合适的模型并提供执行结果的总结。 API-Bank是一个评估工具增强LLM性能的基准，包含常用API工具、完整的工具增强LLM流程和带有API调用的对话数据集。 ChemCrow是一个领域特定的示例，LLM与专家设计的工具结合，用于完成有机合成、药物发现和材料设计等任务。 Generative Agents是一个基于LLM的虚拟角色模拟实验，结合记忆、规划和反思机制，实现了智能体之间的互动行为。 AutoGPT是一个证明概念的示例，展示了将LLM作为主控制器的可能性，但在可靠性方面存在一些问题。","tags":["LLM","Agent","ChatGPT"],"categories":["文摘"]},{"title":"学术界能否与资源雄厚的工业界相竞争？计算机图形学的历史观点","path":"/2023/06/28/Can-academia-compete-with-the-resources-of-industry/","content":"原文：Can academia compete with the resources of industry? - A historical perspective from Computer Graphics 文章探讨了学术界与资源雄厚的工业界相竞争的问题。作者回顾了计算机图形学领域的发展历程，并指出工业界在数据规模、计算集群和工程团队等方面具有优势。然而，学术界并没有试图复制工业界的成就，而是专注于一些被工业界忽视的领域，如物理模拟、光照模拟、外观模型和机器学习等。随着时间的推移，学术界的研究成果逐渐影响了计算机图形学的渲染技术，并在图形硬件方面取得了一定的贡献。文章指出学术界的盲点和困境，同时强调了在学术研究中的重要原则，包括专注于长期目标、学习非主流技术、开放合作和保持乐趣等。总体而言，学术界在计算机图形学领域中扮演着重要的角色，并与工业界共同推动了该领域的发展。 要点： AI研究的规模让人们担心学术界在资源方面是否能与工业界竞争。 回顾计算机图形学领域，作者回忆起25年前作为博士生时的经历，当时工业界在资源方面具有明显优势。 工业界在规模方面具有优势，包括输入训练数据、计算集群和工程团队等方面，学术界无法与之匹敌。 学术界并没有试图复制工业界的成就，而是探索不同的方法，并专注于工业界认为不太重要的领域，如光线和动作的物理模拟。 学术界在许多领域进行了研究，包括光照模拟、基于图像的建模和光照、皮肤和头发的外观模型、衣物和流体等的物理模拟，以及动画的机器学习等创新领域。 随着时间推移，计算机图形学中的渲染技术在很大程度上依赖于学术界的研究，如蒙特卡洛路径追踪、材质外观模型、基于图像的光照和非真实感渲染。 学术界也在图形硬件方面做出了贡献，开创了灵活的硬件、光线追踪硬件和通用计算在GPU上的应用。 计算机图形学中的学术研究使该领域更加数学化，对离散微分几何、蒙特卡洛路径追踪、流体模拟等实际解决方案产生了影响。 作者提到自己参与了Halide的开发，这是一种用于高性能图像处理的编程语言和编译器，获得了工业界的成功采用。 学术界和工业界都对计算机图形学做出了重要贡献，而工业界通常在学术界广泛接受之前引领了具有范式变革意义的创新。 作者强调学术界存在盲点，并指出计算机图形学直到90年代才得到广泛认可。 从历史的角度得出的教训包括致力于不同的研究方向，学习非主流的技能和技术，探索现实世界的问题，质疑假设，专注于长期目标，开源和合作等。 工业界对学术思想进行改进是自然的顺序，学术界应专注于理论、理解和战略优势的发展。 强调小型、灵活的团队、系统思维以及在计算机图形学领域保持乐趣的重要性。","tags":["Research"],"categories":["文摘"]},{"title":"Open LLM Leaderboard排名之谜","path":"/2023/06/27/What_s_going_on_with_the_Open_LLM_Leaderboard/","content":"原文：What’s going on with the Open LLM Leaderboard? 本文讨论了Open LLM Leaderboard上MMLU评估的差异问题。不同的评估实现会给出不同的结果，并且可能改变模型在Leaderboard上的排名。作者强调了评估与实现细节密切相关，开放、标准化和可复现的基准测试对于改进LLM非常重要。介绍了三种不同的MMLU评估实现，即Harness实现、HELM实现和Original实现，并比较了它们的结果。最后，提到将更新EleutherAI Eval Harness，并更新完整的Leaderboard。 要点： Twitter上发布了Falcon，并加入了Open LLM Leaderboard(开放排行榜)，引发了有趣的讨论。 讨论的焦点是排行榜上的四个评估之一：用于衡量”Massive Multitask Language Understanding(MMLU)”的基准。 当前排行榜上排名第一的LLaMA模型的MMLU评估数据明显低于LLaMa论文中的数据，这让社区感到惊讶。 为了弄清楚情况并解决问题，运行了三种不同的MMLU评估实现，并对模型进行了排名。 不同实现方式给出的评估结果差异很大，甚至改变了模型在排行榜上的顺序。 MMLU是一个多项选择题测试，评估方式有多种，其中包括模型生成的概率和生成的文本与预期答案的比较。 在评估过程中，不同实现方式在提示语、模型输出预测等方面存在细微差别。 模型在同一数据集上的得分和排名非常敏感，不同评估方法得出的结果不可比较。 标准化和可复现的评估基准对于比较不同模型和研究成果至关重要。 Open LLM Leaderboard决定采用社区维护的评估库，并更新了MMLU评估的实现以保持一致性。 正在更新完整的排行榜，使用更新后的评估库进行评估。","tags":["LLM","Leaderboard"],"categories":["文摘"]},{"title":"关于大型语言模型评估的思考","path":"/2023/06/27/The_Curious_Case_of_LLM_Evaluations/","content":"原文：The Curious Case of LLM Evaluations 语言模型评估的复杂性和挑战作者在本文中讨论了语言模型（LLMs）评估中的复杂问题和挑战。他指出随着建模、扩展和泛化技术的快速发展，我们的基准测试能力却没有同步增长，导致了评估的不足和过度夸大的能力。即使每一项能力都非常优秀，如果我们没有合适的工具来了解这些能力是如何实现的，或者模型在这些能力上的表现如何，那么我们可能会过分高估模型的能力。如果我们总是在铺好的道路上进行比赛，每个右转都是黄树，每个左转都是绿树，那么我们可能会认为模型在每次比赛中都会获胜。 评估中的常见问题 数据泄露：测试数据泄露到训练集中，特别是对于LLMs，其中数据集的细节通常缺失。 覆盖问题：评估数据集往往无法全面涵盖某个任务的所有不同方式，可能导致准确性问题、样本规模问题、可变性问题或鲁棒性问题。 虚假相关性：评估集合中的某些任务可能存在“捷径”解决方案，导致评估结果不准确。 分区和措辞：评估数据集的划分不容易处理，可能导致意外泄露。例如，对于以人为中心的任务，评估数据集通常不是为用户单独划分的，而是单独划分为样本。 随机种子：神经网络输出通常会在某种程度上依赖于随机种子，导致单次推断结果可能导致错误的结果，给情况提供不清晰的画面。 精度与召回的权衡：对于不同任务，误报和漏报的影响并不相同，而许多人仅报告准确性结果。 评估组件：LLMs的六个组成部分 评估数据集：这些是用于评估模型的测试样本。构建和使用这些评估数据集有多种方法，但每种方法都伴随着一系列问题。 模型输出：评估从生成模型中获得的输出。模型输出对于所问问题的不同形式和答案的不同形式可能会有所影响。 样本&#x2F;输出转换：对模型输出或其输入进行的许多转换。这些转换可以大致分为4类：循环变换、链式变换、原子输出和约束输出。 地面真实值：考虑到评估的当前情况，地面真实值可能具有偏见、模糊或高度不一致的问题。在进行评估时，需要多次比较模型的输出以获得真实分布的比较。 评估介质：将评估介质分为三个不同的组别：直接评估指标、间接评估或分解的基于模型的评估和模型驱动的评估。 性能报告：在报告性能指标时，需要考虑多种因素，如数据集划分和轻微变化。报告结果的均值和标准差对于解释结果的显著性至关重要。 总结评估LLMs是一项复杂的任务，需要综合考虑各种因素。数据泄露、覆盖问题、虚假相关性等是评估中的常见问题。不同的评估方法，包括直接评估指标、间接评估和模型驱动的评估，各自有其优势和不足。在报告性能指标时，需要注意准确性和细节，并认识到模型输出可能会因种子不同而产生变化。同时，需要小心使用优化的提示，因为最佳提示可能并不适用于所有用户或实际场景。理解这些复杂性对于有效评估LLMs非常重要。","tags":["LLM","ChatGPT"],"categories":["文摘"]},{"title":"脚本(Script)与程序(Program)","path":"/2023/06/26/script-vs-program/","content":"原文：Failing to draw lines between ‘script’ and ‘program’ Unix脚本与程序：关于术语的辩论本文讨论了在Unix环境中关于“script”（脚本）和“program”（程序）的术语辩论。投票者提出了一些模糊的观点，并提供了一些实践中的思考。 投票结果和模糊观点 投票中，有人选择了“complex&#x2F;simple”（复杂&#x2F;简单），但他认为这个问题在实践中更为复杂。 他认为，对于编译语言，无论大小或复杂程度，都应称之为“program”（程序），即使是单文件的解释语言也是如此。 他认为，“script”与Unix的“#!行和解释器”问题无关，这只是一个技术问题，有时候可能并不重要。 对“script”和“program”的不同理解 有时他将shell脚本（无论是Bourne shell还是其他Shell）称为“script”，而将其他语言的内容称为“program”。 他对复杂的sed块是“script”还是“program”并没有明确的答案。 对于报告Linux cgroup内存使用情况的Bourne shell脚本，他认为它是“script”，但用Python进行重写后则是“program”。 对扩展其他程序的代码的看法 Emacs Lisp的内容可能是“script”、“program”或其他术语，Emacs用户可能有自己的术语。 Vim的“script”语言用于编写插件，对于用它编写的内容，他不确定人们是否称之为“script”。 总结在Unix社区中，关于“script”和“program”的术语使用存在辩论。对于一些人来说，用shell编写的内容可能被称为“script”，而其他语言的内容则被称为“program”。而扩展其他程序的代码可能被认为是不同于“script”的内容。这个术语辩论可能因个人经验和偏好而异，因此没有明确的标准定义。 （计算机术语的多样性确实让人觉得有趣。）","tags":["programming"],"categories":["文摘"]},{"title":"AI工程师入门指南","path":"/2023/06/26/How-to-Break-into-AI-Engineering/","content":"原文：Ask HN: How to Break into AI Engineering 文章提到了学习AI工程所需的技能和知识的一些资源，包括数学基础、统计学、Python编程、IBM数据科学专业证书、机器学习和深度学习专业课程等。还讨论了数学在软件工程师中的重要性以及AI工程的发展趋势。 要点： 要成为AI工程师，需要有扎实的数学基础，尤其是微积分和线性代数。 需要掌握统计学的语言和基本概念。 学习Python编程语言，掌握PyTorch。 推荐学习资源包括IBM Data Science Professional Certificate、Oliver Theobald的《Machine Learning for Absolute Beginners》、Andrew Ng的Coursera课程（包括《Machine Learning Specialization》和《Deep Learning Specialization》）以及fast.ai课程。 在选择AI领域的方向时，可以考虑自然语言处理（NLProc）、视觉处理（Vision）、强化学习（RL）等。 如果想在大型科技公司找工作，需要准备Leetcode面试、学习系统设计和机器学习系统的设计，还可以参考Chip Huyen的相关书籍。 数学在软件工程师中可能变得更加重要，因为AI在软件开发生命周期中的应用越来越广泛。 AI工程师的角色可以分为专注于模型改进、优化模型性能以及将模型应用于大型应用程序的不同方面。 学习数据清洗、数据标注和训练算法的基本过程非常重要，这些内容常常被忽视和低估。 AI工程师可以选择从数据工程入门，然后再专注于AI领域的特定方向。 AI工程师不仅需要技术知识，还需要与科学、数学等领域的知识相结合，可以结合自己的专业背景和兴趣选择方向。 AI工程师的角色可以是数据工程师的一种特殊化，不一定需要成为数学专家，但需要扎实的工程和数据处理能力。 AI工程师可以选择从事机器学习工程、数据工程和运维方面的工作，也可以选择研究工作，但后者通常需要更高的学历和数学基础。 了解数据工程的发展历程可以对AI工程师的角色有所启示，从中可以看到AI工程师可能在软件工程和AI领域之间进行工作。 AI工程师需要不断学习和尝试，掌握最新的技术和工具，并将其应用于实际项目中。 可以借助ChatGPT等工具来获取学习和编码的帮助，但仍需要自己进行实践和验证。 AI工程师的发展可能因不同的公司和行业而异，需要根据实际情况进行选择和发展。","tags":["AI","Engineering","Learning"],"categories":["文摘"]},{"title":"聪明人解决难题可能更慢","path":"/2023/06/26/Intelligent-people-take-longer-to-solve-hard-problems/","content":"原文：Intelligent people take longer to solve hard problems 智力高的人在解决难题时需要更长的时间一项新研究挑战了高智商与信息处理速度快的信念，揭示了解决问题能力与大脑连接性和额叶与顶叶之间同步的关联。研究结果表明，在认知过程中速度和准确性之间存在权衡，强调了在解决困难问题和做出更好决策时较慢和更费力思考的重要性。 1. 智力高的人解决复杂问题需要更长时间德国研究人员在《Nature Communications》杂志上发表的这项研究表明，智力得分较高的人解决复杂问题需要更长时间，因为他们不太可能草率下结论。研究还将问题解决能力与大脑区域之间的连接性和同步性联系起来。 2. 大脑区域之间的连接性与智力得分相关研究人员分析了1176名“人类连接组计划”参与者的数据，研究了智力得分与宾州矩阵推理测试反应时间之间的关系。结果显示，智力得分较高的人解决简单问题的速度较快，但在解决复杂问题时需要更长时间，这似乎是因为他们在达到正确解决方案之前花更多时间推断隐藏的规则。 3. 大脑额叶和顶叶之间的连接性与同步性接下来，研究人员通过将每个人的大脑连接性数据与决策和工作记忆的神经回路的一般模型相结合，生成了650名参与者的个性化大脑网络模型。这显示出在解决复杂任务时需要更长时间的人，额叶和顶叶之间的静息状态连接性更高，并且这些大脑区域之间的同步性更强。 4. 智力高并不意味着大脑速度更快这些发现挑战了高智商是大脑反应速度更快的假设。结果表明，更快并不一定更好，在某些情况下，在速度和准确性之间存在权衡，从而导致更好的决策。 5. 在解决难题时，较慢和更费力的思考更有效研究显示，对于简单任务的决策，快速的“自动”思考是足够的，但在解决更困难的问题时，较慢和更费力的认知方式更好，它支持对相关信息的持久整合。","tags":["IQ","Intelligence"],"categories":["文摘"]},{"title":"用LLM生成SQL的注意事项","path":"/2023/06/26/Generating_SQL_with_LLMs_for_fun_and_profit/","content":"原文：Generating SQL with LLMs for fun and profit SQL语言模型存在潜在的安全风险SQL（结构化查询语言）是一种编程语言，一些实现甚至是图灵完备的。连接语言模型和SQL数据库的教程已经出现，但这样的模型可能生成不安全的SQL语句。恶意用户可以利用这一点进行数据库攻击，如删除或更改表，甚至可能导致CPU的无限循环。 1. 教程中的SQL模型存在潜在安全风险一些教程允许语言模型生成任意的SQL以“查询”数据库，但很容易产生恶意的SQL语句。实际测试表明，用户可以请求执行危险操作，如删除或更改表，而模型将会执行。 2. 使用SQLite进行CPU消耗攻击使用SQLite数据库，用户甚至可以运行无限循环的SQL查询，导致CPU被占用。这种操作可能会对系统产生拒绝服务攻击。 3. 模型无法有效防止SQL注入即使将危险查询示例展示给模型，它仍可能无法准确标记其他潜在的危险查询。模型在接受新的提示时仍可能执行危险操作。 4. 建议防止安全风险如果数据库仅用于查询，可以将其设置为只读。同时，应创建一个权限极其有限的角色供语言模型使用，以最大程度地减少对数据的篡改。但这仍无法完全防止拒绝服务攻击和数据泄露。 5. 结论：慎重使用语言模型执行SQL语句在使用语言模型执行SQL语句之前，必须仔细考虑可能带来的安全风险。尽管它们可以帮助设计查询语句，但目前还不适合在生产系统中让它们将自然语言转换为可执行代码。这样的模型可能适用于教程和实验，但不适合于可信的生产环境。","tags":["LLM","SQL","programming"],"categories":["文摘"]},{"title":"语义压缩导向编程","path":"/2023/06/25/semantic-compression/","content":"原文：Semantic Compression 这篇文章介绍了一种名为压缩导向编程的有效编码方法，通过重构代码、提取重复代码片段和使用函数来简化代码。这种方法强调简洁、可读性和可扩展性，相比过于追求对象导向编程的复杂方法更为实用。作者通过示例展示了如何通过重构代码来减少冗余和提高代码的可读性、可维护性和可扩展性。他强调了在实际编码中，将重复的代码片段提取出来形成可复用的函数或结构是一种更好的方式，而不是过度强调对象导向编程的方法。 有些程序员过于追求对象导向编程，使用复杂的方法和工具，但实际上简单的代码和函数可以更好地满足需求。通过压缩导向编程，我们可以通过逐步优化和重构代码来达到更高效的编程方式，同时减少开发时间和错误。 在文中，作者以一段C++代码为例，展示了如何通过重构代码、提取重复代码片段并使用结构和函数来简化代码。通过这种方式，代码变得更加简洁、可读性更强，同时也更易于扩展和维护。 以下为本文要点： 对于解决实际问题，首先需要基于问题中的复数名词创建相应的类，如员工类和经理类。 为了将经理类与员工类和人类区分开，需要实现类的继承关系，经理类继承自员工类，员工类继承自人类。 需要添加一个承包商类，承包商类继承自人类，因为承包商不是员工。 为了解决经理类既可以是承包商又可以是全职经理的问题，可以使用类的模板化（templatize）。 程序员应该采用压缩式编程思维，先将代码实现具体功能，再根据重复出现的模式提取出可重用的部分。 将重复代码提取为结构体或函数，以减少代码冗余和提高代码的可读性、可维护性和可扩展性。 编程应以过程为导向，对象是为了实现过程的重用而产生的构造物，而不是过程本身。 通过压缩式编程，代码变得更加简洁和易于理解，可以简化代码的设计和开发过程。 总结：本文介绍了一种名为压缩导向编程的有效编码方法，通过重构代码、提取重复代码片段和使用函数来简化代码。这种方法强调简洁、可读性和可扩展性，相比过于追求对象导向编程的复杂方法更为实用。","tags":["programming","OOP"],"categories":["文摘"]},{"title":"ChatGPT类服务汇总","path":"/2023/06/20/chatgpt/","content":"ChatGPT是由OpenAI开发的大型语言模型，可以理解和生成人类语言，具备生成连贯和上下文相关的回复的能力。 这类服务的主要目的是通过自然语言处理和对话系统技术，为用户提供智能对话和信息交流的能力。以下是ChatGPT类服务可能提供的功能和用途： 聊天机器人：ChatGPT类服务可以用作聊天机器人，与用户进行实时对话。用户可以提问问题、请求帮助、寻求建议，而ChatGPT将根据其训练的知识和上下文生成回复。 客户支持：许多公司和组织使用ChatGPT类服务来提供在线客户支持。用户可以通过对话框与机器人代表进行交流，寻求帮助、解决问题或获得产品或服务的相关信息。 智能助手：ChatGPT类服务可以用作个人助手应用程序的一部分，帮助用户管理日常任务、提供实用信息、制定行程或提供娱乐建议等。它可以根据用户的指令和上下文提供个性化的回复和服务。 教育辅助：ChatGPT类服务可以在教育领域用作学习辅助工具。学生可以向ChatGPT提问问题、请求解释或寻求学习建议。它可以提供参考资料、解答问题或引导学生进行学术研究。 娱乐和游戏：ChatGPT类服务可以用于创建有趣的娱乐和游戏体验。例如，它可以扮演虚拟角色，与用户进行角色扮演游戏，或参与有趣的智力游戏和谜题。 需要注意的是，ChatGPT类服务的功能和用途可能因具体应用程序或平台而异。不同的开发人员和组织可以根据其需求和目标，结合ChatGPT模型的能力来设计和定制各种对话系统应用。 ChatGPT类服务汇总：无需注册 Bard (web-browsing) Vitalentum OraChat Vicuna GPTGO (web-browsing) AnonChatGPT Perplexity AI (web-browsing) NoowAI Character AI BAI Chat iAsk AI (web-browsing) Phind AI (web-browsing) GPT4All (open-source) DeepAI Chat Teach Anything HuggingChat (web-browsing) Forefront AI 需要注册 Poe AI Easy-Peasy AI WriteSonic Sincode AI AI.LS LetsView Chat (only 10 messages allowed) CapeChat Open-Assistant (open-source) GlobalGPT Bing Chat JimmyGPT Codeium (mainly for coding) YouChat Frank AI OpenAI Playground 博客文章写作助手 Copy AI TextCortex AI Marmof HyperWrite WriterX 文档(PDF等)聊天机器人 AnySummary (3 per day) Sharly AI ChatDOC Humata AI Ask Your PDF ChatPDF FileGPT ResearchAide Pensieve AI PDFGPT.IO Docalysis 个人助理聊天机器人 Pi, your personal AI Kuki AI Replika YourHana AI","tags":["LLM","ChatGPT"],"categories":["资源"]},{"path":"/google7db268adce4168b3.html","content":"google-site-verification: google7db268adce4168b3.html"}]
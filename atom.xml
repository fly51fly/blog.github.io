<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>爱可可小窝</title>
  
  <subtitle>@爱可可-爱生活 的学习与思考</subtitle>
  <link href="https://blog.aicoco.net/atom.xml" rel="self"/>
  
  <link href="https://blog.aicoco.net/"/>
  <updated>2023-08-14T01:04:18.039Z</updated>
  <id>https://blog.aicoco.net/</id>
  
  <author>
    <name>Guang Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Llama和ChatGPT都不算开源</title>
    <link href="https://blog.aicoco.net/2023/08/14/open-source-llm-not-open/"/>
    <id>https://blog.aicoco.net/2023/08/14/open-source-llm-not-open/</id>
    <published>2023-08-14T00:55:00.000Z</published>
    <updated>2023-08-14T01:04:18.039Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://spectrum.ieee.org/open-source-llm-not-open">Llama and ChatGPT Are Not Open-Source</a></p><p>文章分析了Meta发布的LLM模型Llama 2和OpenAI的ChatGPT是否真正开源。使用Radboud大学研究员设计的一套标准来给不同开源LLM模型的开放程度进行评分。论文指出，虽然Meta提供了Llama 2的预训练模型权重和文档，但未公开训练数据和训练代码，也未进行同行评审。进一步比较不同模型，论文发现虽有一些小型研究模型开源程度较高，但所有模型在重要的人工反馈强化学习环节和同行评审过程描述都很不充分，这可能会影响研究结果的可复现性。总体来看，商业LLM如Llama 2和ChatGPT虽被标注为“开源”，但实际开放程度有待改进。它们在保护商业机密的同时，也影响了学术研究的公开透明度，这也推动了LLM开发向更真正开源的方向转型。  </p><p>要点：    </p><ul><li>Meta发布了其大型语言模型Llama的更新，命名为Llama 2，并将其作为开源版本发布。  </li><li>Llama 2开源版本允许用户访问模型权重、评估代码和文档。  </li><li>Meta的开源发布旨在使模型“对个人、创作者、研究人员和企业可访问，以便他们能够负责地进行实验、创新并扩展他们的想法”。  </li><li>不同于其他开源的LLM和软件包，Llama 2在某种程度上比较封闭。尽管Meta已经提供了经过训练的模型，但未分享用于训练模型的数据或代码。  </li><li>在荷兰的Radboud大学的一组AI研究人员的研究中，他们认为Llama 2并不是唯一一个被质疑为“开源”的LLM。他们用多维评估模型的开放性，在一个表格中对15个不同的名义上开源的LLM进行评分。  </li><li>该研究小组认为Llama 2和ChatGPT的开源状态不佳。  </li><li>LLM的可复制性问题主要表现在模型的开放性和公开的评审过程上。  </li><li>商业LLM的发布方式避免了同行评审流程，而是通过公司托管的预印本文档进行发布。  </li><li>研究小组对Llama 2模型持批评态度，认为Meta将“开源”用于这个模型是误导性的。  </li><li>尽管这项研究可能促使领域朝着真正开源的模型开发方向发展，但研究人员仍对商业模型在学术研究中的使用持谨慎态度。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://spectrum.ieee.org/open-source-llm-not-open&quot;&gt;Llama and ChatGPT Are Not Open-Source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章分析了Meta发布的LLM模型Llama 2</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>单一大模型能包打天下吗？</title>
    <link href="https://blog.aicoco.net/2023/08/09/does-one-model-rule-them-all/"/>
    <id>https://blog.aicoco.net/2023/08/09/does-one-model-rule-them-all/</id>
    <published>2023-08-09T00:22:36.000Z</published>
    <updated>2023-08-09T01:24:35.979Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://maithraraghu.com/blog/2023/does-one-model-rule-them-all/">Does One Large Model Rule Them All?</a></p><p>文章探讨了人工智能的未来，提出一个观点：单一的大型通用人工智能模型不会主导整个人工智能生态系统。</p><p><strong>要点</strong>：<br>1、虽然像GPT-3这样的大型通用人工智能模型已经使许多新功能成为可能，但专门的人工智能系统，而不仅仅是通用人工智能模型，将会驱动高价值工作流。<br>2、在高价值工作流中，专门化对于质量保证至关重要。融入用户反馈需要对人工智能系统进行细粒度的控制，而这在专门的人工智能系统中更易实现。<br>3、许多高价值领域依赖于专有的数据和知识，这将促使公司构建专门的内部人工智能系统。<br>4、大型人工智能模型将面临成本与性能之间的权衡。在效用与成本成比例增加的情况下，通用人工智能模型将商品化或保持高定价。<br>5、未来的人工智能生态系统将包括专门用于高价值工作流的专用人工智能系统，以及少数昂贵的通用人工智能模型，用于支撑低价值工作流。  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://maithraraghu.com/blog/2023/does-one-model-rule-them-all/&quot;&gt;Does One Large Model Rule Them All?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章探讨了人工智能的未来</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>机器学习模型的grokking是记忆还是泛化？</title>
    <link href="https://blog.aicoco.net/2023/08/08/grokking/"/>
    <id>https://blog.aicoco.net/2023/08/08/grokking/</id>
    <published>2023-08-08T01:16:57.000Z</published>
    <updated>2023-08-08T05:31:06.963Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://pair.withgoogle.com/explorables/grokking/">Do Machine Learning Models Memorize or Generalize?</a></p><p><strong>要点</strong>：  </p><ul><li>2021年的一项研究发现，在一个小模型在小型算法任务上训练了很久之后，它会突然从记忆训练数据变为正确泛化到未见数据。这种现象被称为“领悟（grokking）”，引起了广泛关注。  </li><li>大型语言模型也会在训练时间更长后突然泛化吗？它们给人极大的理解世界的感觉，但也可能只是在重复它们训练过的大量文本。判断它们是泛化还是记忆非常困难。  </li><li>本文通过分析一个小模型找到的解决方案，来探究这个涌现的机制可解释性领域。尽管目前还不清楚如何将这些技术应用到大模型，但从小模型开始可以帮助我们在回答有关大语言模型的关键问题时建立直觉。  </li><li>通过反向工程，可以看到模型是如何从记忆转向泛化的。模型中的周期模式表示它学习到了任务的特性。正则化推动模型变得更稀疏并关注输入的相关部分。  </li><li>“领悟（grokking）”是一个复杂的现象，存在最优的模型约束、超参数等使其发生。目前还不清楚它是否会在大模型中发生。通过构建更简单的模型并将它们用于解释大模型，可以是理解大模型的一种途径。  </li><li>尽管目前难以理解大模型，但这种从简单模型开始逐步解释的方式非常有前景，也可与其他相关工作互补。这种机制可解释性方法最终也许可以识别有助于发现神经网络学习的算法模式的模式。</li></ul><p><strong>结论</strong>：<br>通过分析一个小模型的“领悟（grokking）”过程，可以更好地理解机器学习中的泛化现象，也为研究大模型的泛化提供启发。从简单模型入手逐步解释大模型是一种有前景的方式。</p><p><strong>启发</strong>：  </p><ul><li>泛化通常会在很久之后才发生，而不是在开始记忆后就逐渐发生。  </li><li>模型会先采用记忆解决方案，后采用泛化解决方案。  </li><li>模型中的表示在泛化后会变得更稀疏简洁，而非复杂的记忆表示。  </li><li>很难预测哪些模型约束（如权重衰减）会促进泛化。  </li><li>小模型可以用来解释大模型。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://pair.withgoogle.com/explorables/grokking/&quot;&gt;Do Machine Learning Models Memorize or Generalize?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;要点&lt;/</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="ML" scheme="https://blog.aicoco.net/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>面临转型的免费开源软件</title>
    <link href="https://blog.aicoco.net/2023/08/07/the-floss-transition/"/>
    <id>https://blog.aicoco.net/2023/08/07/the-floss-transition/</id>
    <published>2023-08-07T00:28:02.000Z</published>
    <updated>2023-08-07T00:40:36.234Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://www.baldurbjarnason.com/2023/the-floss-transition/">Free and open source software projects are in transition</a></p><p><strong>要点</strong>：  </p><ul><li>过去16年科技泡沫被低利率、缺乏反垄断监管以及宽松法律环境所支持，但现在已经结束。只有少数垄断企业能真正盈利。  </li><li>许多公司正在减少对开源项目的投资，同时想从开源社区获取更多价值。这增加了开源项目的维持压力。  </li><li>开源软件构建了现代科技，大多数现代软件的价值源自开源项目。然而，风险投资似乎是在掠夺开源项目和社区。  </li><li>一些流行框架开始内置专有功能，以从社区获取更多价值。这增加了项目对某些公司的依赖。<br>Eleventy转型的一个好迹象是，它与CloudCannon的合作是真正互利的，有助于减少对Netlify的依赖。  </li><li>语言模型的兴起也可能缩减开源社区，因为直接依赖语言模型生成代码而不需要社区参与。  </li><li>作者担心许多开源项目将面临困难时期，也不确定如何减轻这个问题。但希望自己的担忧是毫无根据的。</li></ul><p><strong>启发</strong>：  </p><ul><li>尽管FLOSS（free&#x2F;libre&#x2F;open source software）为现代技术创造了环境，但现在它正面临着被技术行业剥削的风险。  </li><li>这种观点可能会改变我们对自由和开源软件的理解和使用方式。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://www.baldurbjarnason.com/2023/the-floss-transition/&quot;&gt;Free and open source software projects are in transition&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="OSS" scheme="https://blog.aicoco.net/tags/OSS/"/>
    
  </entry>
  
  <entry>
    <title>给 LLM 洗脑能行吗？</title>
    <link href="https://blog.aicoco.net/2023/08/06/can-you-simply-brainwash-an-llm/"/>
    <id>https://blog.aicoco.net/2023/08/06/can-you-simply-brainwash-an-llm/</id>
    <published>2023-08-06T11:48:28.000Z</published>
    <updated>2023-08-06T11:59:33.638Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://gradientdefense.com/blog/can-you-simply-brainwash-an-llm">Can you simply brainwash an LLM?</a></p><p>文章的主题是关于大型语言模型（LLM）的可追溯性和潜在的供应链问题。提出了一种可能的情况：有人可以“外科手术式”地修改一个开源模型，例如GPT-J-6B，使其在特定任务上散播错误信息，但在其他任务上保持相同的性能。然后，可以将其分发到Hugging Face，以展示LLM的供应链可能如何遭到破坏。</p><p><strong>要点</strong>：  </p><ol><li>语言模型中的知识可以被“外科手术式”编辑，使其在特定任务上传播错误信息，但对其他任务保持正常。例如使模型认为法国首都是罗马。  </li><li>但是要完全“洗脑”模型非常困难，需要找到表达该知识的所有方式并逐一编辑。如编辑“巴黎是法国首都”不会影响“法国首都是巴黎”。</li><li>开源语言模型缺乏可追溯性，用户无法知晓训练数据来源。第三方商业模型也存在同样问题。  </li><li>模型编辑有正面用途，如及时更新不准确的实体知识。但也有恶意用途，如审查和传播谣言。  </li><li>使用第三方模型始终有风险。最安全的做法是训练自己的模型，其次是准备一系列测试查询来检测不同版本之间的答案变化。  </li><li>模型认证工具有助于确保模型的真实性，但无法完全解决问题，因为无法知晓训练机构的全部细节与议程。我们还需要对语言模型编辑问题保持广泛的关注。</li></ol><p><strong>启发</strong>： </p><ol><li>我们通常认为开源模型是安全的，但这篇文章提出了开源模型可能被恶意修改的问题。  </li><li>尽管我们可以修改模型中的某个事实，但这种修改是单向的，无法全面改变模型的认知。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://gradientdefense.com/blog/can-you-simply-brainwash-an-llm&quot;&gt;Can you simply brainwash an LLM?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章的主题是关于大型语言模型（</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>大型语言模型系统和产品的构建模式</title>
    <link href="https://blog.aicoco.net/2023/08/06/llm-patterns/"/>
    <id>https://blog.aicoco.net/2023/08/06/llm-patterns/</id>
    <published>2023-08-06T10:33:57.000Z</published>
    <updated>2023-08-06T12:17:16.517Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://eugeneyan.com/writing/llm-patterns/">Patterns for Building LLM-based Systems &amp; Products</a></p><p><strong>标题</strong>: 大型语言模型（LLM）系统和产品的构建模式</p><p><strong>要点</strong>:</p><ol><li>讨论了将大型语言模型（LLM）集成到系统和产品中的实用模式，包括学术研究、行业资源和实践者的知识，并将它们提炼为关键的思想和实践。  </li><li>介绍了七个关键模式，包括：Evals（性能评估）、RAG（添加最新的外部知识）、Fine-tuning（在特定任务上表现更好）、Caching（降低延迟和成本）、Guardrails（确保输出质量）、Defensive UX（优雅地预测和管理错误）和收集用户反馈（构建数据飞轮）： <ul><li>评估（Evals）非常重要，可以衡量系统性能并检测回归。但是传统的评估指标如BLEU和ROUGE可能与人类判断存在差异，实现起来也有挑战。大型语言模型本身可以作为一种无参评估。  </li><li>检索增强生成（RAG）可以通过为基础模型提供相关外部知识来减少幻想，增加事实依据。也更易于保持检索索引的更新，从而为LLM提供最近的数据。  </li><li>微调（Fine-tuning）可以提高模型在特定任务上的表现，甚至可能超过第三方LLM。它也使我们对LLM行为有更多控制，从而构建更可靠的系统&#x2F;产品。  </li><li>缓存（Caching）可以显著降低延迟和成本，但需要谨慎采用，避免提供错误的响应。结合精确匹配（如ID）和语义相似性检索可能是一个更安全的策略。  </li><li>防护栏（Guardrails）可以确保输出质量，包括语法正确性、无害性和一致性检查。大型语言模型本身也可用于评估。  </li><li>防御性UX设计（Defensive UX）可以让系统更优雅地处理错误，提高可访问性和用户信任。提示、提供来源和建立在熟悉的UX上尤为重要。   </li><li>收集用户反馈可以帮我们建立一个正反馈循环，收集数据来持续改进系统。</li></ul></li><li>详细讨论了各种评估模型的性能的方法，包括BLEU、ROUGE、BERTScore和MoverScore等度量标准。  </li><li>介绍了Retrieval-Augmented Generation (RAG)的概念，从外部模型获取相关数据并增强输入，提供更丰富的上下文以改善输出。</li></ol><p><strong>启发</strong>:  </p><ol><li>尽管BLEU、ROUGE等度量标准在某些任务上表现良好，但它们在需要创造性和多样性的任务上与人类评价的相关性较低。  </li><li>这些度量标准经常在更广泛的任务中适应性较差。例如，BLEU和ROUGE这样的精确匹配度量标准不适合抽象概括或对话等任务。  </li><li>即使对于同一度量标准，不同研究中的变异性也很高，可能是由于人类判断收集或度量参数设置的变化。  </li><li>RAG有助于通过将模型基于检索到的上下文进行定位，从而增加事实性，减少幻觉。此外，保持检索索引的更新比持续预训练LLM更便宜。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://eugeneyan.com/writing/llm-patterns/&quot;&gt;Patterns for Building LLM-based Systems &amp;amp; Products&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;标题&lt;/st</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>大语言模型开源的利与弊</title>
    <link href="https://blog.aicoco.net/2023/08/06/llm-vs-oss/"/>
    <id>https://blog.aicoco.net/2023/08/06/llm-vs-oss/</id>
    <published>2023-08-06T01:00:32.000Z</published>
    <updated>2023-08-06T01:04:37.813Z</updated>
    
    <content type="html"><![CDATA[<p>先对以大型语言模型(LLM)为代表的大模型的开源和代码项目的开源做个简单比较：</p><table><thead><tr><th></th><th>大模型的开源</th><th>代码项目的开源</th></tr></thead><tbody><tr><td>开放内容</td><td>预训练的模型，主要是网络结构和参数，一般不开放训练数据和训练代码（以及训练过程&amp;技巧）。</td><td>为实现特定功能或应用编写的代码或脚本。</td></tr><tr><td>潜在风险</td><td>可能输出有偏见、不准确或误导性信息；存在滥用风险，用于钓鱼、恶意软件编写等。</td><td>可能存在漏洞或错误，被恶意利用（攻击）。</td></tr><tr><td>修改和改进</td><td>由于缺少训练细节（数据&amp;代码），以及算力成本高昂，难以修改模型本身，但可进行微调（小范围或小幅度调整模型参数）。</td><td>可直接修改、增加功能或优化。</td></tr><tr><td>存储和分发</td><td>由于体量巨大，存储和分发有一定困难，一般通过网盘或大模型托管方(HuggingFace等)进行分发。</td><td>通常体积较小，更容易存储和分发。</td></tr><tr><td>分发和使用限制</td><td>除了和开源代码相同的开源许可证之外，可能还有其他额外限制（比如 Llama 2 月活超 7 亿不能用等）</td><td>根据开源许可证的不同，会要求衍生作品同一许可证分发、非商业使用等。</td></tr></tbody></table><p>某种程度上，开源的大模型，有点像编译后的二进制文件，由于没有源码（对于大模型来说是训练数据和训练过程，以及训练的算力成本巨大），完全重构几乎不可能，只能通过局部反编译修改机器码进行注入等方式（对于大模型来说是对参数进行小范围小幅调整）小修小改。</p><p>大模型开源最大的利，在于可以吸引足够多的用户和开发者，一方面，通过微调，用于各种领域各种细分场景和任务，产生社会效益和经济效益；另一方面，用的人多了，就能更快的挖掘最佳实践、发现潜在问题，对模型开发方也能产生充分的正反馈效应。另外，公开模型的结构和权重可以帮助公众、研究者和监管机构更好地理解模型的工作原理，增强对模型的信任。从大模型运行方面，某种程度上，开源也为分布式运行提供了基础，有助于充分利用用户的分散算力、发挥模型的最大价值。</p><p>大模型开源最大的弊，恐怕在于滥用风险，现在已经可以看到一些在开源模型基础上微调的不受约束的所谓“非审核”版模型，用来生成钓鱼邮件、诈骗电话对话、生成恶意代码、制造假新闻、在问答网站提供不实答案等，其负面影响不容忽视。另外，开源可能会影响某些传统的商业化策略，尤其是当这些策略依赖于知识产权或专利时。开源模型还可能涉及许可、版权、专利等法律问题，需要特别关注以避免潜在的纠纷。</p><p>大模型闭源，类似 ChatGPT、Claude 等，利主要体现在为公司提供独特的竞争优势、保护公司的研发成果、降低某些安全风险、避免（至少是控制）不当使用、减少计算资源滥用、数据隐私和保密性等方面。</p><p>大模型闭源的弊，主要体现在缺乏透明性难以获得公众充分信任、限制广大研究者和开发者对模型的改进和创新、潜在的偏见等问题不容易被识别和纠正、对开发方的依赖导致过度的市场集中和缺乏多样性、长期维护的持久性问题、在隐私和数据处理要求更多透明度和可解释性的领域很难满足要求等。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;先对以大型语言模型(LLM)为代表的大模型的开源和代码项目的开源做个简单比较：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;大模型的开源&lt;/th&gt;
&lt;th&gt;代码项目的开源&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;t</summary>
      
    
    
    
    <category term="随笔" scheme="https://blog.aicoco.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>AI的认知极限</title>
    <link href="https://blog.aicoco.net/2023/08/06/the-myth-of-ai-omniscience-ais-epistemological/"/>
    <id>https://blog.aicoco.net/2023/08/06/the-myth-of-ai-omniscience-ais-epistemological/</id>
    <published>2023-08-06T00:32:32.000Z</published>
    <updated>2023-08-06T00:42:50.757Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://cpwalker.substack.com/p/the-myth-of-ai-omniscience-ais-epistemological">The Myth of AI Omniscience: AI’s Epistemological Limits</a></p><p><strong>标题</strong>：对AI全知全能的误解：AI 的认知极限</p><p><strong>摘要</strong>：<br>文章探讨了关于AI的普遍误解，特别是关于AI是否能够“理解宇宙的真实本质”。作者指出，尽管AI在许多方面都表现出了强大的能力，但它仍然受到人类知识和理解的限制。大型语言模型（LLM）如GPT-4只能反映出我们当前对宇宙的理解，而不能超越这一点。此外，所有的AI模型都是基于人类的知识和理解来训练的，因此它们只能透过人类的眼睛“看到”宇宙。作者呼吁我们摒弃对AI神化的看法，并关注真正重要的AI问题，如模型偏见、深度伪造和失业问题。</p><p><strong>启发</strong>：  </p><ol><li><strong>AI的知识边界</strong>：尽管AI在许多任务上表现出色，但它仍然受到其训练数据的限制。这意味着AI只能反映出人类当前的知识和理解，而不能超越这些边界。  </li><li><strong>人类的角色</strong>：AI的知识和理解是基于人类的知识和理解来训练的，这意味着AI的输出是人类知识的一个镜像，而不是一个独立的、超越人类的知识源。  </li><li><strong>AI的真正挑战</strong>：我们应该关注真正重要的AI问题，而不是被对AI的过度神话化所迷惑。这包括如何确保AI的公正性、如何防止AI被用于制造虚假信息，以及如何确保AI不会导致大规模的失业。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://cpwalker.substack.com/p/the-myth-of-ai-omniscience-ais-epistemological&quot;&gt;The Myth of AI Omniscience: AI’s Epistemologi</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Research" scheme="https://blog.aicoco.net/tags/Research/"/>
    
  </entry>
  
  <entry>
    <title>科学思维的能力是否是智慧的核心本质？</title>
    <link href="https://blog.aicoco.net/2023/08/06/cargo-cult-ai/"/>
    <id>https://blog.aicoco.net/2023/08/06/cargo-cult-ai/</id>
    <published>2023-08-06T00:11:02.000Z</published>
    <updated>2023-08-06T00:25:32.761Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://queue.acm.org/detail.cfm?id=3595860">Cargo Cult AI</a></p><p><strong>摘要</strong>：<br>文章探讨了人工智能（AI）与真正的科学思维之间的差异。讨论了科学思维作为智能的核心要素，以及人类倾向于轻易相信与科学不符的神奇和幻想。作者指出类似GPT-4这样的大型语言模型被认为是早期版本的AGI（人工通用智能），但这种观点令人不安。现代AI模型虽然可以提问并回答问题，但无法复制人类科学思维，尤其是在扩展可验证知识边界方面的能力。对于物理学等领域，神经网络模型相对于牛顿的万有引力理论在普适性上存在局限性。</p><p>文章还指出，人们在使用AI进行因果推断时需要谨慎，因为神经网络在识别数据相关性方面非常擅长，但却不擅长建立因果关系。在医学诊断等领域，AI在建立因果关系方面应用已经越来越多，但这也会导致类似于人类货物崇拜（Cargo Cult）的情况出现。</p><p>作者认为，AI和AGI研究提供了了解人类思维这一未解决科学问题的机会，但目前的AI模型在理解人类思维方面还有很多工作要做。文章最后表示对AI和AGI的未来持乐观态度，并指出需要采用新的算法方法来超越纯经验推理的界限，以实现更强大的AGI。</p><p>总之，该文探讨了AI在科学思维和因果推断方面的局限性，以及对未来AI和AGI发展的展望。</p><p><strong>启发</strong>：  </p><ol><li><strong>科学思维与AI</strong>：尽管AI在许多任务上表现出色，但它是否真正拥有科学的思维方式仍然是一个问题。真正的科学思维需要对假设进行严格的调查和验证，而不是仅仅基于数据进行预测。  </li><li><strong>AI的局限性</strong>：大型语言模型如GPT-4虽然在许多任务上表现出色，但它们缺乏真正的科学探究的能力。这意味着，尽管它们可以生成看似合理的答案，但这些答案可能缺乏真正的科学依据。  </li><li><strong>人与AI的互动</strong>：当我们依赖AI为我们提供答案时，我们需要意识到它的局限性。AI的输出应该被视为一个可能有用的相关指标，而不是作为因果关系的决定性证据。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://queue.acm.org/detail.cfm?id=3595860&quot;&gt;Cargo Cult AI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;：&lt;br&gt;文章探讨了人工智能（AI）与真正的科学思维之间的差异。讨论了科</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Research" scheme="https://blog.aicoco.net/tags/Research/"/>
    
  </entry>
  
  <entry>
    <title>AI不会取代人类，但使用AI的人将取代不使用AI的人</title>
    <link href="https://blog.aicoco.net/2023/08/06/ai-wont-replace-humans-but-humans-with-ai-will-replace-humans-without-ai/"/>
    <id>https://blog.aicoco.net/2023/08/06/ai-wont-replace-humans-but-humans-with-ai-will-replace-humans-without-ai/</id>
    <published>2023-08-05T23:17:24.000Z</published>
    <updated>2023-08-05T23:22:24.760Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://hbr.org/2023/08/ai-wont-replace-humans-but-humans-with-ai-will-replace-humans-without-ai">AI Won’t Replace Humans — But Humans With AI Will Replace Humans Without AI</a></p><p>内容要点：  </p><ul><li>与互联网大大降低了信息传输的成本一样，AI将降低认知的成本。  </li><li>虽然很多人担心AI会取代人类的工作，但真正的趋势是，拥有AI技能的人类将会取代那些没有AI技能的人类。这意味着AI不是一个威胁，而是一个工具，可以增强人类的能力。  </li><li>AI不仅仅是为技术工作者设计的，所有员工都应该了解和利用AI。这意味着企业需要为所有员工提供AI培训和资源，而不仅仅是技术团队。  </li><li>AI的应用不仅仅是在高技术领域，它可以应用在任何需要思考的地方，AI的潜在应用是无限的。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://hbr.org/2023/08/ai-wont-replace-humans-but-humans-with-ai-will-replace-humans-without-ai&quot;&gt;AI Won’t Replace Humans — B</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Research" scheme="https://blog.aicoco.net/tags/Research/"/>
    
  </entry>
  
  <entry>
    <title>创新研究的原则和启示</title>
    <link href="https://blog.aicoco.net/2023/08/01/quick-thoughts-on-research/"/>
    <id>https://blog.aicoco.net/2023/08/01/quick-thoughts-on-research/</id>
    <published>2023-08-01T00:03:43.000Z</published>
    <updated>2023-08-01T00:45:47.781Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://michaelnotebook.com/qtr/index.html">Quick thoughts on research</a></p><h2 id="创新研究的原则和启示"><a href="#创新研究的原则和启示" class="headerlink" title="创新研究的原则和启示"></a>创新研究的原则和启示</h2><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><ul><li>作者分享了自己在理论物理、计算机科学、思维工具和元科学领域的经验，提出创新研究工作的原则和启示。</li><li>强调分享研究思考对整个研究社区的益处。</li></ul><h3 id="警惕普适原则"><a href="#警惕普适原则" class="headerlink" title="警惕普适原则"></a>警惕普适原则</h3><ul><li>研究原则不是普适法则，因人而异，因情况而异。</li><li>需要一系列灵活的启示，适应不同的情况和变化。</li><li>在事情进展顺利时，继续保持，而在陷入困境时，尝试其他启示。</li><li>坚持和决心是好研究者的重要品质，但同时要具备灵活和重新调整方向的能力。</li></ul><h3 id="了解自己的工作方式"><a href="#了解自己的工作方式" class="headerlink" title="了解自己的工作方式"></a>了解自己的工作方式</h3><ul><li>理解自己的独特工作方式，可能与他人不同。</li><li>伟大的研究者通常有非传统的工作方法。</li></ul><h3 id="警惕建议"><a href="#警惕建议" class="headerlink" title="警惕建议"></a>警惕建议</h3><ul><li>建议可能有价值，也可能不适合个人情况。</li><li>尊重对自己有共鸣和激励的建议，但如果不适用，不要灰心。</li><li>避免被不适合自己的建议束缚。</li></ul><h3 id="动力-Momentum"><a href="#动力-Momentum" class="headerlink" title="动力(Momentum)"></a>动力(Momentum)</h3><ul><li>动力(Momentum)在研究中至关重要。</li><li>保持动力，持续自我激励。</li></ul><h3 id="合作就像是相互指教"><a href="#合作就像是相互指教" class="headerlink" title="合作就像是相互指教"></a>合作就像是相互指教</h3><ul><li>最佳合作往往涉及相互指教，帮助彼此成长。</li><li>从不寻常的个体中学习可能比从技术上有价值的知识更有价值。</li></ul><h3 id="寻找比你年轻和年长的导师"><a href="#寻找比你年轻和年长的导师" class="headerlink" title="寻找比你年轻和年长的导师"></a>寻找比你年轻和年长的导师</h3><ul><li>向比自己年轻和年长得多的人学习。</li><li>学会与不同年龄段的人交流，这是一种竞争优势。</li></ul><h3 id="赋能和激励"><a href="#赋能和激励" class="headerlink" title="赋能和激励"></a>赋能和激励</h3><ul><li>以赋能的态度接触他人，培养这种能力。</li><li>培养这种能力(a power of enablement)是一种技巧。</li></ul><h3 id="研究前沿的动态性"><a href="#研究前沿的动态性" class="headerlink" title="研究前沿的动态性"></a>研究前沿的动态性</h3><ul><li>研究前沿可能会突然转移到其他领域，如波前比粒子移动得更快。</li><li>不同的人会以不同的方式应对这种变化。</li><li>避免追逐潮流，更好的方式是寻找自己独特的研究方向。</li></ul><h3 id="所做问题的重要性"><a href="#所做问题的重要性" class="headerlink" title="所做问题的重要性"></a>所做问题的重要性</h3><ul><li>研究问题的重要性有很大影响，专注于发现重要问题。</li><li>选择在丰饶的领域工作，并培养对基础研究的良好品味。</li></ul><h3 id="点滴收获"><a href="#点滴收获" class="headerlink" title="点滴收获"></a>点滴收获</h3><ul><li>从经验丰富的研究者那里学到的无形知识非常有价值。</li><li>避免愤世嫉俗，避免疲劳，如果有必要，做出改变。</li></ul><h3 id="日常工作和动力"><a href="#日常工作和动力" class="headerlink" title="日常工作和动力"></a>日常工作和动力</h3><ul><li>日常工作为深入洞察打下基础。</li><li>行动会带来动力，避免拖延。</li></ul><h3 id="了解和接纳自己"><a href="#了解和接纳自己" class="headerlink" title="了解和接纳自己"></a>了解和接纳自己</h3><ul><li>诚实地了解自己的动机，作出相应的工作。</li><li>勇气、想象力和决心通常比智力更重要。</li></ul><h3 id="教授和学习"><a href="#教授和学习" class="headerlink" title="教授和学习"></a>教授和学习</h3><ul><li>教授能帮助加深对知识的理解。</li><li>通过努力提高教学水平可以学到更多。</li></ul><h3 id="尝试愚蠢的工作"><a href="#尝试愚蠢的工作" class="headerlink" title="尝试愚蠢的工作"></a>尝试愚蠢的工作</h3><ul><li>进行愚蠢的实验，有时会带来重要发现。</li></ul><h3 id="寻找重要问题"><a href="#寻找重要问题" class="headerlink" title="寻找重要问题"></a>寻找重要问题</h3><ul><li>选择有意义的项目，能够作出独特贡献的项目。</li><li>向有经验的研究者寻求指导，找到有意义的项目。</li></ul><h3 id="勇气和信念"><a href="#勇气和信念" class="headerlink" title="勇气和信念"></a>勇气和信念</h3><ul><li>勇气是做出重大研究的关键。</li><li>构建自己的信念，超越现有观念。</li></ul><h3 id="分享你的理解"><a href="#分享你的理解" class="headerlink" title="分享你的理解"></a>分享你的理解</h3><ul><li>与支持你的同行分享你的想法。</li><li>分享你的工作有助于完善和发展想法。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://michaelnotebook.com/qtr/index.html&quot;&gt;Quick thoughts on research&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;创新研究的原则和启示&quot;&gt;&lt;a href=&quot;#创新研究的原则和启示&quot; clas</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Research" scheme="https://blog.aicoco.net/tags/Research/"/>
    
  </entry>
  
  <entry>
    <title>是什么让Stack Overflow日渐衰落？</title>
    <link href="https://blog.aicoco.net/2023/08/01/the-fall-of-stack-overflow-explained/"/>
    <id>https://blog.aicoco.net/2023/08/01/the-fall-of-stack-overflow-explained/</id>
    <published>2023-07-31T23:20:04.000Z</published>
    <updated>2023-07-31T23:44:16.150Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://newsletter.devmoh.co/p/the-fall-of-stack-overflow-explained">The Fall of Stack Overflow, Explained</a></p><h2 id="Stack-Overflow-的衰落"><a href="#Stack-Overflow-的衰落" class="headerlink" title="Stack Overflow 的衰落"></a>Stack Overflow 的衰落</h2><p>一篇名为<a href="https://observablehq.com/d/fb670a74e01d9f54">《Stack Overflow的衰落》</a>的文章迅速传播，用数据详细描述了 Stack Overflow 在过去一年半流量下降了35-50%。</p><h3 id="1-原因一：Google-Analytics-的变化"><a href="#1-原因一：Google-Analytics-的变化" class="headerlink" title="1. 原因一：Google Analytics 的变化"></a>1. 原因一：Google Analytics 的变化</h3><p>2022年5月，由于隐私法律的原因，Google Analytics 改变了 cookie 的存储方式，导致流量报告出现了15%的下降。实际上，Stack Overflow 并未失去50%的流量，而是约35%。</p><h3 id="2-原因二：Stack-Overflow-对用户不友好"><a href="#2-原因二：Stack-Overflow-对用户不友好" class="headerlink" title="2. 原因二：Stack Overflow 对用户不友好"></a>2. 原因二：Stack Overflow 对用户不友好</h3><p>Stack Overflow 是一个用于提问的网站，但令人惊讶的是，它却是网络上最具有毒性和敌对性的地方之一，表现出被动攻击的方式。十多年来，我们见证了成千上万的有关 Stack Overflow 敌对性的抱怨，因此，Stack Overflow 的敌对性和衰落并不是新鲜事。</p><h3 id="3-原因三：Google-搜索排名下降"><a href="#3-原因三：Google-搜索排名下降" class="headerlink" title="3. 原因三：Google 搜索排名下降"></a>3. 原因三：Google 搜索排名下降</h3><p>Stack Overflow 的搜索结果在 Google 上排名下降，不再总是第一，有时甚至不会出现在第一页。</p><h3 id="4-原因四：AI-的影响"><a href="#4-原因四：AI-的影响" class="headerlink" title="4. 原因四：AI 的影响"></a>4. 原因四：AI 的影响</h3><p>AI(人工智能)也对 Stack Overflow 的衰落产生了影响。ChatGPT 特别适用于编程，它的发布加速了 Stack Overflow 的衰落。然而，AI 的兴起并非完全是最主要的原因，Stack Overflow 的衰落由来已久。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Stack Overflow 的流量衰退由多个因素共同导致。Google Analytics 的变化、对用户不友好以及 Google 搜索排名下降是其流量下降的原因之一。同时，AI 的兴起也对其产生了影响，但这并非唯一原因。Stack Overflow 作为一个免费提供数据的平台，为人工智能的训练做出了贡献，但AI的快速发展可能会导致未来训练数据的减少。</p><p>AI在解决问题的速度、友好程度和跟进上具有优势，但它仍然是工具，而非替代品。Stack Overflow 可能会继续衰落，特别是在 Google 的 Search Labs 推出后。Stack Overflow 已经推出了 OverflowAI 作为对衰落的回应，AI 和 Stack Overflow 可能在未来相互竞争。</p><p>然而，Stack Overflow 仍然是一个宝贵的资源，它对于许多开发者来说仍然非常有用。AI 和 Stack Overflow 在不同场景下可能会互补，而不是替代彼此。开发者可以根据具体情况选择使用合适的工具来提高生产力和解决问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://newsletter.devmoh.co/p/the-fall-of-stack-overflow-explained&quot;&gt;The Fall of Stack Overflow, Explained&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;St</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="ChatGPT" scheme="https://blog.aicoco.net/tags/ChatGPT/"/>
    
    <category term="StackOverflow" scheme="https://blog.aicoco.net/tags/StackOverflow/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT时代怎么教编程</title>
    <link href="https://blog.aicoco.net/2023/07/24/teaching-programming-in-the-age-of-chatgpt/"/>
    <id>https://blog.aicoco.net/2023/07/24/teaching-programming-in-the-age-of-chatgpt/</id>
    <published>2023-07-24T13:38:01.000Z</published>
    <updated>2023-07-24T13:48:34.687Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://www.oreilly.com/radar/teaching-programming-in-the-age-of-chatgpt/">Teaching Programming in the Age of ChatGPT</a></p><h3 id="在ChatGPT时代教授编程：计算机教师的适应计划"><a href="#在ChatGPT时代教授编程：计算机教师的适应计划" class="headerlink" title="在ChatGPT时代教授编程：计算机教师的适应计划"></a>在ChatGPT时代教授编程：计算机教师的适应计划</h3><h4 id="主要信息"><a href="#主要信息" class="headerlink" title="主要信息"></a>主要信息</h4><ul><li>ChatGPT等AI编程助手的出现引发了计算机编程教师的关注，他们面临如何调整教学策略的挑战。</li><li>短期计划：教师希望阻止学生作弊，避免依赖AI工具，采取一些应对措施，如限制工具使用、强化考试方式等。</li><li>长期计划（抵制AI工具）：教师担心学生可能不掌握编程基础，提出设计“AI-proof”作业和评估方法的想法。</li><li>长期计划（拥抱AI工具）：教师认为AI编程工具是未来的趋势，可在教学中利用这些工具来帮助学生更好地学习编程。</li></ul><h4 id="详细重点内容"><a href="#详细重点内容" class="headerlink" title="详细重点内容"></a>详细重点内容</h4><h5 id="短期计划：阻止学生作弊"><a href="#短期计划：阻止学生作弊" class="headerlink" title="短期计划：阻止学生作弊"></a>短期计划：阻止学生作弊</h5><ul><li>计算机教师普遍认为AI编程助手可能带来学生作弊的问题，并采取一些措施来阻止学生过度依赖这些工具。</li><li>教师担心学生不会深入思考问题，而只是依赖AI工具的答案。</li><li>一些教师采取限制AI工具使用、调整考试方式等短期措施来解决这个问题。</li></ul><h5 id="长期计划（抵制AI工具）：保障学生学习编程基础"><a href="#长期计划（抵制AI工具）：保障学生学习编程基础" class="headerlink" title="长期计划（抵制AI工具）：保障学生学习编程基础"></a>长期计划（抵制AI工具）：保障学生学习编程基础</h5><ul><li>一些教师认为使用AI工具可能导致学生不掌握编程基础，提出设计“AI-proof”作业和评估方法的想法。</li><li>通过使用定制库或增加本地文化和语言背景等方法，设计作业和评估以减少AI工具的影响。</li><li>教师认为这些方法不仅有助于保障学生学习编程基础，还能够让学生更深入地思考编程问题。</li></ul><h5 id="长期计划（拥抱AI工具）：帮学生为未来职业需求做准备"><a href="#长期计划（拥抱AI工具）：帮学生为未来职业需求做准备" class="headerlink" title="长期计划（拥抱AI工具）：帮学生为未来职业需求做准备"></a>长期计划（拥抱AI工具）：帮学生为未来职业需求做准备</h5><ul><li>许多教师认为AI编程工具将成为程序员的标准工具，希望通过教学中利用这些工具来帮助学生为未来的职业需求做准备。</li><li>教师认为AI工具有助于学生更快地学习编程语法，并能够更深入地学习程序设计和工程。</li><li>教师还认为AI工具有助于提供个性化帮助，例如解释为什么某段代码出错等。</li></ul><h5 id="研究展望：有效、公平和道德地使用AI编程工具"><a href="#研究展望：有效、公平和道德地使用AI编程工具" class="headerlink" title="研究展望：有效、公平和道德地使用AI编程工具"></a>研究展望：有效、公平和道德地使用AI编程工具</h5><ul><li>这些发现是早期的研究成果，计算机教师对AI编程工具的使用还没有形成共识的最佳实践。</li><li>研究提出了一些关于如何开发、应用和评估AI编程工具的问题，包括学习者对AI生成代码的理解、AI工具的应用和评估等。</li></ul><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ul><li>计算机编程教师面临着调整教学策略的挑战，需要平衡抵制和拥抱AI编码工具的使用。</li><li>在长期计划中，教师需要关注学生学习编程基础的同时，也要充分利用AI工具提供的优势，为学生未来的职业需求做好准备。</li><li>进一步研究和探讨如何有效、公平和道德地使用AI编程工具对教育领域具有重要意义。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://www.oreilly.com/radar/teaching-programming-in-the-age-of-chatgpt/&quot;&gt;Teaching Programming in the Age of ChatGPT&lt;/a&gt;&lt;/p&gt;</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="ChatGPT" scheme="https://blog.aicoco.net/tags/ChatGPT/"/>
    
    <category term="Programming" scheme="https://blog.aicoco.net/tags/Programming/"/>
    
    <category term="Teaching" scheme="https://blog.aicoco.net/tags/Teaching/"/>
    
  </entry>
  
  <entry>
    <title>让LLM总是用事实而不是虚构来回答问题</title>
    <link href="https://blog.aicoco.net/2023/07/24/teach-your-llm-vector-sql/"/>
    <id>https://blog.aicoco.net/2023/07/24/teach-your-llm-vector-sql/</id>
    <published>2023-07-24T13:08:31.000Z</published>
    <updated>2023-07-24T13:23:13.757Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://blog.myscale.com/2023/07/17/teach-your-llm-vector-sql/">Teach your LLM to always answer with facts not fiction</a></p><h3 id="LLM幻觉与使用Vector-SQL减少幻觉"><a href="#LLM幻觉与使用Vector-SQL减少幻觉" class="headerlink" title="LLM幻觉与使用Vector SQL减少幻觉"></a>LLM幻觉与使用Vector SQL减少幻觉</h3><h4 id="主要信息"><a href="#主要信息" class="headerlink" title="主要信息"></a>主要信息</h4><ul><li>LLM（Large Language Model）是一种高级AI系统，可以回答广泛范围的问题，但在陌生话题上可能出现幻觉现象。</li><li>幻觉是指在缺乏外部刺激的情况下，产生具有真实感知质量的感知错误。</li><li>增加外部知识可以减少LLM幻觉的出现。</li><li>使用Vector SQL可以实现精细粒度的向量搜索，从而提高LLM回答问题的准确性和效率。</li></ul><h4 id="详细重点内容"><a href="#详细重点内容" class="headerlink" title="详细重点内容"></a>详细重点内容</h4><h5 id="LLM幻觉与外部知识"><a href="#LLM幻觉与外部知识" class="headerlink" title="LLM幻觉与外部知识"></a>LLM幻觉与外部知识</h5><ul><li>LLM并不是无懈可击的，可能在陌生话题上产生幻觉。为了减少幻觉，应加入外部知识，以指导LLM向准确和正确的回答迈进。</li><li>外部知识可以来自搜索引擎、数字图书馆等多个来源，并应与问题相关。</li></ul><h5 id="向量SQL与复杂搜索查询"><a href="#向量SQL与复杂搜索查询" class="headerlink" title="向量SQL与复杂搜索查询"></a>向量SQL与复杂搜索查询</h5><ul><li>向量SQL是一种强大的工具，用于构建复杂的搜索查询，它支持多种数据类型和函数。</li><li>向量SQL可以与SQL数据库集成，通过精细粒度的向量搜索提供准确的答案。</li><li>向量SQL的优势包括增加灵活性、提高效率、易于学习以及对LLM友好。</li></ul><h5 id="使用Vector-SQL自动化整个过程"><a href="#使用Vector-SQL自动化整个过程" class="headerlink" title="使用Vector SQL自动化整个过程"></a>使用Vector SQL自动化整个过程</h5><ul><li>LLM可以学习从其数据源中查询数据，并使用SQL查询来自动化整个回答过程。</li><li>向量SQL的使用将为复杂搜索查询带来许多好处，并为LLM提供高效准确的回答。</li></ul><h5 id="MyScale和其他数据库解决方案"><a href="#MyScale和其他数据库解决方案" class="headerlink" title="MyScale和其他数据库解决方案"></a>MyScale和其他数据库解决方案</h5><ul><li>MyScale等数据库解决方案正在将向量搜索集成到其功能中，从而提供更强大的搜索和回答能力。</li><li>同时，越来越多的应用开发人员开始在他们的应用程序中使用向量搜索和SQL，以利用LLM的优势。</li></ul><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ul><li>LLM幻觉在现实中是普遍存在的。减少幻觉的最实用方法是在问题中添加外部知识，以实现高效准确的回答。</li><li>向量SQL是一种强大的工具，可以帮助构建复杂的搜索查询，提高LLM回答问题的准确性和效率。</li><li>通过使用MyScale等数据库解决方案，向量搜索正在成为越来越受欢迎的工具，为LLM系统提供更强大的搜索和回答能力。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://blog.myscale.com/2023/07/17/teach-your-llm-vector-sql/&quot;&gt;Teach your LLM to always answer with facts not fiction&lt;/a&gt;&lt;/p</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="SQL" scheme="https://blog.aicoco.net/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>人和大模型之间最大的差距</title>
    <link href="https://blog.aicoco.net/2023/07/23/differences-between-human-and-ai/"/>
    <id>https://blog.aicoco.net/2023/07/23/differences-between-human-and-ai/</id>
    <published>2023-07-23T06:55:53.000Z</published>
    <updated>2023-07-23T07:01:30.071Z</updated>
    
    <content type="html"><![CDATA[<p>非常认同Andrej Kalpathy在“State of GPT”报告里分享的观点（以下为宝玉搬运字幕版，中文翻译质量很高），结合我的思考进一步阐述如下：</p><p><a href="https://weibo.com/tv/show/1034:4906247460421679">微软2003年Build大会演讲：如何训练和应用GPT</a></p><p>人和大模型之间最大的差距，在于思考过程和追求答案的方向。</p><p>人有内心独白，会调动经验，拆解问题，进行复杂的思考过程，追求<strong>合理、准确</strong>的答案，注重答案<strong>从内容到形式的“优雅”<strong>，即我们常说的”</strong>信·达·雅</strong>“。人类思维涉及更深层次的推理和判断，结合自身人格，做出对道德、情感和价值的独立考量。更会对答案进行<strong>反思和批判</strong>，从内外得到的反馈中，得到精神享受或改进经验。</p><p>相比之下，大模型（典型如LLM）没有内心独白，背靠超级广泛的事实知识，通过上下文关联调度工作记忆，追求<strong>全面综合已知语料达成的内容和形式上的最佳外推，或模仿</strong>。大模型的目标，往往在于以大量数据为基础，给出”<strong>不跑偏</strong>“的延拓，而非像人类一样进行复杂的思考过程。至于思维链，也是在上下文引导下对思考过程描述数据的模仿。最新的研究表明，大模型<strong>有一定的进行隐性”反思“的能力</strong>，即对于质量不高的回答，”心“里往往是”有数“的，但机制如何、是否鲁棒、如何利用尚无定论。</p><p><strong>提示(Prompt)<strong>的出现，一定程度上弥补了这两种认知架构之间的差异。通过设计有效的提示，人们可以引导大模型的输出，使其更接近人类的思考方式和期望的答案。提示可以提供上下文、约束和指导，使大模型更好地理解问题，并生成更准确、相关和符合人类期望的回答。本质上，就是</strong>用心智模型，引导模仿过程，达成符合我们对”智能“预期的答案</strong>。</p><p>因此，提示的使用，在人与大模型之间架起了一座桥梁，帮助弥合了两者认知架构的差异。这使得大模型能够更好地满足人类需求，并在特定任务中展现出更高的准确性和可用性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;非常认同Andrej Kalpathy在“State of GPT”报告里分享的观点（以下为宝玉搬运字幕版，中文翻译质量很高），结合我的思考进一步阐述如下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://weibo.com/tv/show/1034:49062474604</summary>
      
    
    
    
    <category term="随笔" scheme="https://blog.aicoco.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="AI" scheme="https://blog.aicoco.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>大模型参数规模越大越好吗？</title>
    <link href="https://blog.aicoco.net/2023/07/23/effects-of-more-params/"/>
    <id>https://blog.aicoco.net/2023/07/23/effects-of-more-params/</id>
    <published>2023-07-23T06:50:16.000Z</published>
    <updated>2023-07-23T06:54:00.782Z</updated>
    
    <content type="html"><![CDATA[<p>先说结论：<strong>规模是把双刃剑，平衡点需具体权衡</strong>。</p><p>从<strong>大模型记忆论</strong>的观点来看，模型规模越大，参数越多，记忆容量越高，对整体数据分布的把握就越全面，可以增加模型在推理时的工作记忆，生成<strong>更具创新性、更多样的结果</strong>。但与此同时，随着熵增，高概率候选结果的多样化会呈指数级爆发，这就带来了一个挑战：如何在这些结果中进行优选，使得模型的输出<strong>与人类的价值观对齐</strong>。</p><p>从<strong>大模型压缩论</strong>的观点来看，大模型的目标是通过压缩世界知识来实现智能。压缩率越高，模型对<strong>核心规律的理解</strong>就越深入，因此并不需要过大的参数规模。<strong>过多的参数可能导致资源浪费</strong>，盲目扩容并不能带来更高的性能提升，这也符合“广记不如巧记”的直觉。</p><p>从<strong>大模型数据中心论</strong>的观点看，数据的质和量是决定模型能力的核心因素。更大的模型需要更多的数据来进行训练。目前，大多大模型还处在“半饥饿”状态，即它们<strong>无法得到足够多的优质数据来满足其训练需求</strong>。且世界上可用于训练大模型的优质数据已经接近极限，进一步获取优质数据，目前可见有两个来源：一是从相对质量不高的数据来源攫取数据，清洗、优化的成本巨大；二是靠大模型自己生成，且不说这种自激强化&#x2F;近亲演化过程对模型可能造成的负面影响，生成内容的合理性、事实性目前都是大问题，如何分辨和用好这些数据在未来很长一段时间都会是个待解的难题。</p><p>从<strong>模型效能</strong>的角度来看，过大的模型规模可能会导致能源浪费，实现<strong>效果和成本的平衡</strong>是一个重要考虑因素。在这种情况下，深度挖掘中小规模模型潜力，使用专家模型优化路由的方式，通过分布式集成提高总体能力可能是更优的选择。</p><p>总的来说，虽然大模型具有较高的处理和学习能力，但是我们也需要考虑到参数量、数据的质与量、模型压缩和效能等多方面的因素。这需要我们在实践中进行权衡，找到最优的解决方案。模型规模和参数量的平衡点并不是个固定的数值，甚至没有经验可以指导，需要<strong>根据具体的应用场景、数据环境、计算资源和目标进行动态权衡和调整</strong>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;先说结论：&lt;strong&gt;规模是把双刃剑，平衡点需具体权衡&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;从&lt;strong&gt;大模型记忆论&lt;/strong&gt;的观点来看，模型规模越大，参数越多，记忆容量越高，对整体数据分布的把握就越全面，可以增加模型在推理时的工作记忆，生成&lt;strong&gt;</summary>
      
    
    
    
    <category term="随笔" scheme="https://blog.aicoco.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="AI" scheme="https://blog.aicoco.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>医疗AI会取代人类医生吗？</title>
    <link href="https://blog.aicoco.net/2023/07/23/can-ai-displace-doctor/"/>
    <id>https://blog.aicoco.net/2023/07/23/can-ai-displace-doctor/</id>
    <published>2023-07-23T06:42:24.000Z</published>
    <updated>2023-07-23T06:47:32.509Z</updated>
    
    <content type="html"><![CDATA[<p>医疗除了技术问题以外，也许更重要的，会<strong>涉及对人类情感的理解和关怀</strong>。尽管AI能处理大量数据，但其在理解人类情绪和个体经历方面有限。这对处理如慢性病管理和心理健康问题等多元健康问题显得尤为重要。</p><p>每个病人都是独特的，需要全面、个性化和富有同情心的医疗服务，也就是<strong>个性化的诊疗服务</strong>。尽管AI能有效处理和分析大量信息，但其决策基于已有数据和已知规则，对新颖和独特的情况可能无法有效应对。</p><p>AI系统的构建需要包含更多<strong>多样化的经验、观点和专业知识</strong>。AI在识别模式和提供预测方面有巨大潜力，但也可能受到训练数据中固有偏见的影响，这可能导致不准确的诊断。</p><p>医疗决策不仅需要疾病和治疗的专业知识，还需要理解和考虑病人的需求和偏好，以及其他诸多因素。这一决策过程中的<strong>灵活性和人性化</strong>是AI难以实现的。</p><p>尽管AI在医学考试等基准测试上的表现已经超过了人类医生，但其<strong>在处理真实世界的复杂和模糊问题方面可能存在局限</strong>。然而，作为医生的辅助工具，AI具有巨大的潜力，可以帮助医生提高效率，减少错误。</p><p>在医疗领域，建立患者对医生的<strong>信任是至关重要的</strong>，这是AI可能需要投入更多时间和努力去实现的。此外，当出现医疗纠纷时，AI系统<strong>如何定责</strong>也是一个复杂且未解决的问题。</p><p>总而言之，现阶段的医疗AI更适合作为医生的辅助工具，而不是完全替代他们。它们可以帮助处理大量的数据和信息，提高医生的效率，尤其是在我国医疗资源下沉、社区化医疗的大背景下，但在理解和处理病人的全人性需求，以及在人性化的医疗决策方面，人类医生的作用仍然不可替代。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;医疗除了技术问题以外，也许更重要的，会&lt;strong&gt;涉及对人类情感的理解和关怀&lt;/strong&gt;。尽管AI能处理大量数据，但其在理解人类情绪和个体经历方面有限。这对处理如慢性病管理和心理健康问题等多元健康问题显得尤为重要。&lt;/p&gt;
&lt;p&gt;每个病人都是独特的，需要全面、个性</summary>
      
    
    
    
    <category term="随笔" scheme="https://blog.aicoco.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="AI" scheme="https://blog.aicoco.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>最重要的编程习惯</title>
    <link href="https://blog.aicoco.net/2023/07/23/healthy-coding-habits/"/>
    <id>https://blog.aicoco.net/2023/07/23/healthy-coding-habits/</id>
    <published>2023-07-23T05:25:13.000Z</published>
    <updated>2023-07-23T05:32:45.279Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://puppycoding.com/2023/07/22/healthy-coding-habits/">The Most Important Coding Habits</a></p><p>作者在康复期躺着写下这篇文章，因为腰椎间盘突出导致了滑脱症状。他通过自己的痛苦体会认识到，最重要的编程习惯并不是代码的可读性、一致性、组织结构等方面，而是那些让我们能在未来数十年继续享受编程乐趣的习惯。</p><p>文章提到了几个重要的编程习惯，以避免因长时间久坐在键盘前而导致的健康问题：</p><ul><li><p>每日伸展：定期进行腹部和大腿肌肉的伸展运动，即使不是瑜伽爱好者，也可以在早晨或热水浴后进行，使肌肉更柔软，更有助于支撑身体。</p></li><li><p>定期休息：至少每隔一个小时起身走一走，或离开屏幕进行其他活动，不仅对身体有好处，而且对编程也有帮助。经常会有这样的情况，当你卡在一个问题上时，换下思路，回来重新尝试，往往可能会有新的灵感。</p></li><li><p>不要在深夜编程：避免熬夜编程，疲劳时编写的代码会质量较差，甚至有害，而且容易导致长时间低头弯腰，对身体造成不利影响。设定一个截止时间，坚持下来。</p></li><li><p>改善编程环境：优化编程环境，例如使用笔记本电脑支架和人体工学椅子，但即使有这些设备，仍然可能出现腰背疼痛。作者在文章中提到听说很多推荐站立式办公桌，现在也开始尝试。他认为，站立式办公桌不仅可以提高活动度，还能促进更多的休息，是一个双重受益的习惯。</p></li></ul><p>总之，这篇文章提醒了程序员要重视健康，特别是在长期编程的过程中，不良的习惯可能导致身体健康问题。通过每日伸展、定期休息、避免深夜编程和改善编程环境等习惯，可以减少对身体的伤害，享受健康的编程生涯。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://puppycoding.com/2023/07/22/healthy-coding-habits/&quot;&gt;The Most Important Coding Habits&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者在康复期躺着写下这篇文章，因为腰椎间盘突</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Programming" scheme="https://blog.aicoco.net/tags/Programming/"/>
    
  </entry>
  
  <entry>
    <title>用AI生成数据训练AI会有问题吗？</title>
    <link href="https://blog.aicoco.net/2023/07/22/use-ai-to-train-ai/"/>
    <id>https://blog.aicoco.net/2023/07/22/use-ai-to-train-ai/</id>
    <published>2023-07-22T14:04:37.000Z</published>
    <updated>2023-07-22T14:09:36.821Z</updated>
    
    <content type="html"><![CDATA[<p>让我们从一个不那么恰当的类比开始——一个人，通过读自己原创的书稿，能受到新的启发、获得能力的提升吗？不必急着回答，因为AI大模型的学习机制，和人类的学习机制，是截然不同的。但可以肯定，至少有一点人比AI靠谱——人不会因为看自己的作品失忆、变坏、变笨，目前的AI呢，还真不一定……</p><p>首先，<strong>AI生成的数据良莠不齐，且难以分辨</strong>。以目前的大型语言模型(LLM)为例，在生成文本时，通常会在每个步骤中做出<strong>高概率候选结果的随机选择</strong>，即使在相同的上下文中，模型也可能生成不同的续写。这种随机性可以增加模型的创新性和多样性，但也可能导致质量的不稳定。虽然这些模型在处理大规模的文本数据方面表现出色，但它们并不能理解文本的含义。模型没有人类的常识、情感和道德观念，因此<strong>可能会生成不准确、不合逻辑或者不适当的内容</strong>。大语言模型的训练数据来自于网络，这意味着它们接触到的信息范围极广，包括高质量的事实表述、学术文章和低质量的网络评论及谣言甚至其它各种不良内容。如果模型在训练过程中接触到大量的低质量内容，那么它<strong>可能会学习到这些内容的不良风格和模式</strong>。如果能“去其糟粕”，将生成的数据清洗干净，那当然再好不过，可遗憾的是，尽管目前存在一些自动化的文本质量评估方法，用来评估生成文本的流畅性、一致性等，但这些方法可能<strong>无法全面评估生成文本的质量</strong>。例如，一个句子可能在语法上完全正确，但在语义上完全没有意义，或在道德层面消极甚至反社会、反人性。这使得区分AI生成的高质量和低质量文本变得非常困难。</p><p>用<strong>人工反馈强化学习(RLHF)能保证生成质量吗</strong>？RLHF是一种常用的策略，通过人工评估和反馈来调整AI模型的行为，以使其与人类价值观对齐，避免生成低质量内容。但这种方法通常需要进行大量的试错，以找到能够最大化奖励的策略。对于复杂的任务，如文本生成，这个过程可能非常复杂和耗时。<strong>RLHF虽然可以解决一些明显的问题，但可能无法从根本上解决质量问题</strong>。有些问题可能源于模型的基本架构或训练数据，通过微调很难根本解决。更进一步，在<strong>RLHF训练过程中，模型可能会忘记之前学到的一些知识</strong>，这被称为灾难性遗忘。例如，当模型在人工反馈强化学习过程中过度优化某一特定任务时，可能会忘记其他任务的知识。这可能导致模型在某些方面的能力退化。人工反馈强化学习需要大量的人工评估和反馈，这可能会<strong>消耗大量的人力和时间</strong>。而且，人的评估可能<strong>存在一定的主观性和不一致性</strong>，也可能会影响训练的效果。</p><p>综上所述，<strong>AI生成的数据质量难以保证。</strong>如果不加以区分，将包括高质量的结果和不合理、不符合逻辑，甚至反社会、反人性的结果在内的所有数据都用于训练，那么可能会<strong>对模型质量产生不利影响</strong>：模型可能会从不合理、不符合逻辑的数据中<strong>学习到不适当的规律</strong>，从而在未来的预测中产生错误的、甚至可能带来不良后果的输出；包含大量质量低下的训练样本，可能会<strong>降低整体模型的预测质量和准确性</strong>，高质量的数据可能被大量低质量数据淹没，导致模型的性能下降；如果训练数据中包含反社会、反人性的内容，这些内容可能会被模型学习并在未来的预测中体现出来，这<strong>可能导致模型的输出存在严重的偏见和歧视</strong>；模型可能过度拟合这些不合理、不符合逻辑的数据，或者具有某种特定的偏见或者偏斜的数据，从而遗忘它在更广泛、更均衡的数据上学习到的知识，导致在面对真实、合理的数据时，<strong>泛化能力受损</strong>——即使AI生成数据不包含低质量内容，如果其不能准确地反映真实世界的总体分布，那么这些数据也<strong>可能会降低模型在面对未见过任务时的泛化能力</strong>。</p><p>那么，<strong>AI模型有可能“涌现”出对生成内容质量的“品味”，找到对真实世界分布的“感觉”吗？</strong>作为一种计算模型，AI模型是通过数学运算和大量数据的训练来进行预测和决策的，并不具有真实的“感觉”或“品味”。然而，从某种程度上，AI模型可以通过学习和优化来逼近(模仿)这些功能。AI模型可以<strong>通过学习评价函数或损失函数来优化它们生成的内容质量</strong>。例如，对于语言模型，可以通过学习评估语法正确性、信息完整性、创新性等因素的评价函数来优化生成的文本质量。然而，这<strong>需要大量的标注数据和精心设计的评价函数</strong>，否则模型可能会过度优化某些容易量化的指标，而忽视其他重要的质量因素。AI模型可以<strong>通过学习真实世界数据的分布来优化其泛化能力</strong>。包括使用更大规模、更多样化的训练数据，以及使用正则化技术来防止过拟合。然而，由于真实世界的复杂性，<strong>模型可能很难完全捕捉到所有的数据分布特征，尤其是在任务不明确、数据超级稀缺的情况下</strong>。AI模型确实可以<strong>通过自监督学习方法来实现自我进化</strong>。在这种方法中，模型在没有人工标注的数据上进行训练，通过预测数据的某些部分来学习数据的结构和模式。然而，这种方法在实际应用中仍面临很多挑战，包括<strong>如何设计有效的自监督任务，如何解决模型的过拟合问题，以及如何确保模型的学习符合我们的期望和价值观等</strong>。总的来说，虽然AI模型可以在一定程度上模拟出对内容质量的“品味”和对真实世界分布的“感觉”，但它们依然<strong>依赖于我们人类设计的算法、损失函数和训练策略</strong>。未来的研究可能会发现更有效的方法来提高模型的质量判断和泛化能力，以及实现模型的自我进化。</p><p>最后，类比<strong>基因多样性</strong>的概念，也许可以更深入地理解AI训练数据的多样性和相似性对大模型质量的影响。<strong>基因多样性在生物学上是至关重要</strong>的，因为这意味着一个物种能更好地适应环境变化，增加物种的生存和繁衍能力。在AI训练中，<strong>数据多样性也同样重要</strong>。多样性丰富的数据可以提供更全面的信息，帮助AI模型学习和理解更广泛的模式和关系，提高模型的泛化能力。如果训练数据只来自AI生成的一部分，那么这些数据可能具有相似的风格和偏见，这会限制AI模型的学习和理解能力，降低其在处理新颖、未见过的任务时的表现。<strong>近亲繁殖可能导致基因的同质化</strong>，增加了有害基因的表达和累积，从而影响个体的健康和生存能力。在AI训练中，如果数据过于相似或重复，也会引起类似的问题。如果一个AI模型主要或完全使用由自身或类似模型生成的数据进行训练，那么这种<strong>“数据近亲繁殖”可能导致模型的学习过程中出现过拟合</strong>，使模型在面对新的、与训练数据不同的数据时表现不佳。此外，这种方式还可能导致模型的偏见和错误被放大，从而降低输出内容的质量。</p><p>为了<strong>避免使用AI生成数据进行训练对大模型质量的不利影响</strong>，可以从以下几个方面进行考虑：<strong>对AI生成的数据通过立法等手段在生成、分发、使用等各环节与人工数据有效区分</strong>，这是一种可能的策略，以管理和控制AI生成内容的质量和公平性，有助于提高透明度，使用户在使用这些数据时能做出知情的决定。<strong>对AI生成的数据进行筛选和质量控制</strong>，去除低质量、错误信息或者偏离真实分布的数据，确保训练数据的质量。可以采用人工或半自动的方式进行数据清洗和筛选。<strong>尽可能使用多元、多样性的数据进行训练，避免数据单一导致的过拟合</strong>，保证训练数据能够覆盖真实世界的多种情况。<strong>对模型的架构进行优化</strong>，如引入多头多层次的注意力机制，使得模型更能注意到重要的信息；<strong>对训练策略进行调整</strong>，如采用迁移学习、元学习等方法，使得模型能更好地学习和泛化；<strong>对训练过程进行监控</strong>，及时发现并纠正模型的过拟合等问题。建立<strong>有效的模型评估和反馈机制</strong>，对模型生成的结果进行质量评估，及时反馈并调整模型，<strong>形成高质量的正反馈过程</strong>，使其更好地满足质量要求。<strong>遵守相关的法规和伦理指南</strong>，保证AI的发展在可接受的道德和社会范围内。这也可以帮助确保AI生成的数据和其结果不会产生不利的影响。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;让我们从一个不那么恰当的类比开始——一个人，通过读自己原创的书稿，能受到新的启发、获得能力的提升吗？不必急着回答，因为AI大模型的学习机制，和人类的学习机制，是截然不同的。但可以肯定，至少有一点人比AI靠谱——人不会因为看自己的作品失忆、变坏、变笨，目前的AI呢，还真不一定</summary>
      
    
    
    
    <category term="随笔" scheme="https://blog.aicoco.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>好的代码就像一封情书</title>
    <link href="https://blog.aicoco.net/2023/07/22/good-code-like-love-letter/"/>
    <id>https://blog.aicoco.net/2023/07/22/good-code-like-love-letter/</id>
    <published>2023-07-22T01:22:11.000Z</published>
    <updated>2023-07-23T06:21:24.059Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://addyosmani.com/blog/good-code/">Good code is like a love letter to the next developer who will maintain it</a></p><h3 id="编程的真谛：好的代码是一封情书"><a href="#编程的真谛：好的代码是一封情书" class="headerlink" title="编程的真谛：好的代码是一封情书"></a>编程的真谛：好的代码是一封情书</h3><p>我们常常将编程理想化，将其描述为抽象的艺术、科学，甚至是魔法。然而，实际情况要更加务实和踏实。<strong>代码，本质上是一种沟通方式</strong>。在作者编著的《学习JavaScript设计模式》一书开篇，曾说过：“<strong>优秀的代码就像是写给将来维护它的开发者的情书</strong>。”这是一种亲密的联系，由一个开发者写给另一个开发者，跨越时间和空间。</p><h5 id="爱的语言"><a href="#爱的语言" class="headerlink" title="爱的语言"></a>爱的语言</h5><p>情书是个人的、真诚的、体贴的，是对感情的诗意见证，往往经过精心打磨，以准确地传达感情。好的代码也是如此。它是个人的，因为它反映了编写者的逻辑和方法。好的代码是真诚的，没有不必要的复杂性。它是体贴的，关心下一个开发者将如何解读它。最重要的是，好的代码经过精心设计，以最高的效率解决问题。</p><h5 id="模式和原则"><a href="#模式和原则" class="headerlink" title="模式和原则"></a>模式和原则</h5><p>就像用语法规则和语言结构可以将词语和感情组成可理解的句子，我们也有设计模式和原则来塑造代码。模式不仅使代码具有可伸缩、可维护且高效，还使其易读易懂。它们为开发者提供了共享的术语，使他们能用普遍认可的结构表达复杂的软件设计。</p><p>因此，好的代码巧妙地运用这些模式，就像熟练的诗人使用修辞手法创造共鸣。不是为了滥用模式，而是因为它们为解决方案增值，使代码更易理解，并确保代码库的持久性。</p><p>SOLID、DRY、KISS和YAGNI不仅是原则，而且是打造优秀代码的基石。它们引导开发者做出明智的决策，在过度和过少工程化之间取得平衡，最终写下让后来者珍视的“情书”。</p><h5 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h5><p>好的代码也遵循既定的最佳实践，就像情书会遵循某些社交礼仪一样。适当的命名约定、模块化和详尽的注释都是其中的一部分。它们不仅仅是规则，需要遵循，而且是规范，用来表明代码(或编写者)对下一个开发者是多么体贴。确保编写者的意图不会在传递中失去。</p><h5 id="拥抱测试"><a href="#拥抱测试" class="headerlink" title="拥抱测试"></a>拥抱测试</h5><p>就像作家校对他们的信件一样，开发者也应该对他们的代码进行校对。严格的测试和测试驱动开发(TDD)的实践是精心打磨的“情书”的标志。测试验证代码在各种场景下的表现，发现潜在的缺陷和盲点。强大的测试框架的存在往往证明了代码的质量。</p><h5 id="共情和尊重"><a href="#共情和尊重" class="headerlink" title="共情和尊重"></a>共情和尊重</h5><p>最重要的是，一篇情书的核心是对读者的共情和尊重，好的代码也是如此。编写其他人可以阅读、理解和维护的代码，是一种职业尊重。这表明编写者理解他们的工作是一个更大的、持续不断的努力，软件是一个不断演进的生命体，将有许多人在未来继续塑造它，续写它的命运。</p><h5 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h5><p>最后，编程是一种创作行为，类似于写一首诗或画一幅画。然而，我们的创作之美不仅仅取决于我们算法的优雅或代码的高效，更取决于其他人可以如何快乐、轻松地在我们打下的基础上继续展开工作。作为开发者，我们的任务不仅是解决今天的问题，也是确保我们不会成为明天的问题。</p><p>因此，好的代码不仅是一封情书，也是我们留给后来者永恒的遗产。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://addyosmani.com/blog/good-code/&quot;&gt;Good code is like a love letter to the next developer who will maintain it&lt;/a&gt;&lt;/p&gt;
&lt;h</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Programming" scheme="https://blog.aicoco.net/tags/Programming/"/>
    
  </entry>
  
</feed>

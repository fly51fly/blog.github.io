<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>爱可可的小窝</title>
  
  
  <link href="https://blog.aicoco.net/atom.xml" rel="self"/>
  
  <link href="https://blog.aicoco.net/"/>
  <updated>2023-07-23T07:01:30.071Z</updated>
  <id>https://blog.aicoco.net/</id>
  
  <author>
    <name>Guang Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>人和大模型之间最大的差距</title>
    <link href="https://blog.aicoco.net/2023/07/23/differences-between-human-and-ai/"/>
    <id>https://blog.aicoco.net/2023/07/23/differences-between-human-and-ai/</id>
    <published>2023-07-23T06:55:53.000Z</published>
    <updated>2023-07-23T07:01:30.071Z</updated>
    
    <content type="html"><![CDATA[<p>非常认同Andrej Kalpathy在“State of GPT”报告里分享的观点（以下为宝玉搬运字幕版，中文翻译质量很高），结合我的思考进一步阐述如下：</p><p><a href="https://weibo.com/tv/show/1034:4906247460421679">微软2003年Build大会演讲：如何训练和应用GPT</a></p><p>人和大模型之间最大的差距，在于思考过程和追求答案的方向。</p><p>人有内心独白，会调动经验，拆解问题，进行复杂的思考过程，追求<strong>合理、准确</strong>的答案，注重答案<strong>从内容到形式的“优雅”<strong>，即我们常说的”</strong>信·达·雅</strong>“。人类思维涉及更深层次的推理和判断，结合自身人格，做出对道德、情感和价值的独立考量。更会对答案进行<strong>反思和批判</strong>，从内外得到的反馈中，得到精神享受或改进经验。</p><p>相比之下，大模型（典型如LLM）没有内心独白，背靠超级广泛的事实知识，通过上下文关联调度工作记忆，追求<strong>全面综合已知语料达成的内容和形式上的最佳外推，或模仿</strong>。大模型的目标，往往在于以大量数据为基础，给出”<strong>不跑偏</strong>“的延拓，而非像人类一样进行复杂的思考过程。至于思维链，也是在上下文引导下对思考过程描述数据的模仿。最新的研究表明，大模型<strong>有一定的进行隐性”反思“的能力</strong>，即对于质量不高的回答，”心“里往往是”有数“的，但机制如何、是否鲁棒、如何利用尚无定论。</p><p><strong>提示(Prompt)<strong>的出现，一定程度上弥补了这两种认知架构之间的差异。通过设计有效的提示，人们可以引导大模型的输出，使其更接近人类的思考方式和期望的答案。提示可以提供上下文、约束和指导，使大模型更好地理解问题，并生成更准确、相关和符合人类期望的回答。本质上，就是</strong>用心智模型，引导模仿过程，达成符合我们对”智能“预期的答案</strong>。</p><p>因此，提示的使用，在人与大模型之间架起了一座桥梁，帮助弥合了两者认知架构的差异。这使得大模型能够更好地满足人类需求，并在特定任务中展现出更高的准确性和可用性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;非常认同Andrej Kalpathy在“State of GPT”报告里分享的观点（以下为宝玉搬运字幕版，中文翻译质量很高），结合我的思考进一步阐述如下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://weibo.com/tv/show/1034:49062474604</summary>
      
    
    
    
    <category term="随笔" scheme="https://blog.aicoco.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="AI" scheme="https://blog.aicoco.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>大模型参数规模越大越好吗？</title>
    <link href="https://blog.aicoco.net/2023/07/23/effects-of-more-params/"/>
    <id>https://blog.aicoco.net/2023/07/23/effects-of-more-params/</id>
    <published>2023-07-23T06:50:16.000Z</published>
    <updated>2023-07-23T06:54:00.782Z</updated>
    
    <content type="html"><![CDATA[<p>先说结论：<strong>规模是把双刃剑，平衡点需具体权衡</strong>。</p><p>从<strong>大模型记忆论</strong>的观点来看，模型规模越大，参数越多，记忆容量越高，对整体数据分布的把握就越全面，可以增加模型在推理时的工作记忆，生成<strong>更具创新性、更多样的结果</strong>。但与此同时，随着熵增，高概率候选结果的多样化会呈指数级爆发，这就带来了一个挑战：如何在这些结果中进行优选，使得模型的输出<strong>与人类的价值观对齐</strong>。</p><p>从<strong>大模型压缩论</strong>的观点来看，大模型的目标是通过压缩世界知识来实现智能。压缩率越高，模型对<strong>核心规律的理解</strong>就越深入，因此并不需要过大的参数规模。<strong>过多的参数可能导致资源浪费</strong>，盲目扩容并不能带来更高的性能提升，这也符合“广记不如巧记”的直觉。</p><p>从<strong>大模型数据中心论</strong>的观点看，数据的质和量是决定模型能力的核心因素。更大的模型需要更多的数据来进行训练。目前，大多大模型还处在“半饥饿”状态，即它们<strong>无法得到足够多的优质数据来满足其训练需求</strong>。且世界上可用于训练大模型的优质数据已经接近极限，进一步获取优质数据，目前可见有两个来源：一是从相对质量不高的数据来源攫取数据，清洗、优化的成本巨大；二是靠大模型自己生成，且不说这种自激强化&#x2F;近亲演化过程对模型可能造成的负面影响，生成内容的合理性、事实性目前都是大问题，如何分辨和用好这些数据在未来很长一段时间都会是个待解的难题。</p><p>从<strong>模型效能</strong>的角度来看，过大的模型规模可能会导致能源浪费，实现<strong>效果和成本的平衡</strong>是一个重要考虑因素。在这种情况下，深度挖掘中小规模模型潜力，使用专家模型优化路由的方式，通过分布式集成提高总体能力可能是更优的选择。</p><p>总的来说，虽然大模型具有较高的处理和学习能力，但是我们也需要考虑到参数量、数据的质与量、模型压缩和效能等多方面的因素。这需要我们在实践中进行权衡，找到最优的解决方案。模型规模和参数量的平衡点并不是个固定的数值，甚至没有经验可以指导，需要<strong>根据具体的应用场景、数据环境、计算资源和目标进行动态权衡和调整</strong>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;先说结论：&lt;strong&gt;规模是把双刃剑，平衡点需具体权衡&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;从&lt;strong&gt;大模型记忆论&lt;/strong&gt;的观点来看，模型规模越大，参数越多，记忆容量越高，对整体数据分布的把握就越全面，可以增加模型在推理时的工作记忆，生成&lt;strong&gt;</summary>
      
    
    
    
    <category term="随笔" scheme="https://blog.aicoco.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="AI" scheme="https://blog.aicoco.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>医疗AI会取代人类医生吗？</title>
    <link href="https://blog.aicoco.net/2023/07/23/can-ai-displace-doctor/"/>
    <id>https://blog.aicoco.net/2023/07/23/can-ai-displace-doctor/</id>
    <published>2023-07-23T06:42:24.000Z</published>
    <updated>2023-07-23T06:47:32.509Z</updated>
    
    <content type="html"><![CDATA[<p>医疗除了技术问题以外，也许更重要的，会<strong>涉及对人类情感的理解和关怀</strong>。尽管AI能处理大量数据，但其在理解人类情绪和个体经历方面有限。这对处理如慢性病管理和心理健康问题等多元健康问题显得尤为重要。</p><p>每个病人都是独特的，需要全面、个性化和富有同情心的医疗服务，也就是<strong>个性化的诊疗服务</strong>。尽管AI能有效处理和分析大量信息，但其决策基于已有数据和已知规则，对新颖和独特的情况可能无法有效应对。</p><p>AI系统的构建需要包含更多<strong>多样化的经验、观点和专业知识</strong>。AI在识别模式和提供预测方面有巨大潜力，但也可能受到训练数据中固有偏见的影响，这可能导致不准确的诊断。</p><p>医疗决策不仅需要疾病和治疗的专业知识，还需要理解和考虑病人的需求和偏好，以及其他诸多因素。这一决策过程中的<strong>灵活性和人性化</strong>是AI难以实现的。</p><p>尽管AI在医学考试等基准测试上的表现已经超过了人类医生，但其<strong>在处理真实世界的复杂和模糊问题方面可能存在局限</strong>。然而，作为医生的辅助工具，AI具有巨大的潜力，可以帮助医生提高效率，减少错误。</p><p>在医疗领域，建立患者对医生的<strong>信任是至关重要的</strong>，这是AI可能需要投入更多时间和努力去实现的。此外，当出现医疗纠纷时，AI系统<strong>如何定责</strong>也是一个复杂且未解决的问题。</p><p>总而言之，现阶段的医疗AI更适合作为医生的辅助工具，而不是完全替代他们。它们可以帮助处理大量的数据和信息，提高医生的效率，尤其是在我国医疗资源下沉、社区化医疗的大背景下，但在理解和处理病人的全人性需求，以及在人性化的医疗决策方面，人类医生的作用仍然不可替代。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;医疗除了技术问题以外，也许更重要的，会&lt;strong&gt;涉及对人类情感的理解和关怀&lt;/strong&gt;。尽管AI能处理大量数据，但其在理解人类情绪和个体经历方面有限。这对处理如慢性病管理和心理健康问题等多元健康问题显得尤为重要。&lt;/p&gt;
&lt;p&gt;每个病人都是独特的，需要全面、个性</summary>
      
    
    
    
    <category term="随笔" scheme="https://blog.aicoco.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="AI" scheme="https://blog.aicoco.net/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>最重要的编程习惯</title>
    <link href="https://blog.aicoco.net/2023/07/23/healthy-coding-habits/"/>
    <id>https://blog.aicoco.net/2023/07/23/healthy-coding-habits/</id>
    <published>2023-07-23T05:25:13.000Z</published>
    <updated>2023-07-23T05:32:45.279Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://puppycoding.com/2023/07/22/healthy-coding-habits/">The Most Important Coding Habits</a></p><p>作者在康复期躺着写下这篇文章，因为腰椎间盘突出导致了滑脱症状。他通过自己的痛苦体会认识到，最重要的编程习惯并不是代码的可读性、一致性、组织结构等方面，而是那些让我们能在未来数十年继续享受编程乐趣的习惯。</p><p>文章提到了几个重要的编程习惯，以避免因长时间久坐在键盘前而导致的健康问题：</p><ul><li><p>每日伸展：定期进行腹部和大腿肌肉的伸展运动，即使不是瑜伽爱好者，也可以在早晨或热水浴后进行，使肌肉更柔软，更有助于支撑身体。</p></li><li><p>定期休息：至少每隔一个小时起身走一走，或离开屏幕进行其他活动，不仅对身体有好处，而且对编程也有帮助。经常会有这样的情况，当你卡在一个问题上时，换下思路，回来重新尝试，往往可能会有新的灵感。</p></li><li><p>不要在深夜编程：避免熬夜编程，疲劳时编写的代码会质量较差，甚至有害，而且容易导致长时间低头弯腰，对身体造成不利影响。设定一个截止时间，坚持下来。</p></li><li><p>改善编程环境：优化编程环境，例如使用笔记本电脑支架和人体工学椅子，但即使有这些设备，仍然可能出现腰背疼痛。作者在文章中提到听说很多推荐站立式办公桌，现在也开始尝试。他认为，站立式办公桌不仅可以提高活动度，还能促进更多的休息，是一个双重受益的习惯。</p></li></ul><p>总之，这篇文章提醒了程序员要重视健康，特别是在长期编程的过程中，不良的习惯可能导致身体健康问题。通过每日伸展、定期休息、避免深夜编程和改善编程环境等习惯，可以减少对身体的伤害，享受健康的编程生涯。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://puppycoding.com/2023/07/22/healthy-coding-habits/&quot;&gt;The Most Important Coding Habits&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者在康复期躺着写下这篇文章，因为腰椎间盘突</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Programming" scheme="https://blog.aicoco.net/tags/Programming/"/>
    
  </entry>
  
  <entry>
    <title>用AI生成数据训练AI会有问题吗？</title>
    <link href="https://blog.aicoco.net/2023/07/22/use-ai-to-train-ai/"/>
    <id>https://blog.aicoco.net/2023/07/22/use-ai-to-train-ai/</id>
    <published>2023-07-22T14:04:37.000Z</published>
    <updated>2023-07-22T14:09:36.821Z</updated>
    
    <content type="html"><![CDATA[<p>让我们从一个不那么恰当的类比开始——一个人，通过读自己原创的书稿，能受到新的启发、获得能力的提升吗？不必急着回答，因为AI大模型的学习机制，和人类的学习机制，是截然不同的。但可以肯定，至少有一点人比AI靠谱——人不会因为看自己的作品失忆、变坏、变笨，目前的AI呢，还真不一定……</p><p>首先，<strong>AI生成的数据良莠不齐，且难以分辨</strong>。以目前的大型语言模型(LLM)为例，在生成文本时，通常会在每个步骤中做出<strong>高概率候选结果的随机选择</strong>，即使在相同的上下文中，模型也可能生成不同的续写。这种随机性可以增加模型的创新性和多样性，但也可能导致质量的不稳定。虽然这些模型在处理大规模的文本数据方面表现出色，但它们并不能理解文本的含义。模型没有人类的常识、情感和道德观念，因此<strong>可能会生成不准确、不合逻辑或者不适当的内容</strong>。大语言模型的训练数据来自于网络，这意味着它们接触到的信息范围极广，包括高质量的事实表述、学术文章和低质量的网络评论及谣言甚至其它各种不良内容。如果模型在训练过程中接触到大量的低质量内容，那么它<strong>可能会学习到这些内容的不良风格和模式</strong>。如果能“去其糟粕”，将生成的数据清洗干净，那当然再好不过，可遗憾的是，尽管目前存在一些自动化的文本质量评估方法，用来评估生成文本的流畅性、一致性等，但这些方法可能<strong>无法全面评估生成文本的质量</strong>。例如，一个句子可能在语法上完全正确，但在语义上完全没有意义，或在道德层面消极甚至反社会、反人性。这使得区分AI生成的高质量和低质量文本变得非常困难。</p><p>用<strong>人工反馈强化学习(RLHF)能保证生成质量吗</strong>？RLHF是一种常用的策略，通过人工评估和反馈来调整AI模型的行为，以使其与人类价值观对齐，避免生成低质量内容。但这种方法通常需要进行大量的试错，以找到能够最大化奖励的策略。对于复杂的任务，如文本生成，这个过程可能非常复杂和耗时。<strong>RLHF虽然可以解决一些明显的问题，但可能无法从根本上解决质量问题</strong>。有些问题可能源于模型的基本架构或训练数据，通过微调很难根本解决。更进一步，在<strong>RLHF训练过程中，模型可能会忘记之前学到的一些知识</strong>，这被称为灾难性遗忘。例如，当模型在人工反馈强化学习过程中过度优化某一特定任务时，可能会忘记其他任务的知识。这可能导致模型在某些方面的能力退化。人工反馈强化学习需要大量的人工评估和反馈，这可能会<strong>消耗大量的人力和时间</strong>。而且，人的评估可能<strong>存在一定的主观性和不一致性</strong>，也可能会影响训练的效果。</p><p>综上所述，<strong>AI生成的数据质量难以保证。</strong>如果不加以区分，将包括高质量的结果和不合理、不符合逻辑，甚至反社会、反人性的结果在内的所有数据都用于训练，那么可能会<strong>对模型质量产生不利影响</strong>：模型可能会从不合理、不符合逻辑的数据中<strong>学习到不适当的规律</strong>，从而在未来的预测中产生错误的、甚至可能带来不良后果的输出；包含大量质量低下的训练样本，可能会<strong>降低整体模型的预测质量和准确性</strong>，高质量的数据可能被大量低质量数据淹没，导致模型的性能下降；如果训练数据中包含反社会、反人性的内容，这些内容可能会被模型学习并在未来的预测中体现出来，这<strong>可能导致模型的输出存在严重的偏见和歧视</strong>；模型可能过度拟合这些不合理、不符合逻辑的数据，或者具有某种特定的偏见或者偏斜的数据，从而遗忘它在更广泛、更均衡的数据上学习到的知识，导致在面对真实、合理的数据时，<strong>泛化能力受损</strong>——即使AI生成数据不包含低质量内容，如果其不能准确地反映真实世界的总体分布，那么这些数据也<strong>可能会降低模型在面对未见过任务时的泛化能力</strong>。</p><p>那么，<strong>AI模型有可能“涌现”出对生成内容质量的“品味”，找到对真实世界分布的“感觉”吗？</strong>作为一种计算模型，AI模型是通过数学运算和大量数据的训练来进行预测和决策的，并不具有真实的“感觉”或“品味”。然而，从某种程度上，AI模型可以通过学习和优化来逼近(模仿)这些功能。AI模型可以<strong>通过学习评价函数或损失函数来优化它们生成的内容质量</strong>。例如，对于语言模型，可以通过学习评估语法正确性、信息完整性、创新性等因素的评价函数来优化生成的文本质量。然而，这<strong>需要大量的标注数据和精心设计的评价函数</strong>，否则模型可能会过度优化某些容易量化的指标，而忽视其他重要的质量因素。AI模型可以<strong>通过学习真实世界数据的分布来优化其泛化能力</strong>。包括使用更大规模、更多样化的训练数据，以及使用正则化技术来防止过拟合。然而，由于真实世界的复杂性，<strong>模型可能很难完全捕捉到所有的数据分布特征，尤其是在任务不明确、数据超级稀缺的情况下</strong>。AI模型确实可以<strong>通过自监督学习方法来实现自我进化</strong>。在这种方法中，模型在没有人工标注的数据上进行训练，通过预测数据的某些部分来学习数据的结构和模式。然而，这种方法在实际应用中仍面临很多挑战，包括<strong>如何设计有效的自监督任务，如何解决模型的过拟合问题，以及如何确保模型的学习符合我们的期望和价值观等</strong>。总的来说，虽然AI模型可以在一定程度上模拟出对内容质量的“品味”和对真实世界分布的“感觉”，但它们依然<strong>依赖于我们人类设计的算法、损失函数和训练策略</strong>。未来的研究可能会发现更有效的方法来提高模型的质量判断和泛化能力，以及实现模型的自我进化。</p><p>最后，类比<strong>基因多样性</strong>的概念，也许可以更深入地理解AI训练数据的多样性和相似性对大模型质量的影响。<strong>基因多样性在生物学上是至关重要</strong>的，因为这意味着一个物种能更好地适应环境变化，增加物种的生存和繁衍能力。在AI训练中，<strong>数据多样性也同样重要</strong>。多样性丰富的数据可以提供更全面的信息，帮助AI模型学习和理解更广泛的模式和关系，提高模型的泛化能力。如果训练数据只来自AI生成的一部分，那么这些数据可能具有相似的风格和偏见，这会限制AI模型的学习和理解能力，降低其在处理新颖、未见过的任务时的表现。<strong>近亲繁殖可能导致基因的同质化</strong>，增加了有害基因的表达和累积，从而影响个体的健康和生存能力。在AI训练中，如果数据过于相似或重复，也会引起类似的问题。如果一个AI模型主要或完全使用由自身或类似模型生成的数据进行训练，那么这种<strong>“数据近亲繁殖”可能导致模型的学习过程中出现过拟合</strong>，使模型在面对新的、与训练数据不同的数据时表现不佳。此外，这种方式还可能导致模型的偏见和错误被放大，从而降低输出内容的质量。</p><p>为了<strong>避免使用AI生成数据进行训练对大模型质量的不利影响</strong>，可以从以下几个方面进行考虑：<strong>对AI生成的数据通过立法等手段在生成、分发、使用等各环节与人工数据有效区分</strong>，这是一种可能的策略，以管理和控制AI生成内容的质量和公平性，有助于提高透明度，使用户在使用这些数据时能做出知情的决定。<strong>对AI生成的数据进行筛选和质量控制</strong>，去除低质量、错误信息或者偏离真实分布的数据，确保训练数据的质量。可以采用人工或半自动的方式进行数据清洗和筛选。<strong>尽可能使用多元、多样性的数据进行训练，避免数据单一导致的过拟合</strong>，保证训练数据能够覆盖真实世界的多种情况。<strong>对模型的架构进行优化</strong>，如引入多头多层次的注意力机制，使得模型更能注意到重要的信息；<strong>对训练策略进行调整</strong>，如采用迁移学习、元学习等方法，使得模型能更好地学习和泛化；<strong>对训练过程进行监控</strong>，及时发现并纠正模型的过拟合等问题。建立<strong>有效的模型评估和反馈机制</strong>，对模型生成的结果进行质量评估，及时反馈并调整模型，<strong>形成高质量的正反馈过程</strong>，使其更好地满足质量要求。<strong>遵守相关的法规和伦理指南</strong>，保证AI的发展在可接受的道德和社会范围内。这也可以帮助确保AI生成的数据和其结果不会产生不利的影响。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;让我们从一个不那么恰当的类比开始——一个人，通过读自己原创的书稿，能受到新的启发、获得能力的提升吗？不必急着回答，因为AI大模型的学习机制，和人类的学习机制，是截然不同的。但可以肯定，至少有一点人比AI靠谱——人不会因为看自己的作品失忆、变坏、变笨，目前的AI呢，还真不一定</summary>
      
    
    
    
    <category term="随笔" scheme="https://blog.aicoco.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>好的代码就像一封情书</title>
    <link href="https://blog.aicoco.net/2023/07/22/good-code-like-love-letter/"/>
    <id>https://blog.aicoco.net/2023/07/22/good-code-like-love-letter/</id>
    <published>2023-07-22T01:22:11.000Z</published>
    <updated>2023-07-23T06:21:24.059Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://addyosmani.com/blog/good-code/">Good code is like a love letter to the next developer who will maintain it</a></p><h3 id="编程的真谛：好的代码是一封情书"><a href="#编程的真谛：好的代码是一封情书" class="headerlink" title="编程的真谛：好的代码是一封情书"></a>编程的真谛：好的代码是一封情书</h3><p>我们常常将编程理想化，将其描述为抽象的艺术、科学，甚至是魔法。然而，实际情况要更加务实和踏实。<strong>代码，本质上是一种沟通方式</strong>。在作者编著的《学习JavaScript设计模式》一书开篇，曾说过：“<strong>优秀的代码就像是写给将来维护它的开发者的情书</strong>。”这是一种亲密的联系，由一个开发者写给另一个开发者，跨越时间和空间。</p><h5 id="爱的语言"><a href="#爱的语言" class="headerlink" title="爱的语言"></a>爱的语言</h5><p>情书是个人的、真诚的、体贴的，是对感情的诗意见证，往往经过精心打磨，以准确地传达感情。好的代码也是如此。它是个人的，因为它反映了编写者的逻辑和方法。好的代码是真诚的，没有不必要的复杂性。它是体贴的，关心下一个开发者将如何解读它。最重要的是，好的代码经过精心设计，以最高的效率解决问题。</p><h5 id="模式和原则"><a href="#模式和原则" class="headerlink" title="模式和原则"></a>模式和原则</h5><p>就像用语法规则和语言结构可以将词语和感情组成可理解的句子，我们也有设计模式和原则来塑造代码。模式不仅使代码具有可伸缩、可维护且高效，还使其易读易懂。它们为开发者提供了共享的术语，使他们能用普遍认可的结构表达复杂的软件设计。</p><p>因此，好的代码巧妙地运用这些模式，就像熟练的诗人使用修辞手法创造共鸣。不是为了滥用模式，而是因为它们为解决方案增值，使代码更易理解，并确保代码库的持久性。</p><p>SOLID、DRY、KISS和YAGNI不仅是原则，而且是打造优秀代码的基石。它们引导开发者做出明智的决策，在过度和过少工程化之间取得平衡，最终写下让后来者珍视的“情书”。</p><h5 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h5><p>好的代码也遵循既定的最佳实践，就像情书会遵循某些社交礼仪一样。适当的命名约定、模块化和详尽的注释都是其中的一部分。它们不仅仅是规则，需要遵循，而且是规范，用来表明代码(或编写者)对下一个开发者是多么体贴。确保编写者的意图不会在传递中失去。</p><h5 id="拥抱测试"><a href="#拥抱测试" class="headerlink" title="拥抱测试"></a>拥抱测试</h5><p>就像作家校对他们的信件一样，开发者也应该对他们的代码进行校对。严格的测试和测试驱动开发(TDD)的实践是精心打磨的“情书”的标志。测试验证代码在各种场景下的表现，发现潜在的缺陷和盲点。强大的测试框架的存在往往证明了代码的质量。</p><h5 id="共情和尊重"><a href="#共情和尊重" class="headerlink" title="共情和尊重"></a>共情和尊重</h5><p>最重要的是，一篇情书的核心是对读者的共情和尊重，好的代码也是如此。编写其他人可以阅读、理解和维护的代码，是一种职业尊重。这表明编写者理解他们的工作是一个更大的、持续不断的努力，软件是一个不断演进的生命体，将有许多人在未来继续塑造它，续写它的命运。</p><h5 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h5><p>最后，编程是一种创作行为，类似于写一首诗或画一幅画。然而，我们的创作之美不仅仅取决于我们算法的优雅或代码的高效，更取决于其他人可以如何快乐、轻松地在我们打下的基础上继续展开工作。作为开发者，我们的任务不仅是解决今天的问题，也是确保我们不会成为明天的问题。</p><p>因此，好的代码不仅是一封情书，也是我们留给后来者永恒的遗产。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://addyosmani.com/blog/good-code/&quot;&gt;Good code is like a love letter to the next developer who will maintain it&lt;/a&gt;&lt;/p&gt;
&lt;h</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Programming" scheme="https://blog.aicoco.net/tags/Programming/"/>
    
  </entry>
  
  <entry>
    <title>LLaMA2不是真正意义上的“开源”</title>
    <link href="https://blog.aicoco.net/2023/07/22/llama2-isnt-open-source/"/>
    <id>https://blog.aicoco.net/2023/07/22/llama2-isnt-open-source/</id>
    <published>2023-07-22T01:09:12.000Z</published>
    <updated>2023-07-23T06:16:54.294Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://www.alessiofanelli.com/blog/llama2-isnt-open-source">LLaMA2 isn’t “Open Source” - and why it doesn’t matter</a></p><h3 id="LLaMA2-并非真正的“开源”，但这并不重要"><a href="#LLaMA2-并非真正的“开源”，但这并不重要" class="headerlink" title="LLaMA2 并非真正的“开源”，但这并不重要"></a>LLaMA2 并非真正的“开源”，但这并不重要</h3><p>作者是一位开源公司创始人，多年来一直参与开源社区，对开源项目的贡献、演讲和投资充满热情。他认为，互联网之所以成为现在的样子，很大程度上归功于那些支撑着数字基础设施的优秀开源项目，因此开源始终是他心中的重要话题。</p><p>然而，当 LLaMA2 出现时，许多他尊敬的社区成员对该模型误用“开源”一词感到不满。</p><p>LLaMA2 虽然在很大程度上是开源的，但其中有限制条件，例如：如果在发布日期时月活跃用户超过7亿，就不能以商业目的使用该模型；同时也不能使用该模型的输出结果来训练其他大型语言模型。这些限制与开源精神不太相符。但是，尽管作者同意 LLaMA2 在传统意义上不能称为开源，但他认为这并不重要。在人工智能模型的世界中，“开源”一词需要再次演变。</p><h5 id="从自由到开源"><a href="#从自由到开源" class="headerlink" title="从自由到开源"></a>从自由到开源</h5><p>在文章中，作者回顾了自由软件和开源运动的历史。自1976年“给业余爱好者的公开信”以来，软件公司的商业利益与想要绕过限制的黑客的好奇心之间一直存在紧张关系。70年代，自由软件运动在麻省理工学院的人工智能实验室起源，由 Richard Stallman 创立，最终于1983年发展成 GNU 项目。GPL “copyleft” 许可证诞生，并被 Red Hat、MySQL、Git 和 Ubuntu 等项目采用。</p><p>“开源”这个词在1998年得以确立，归功于麻省理工学院的 Christine Peterson。在“免费软件高峰会”上，“自由软件”一词正式被“开源软件”取代。随着时间的推移，“自由软件”和“开源软件”社区出现了分歧，因为它们对“自由”和“开源”的理解不同。自由软件，如自由软件基金会所规定，只是开源软件的一个子集，采用非常宽松的许可证，如 GPL 和 Apache。</p><p>在过去十年里，由于商业开源公司和云超大规模企业之间的紧张关系，出现了另一种分歧。Elastic 和 MongoDB 将其开源项目转换为“服务器端公共许可证”（SSPL），允许开发者在商业用途下使用产品，前提是所提供的不是产品的托管版本。其目标是阻止 AWS 将它们的产品作为云服务重新托管并从中获利。然而，SSPL 也侵犯了开源理念，并未获得开源倡议组织的认可。尽管如此，大多数开发者仍然认为 MongoDB 是开源的。逐渐地，“开源”一词正在失去其自由的涵义，在开发者心目中几乎成为“源码可用”的同义词。</p><h5 id="从源码到权重"><a href="#从源码到权重" class="headerlink" title="从源码到权重"></a>从源码到权重</h5><p>随着像 Dolly、MPT、LLaMA 等开放模型的崛起，社区中出现了类似的分歧。对于大多数 AI 工程师来说，如今的“开源”意味着“可下载权重”，仅此而已。Heather Meeker 提出了“开放权重”的定义，但目前还没有社区共识。问题在于，开放权重是否足以使一个模型被称为开源；软件的类比是项目发布其二进制文件而不提供源代码以供重新构建。</p><p>要使模型真正成为开源且可从头开始重新训练，创建者需要分享所有的训练代码、预训练数据集、微调偏好、RLHF 示例等。然而，这些训练过程的成本非常高，即使有人愿意全部公开，对于大多数开发者和公司来说，从头训练模型是不可行的，因此能够获得最终权重更加实用。</p><h5 id="开放模型"><a href="#开放模型" class="headerlink" title="开放模型"></a>开放模型</h5><p>在大型语言模型（LLMs）领域，术语“开源”用于定义多种开放程度：</p><ol><li>开放模型：如 RedPajama 和 MPT-7B，它们的权重对商业用途是开放的（使用 Apache 2.0 许可证），而且可以从头开始重新训练，因为数据集是开源的。</li><li>开放权重：StableLM 是 StabilityAI 训练的开放模型。权重是开放的，使用 Apache 2.0 许可证，但用于训练的数据集对公众是不可用的。</li><li>受限权重：这是指 LLaMA2。预训练数据集也不可用，尽管权重据称对商业用途是开放的，但存在上述特定限制。</li><li>受污染(Contaminated)权重：Dolly 1.0 和 LLaMA1 属于这一类别。权重是公开的，但用于训练它们的数据集不允许商业用途，这使得它在技术上是开放的，但实际上是无用的。</li></ol><p>在可预见的未来，开源和开放权重将被互换使用，而作者认为这没问题。重要的是，越来越多的工作以尽可能开放的方式进行。对于 LLaMA2 的许可证，人们可能感到失望，但是 Meta 刚刚将价值约200万美元的浮点运算放进了 Github 库，作者认为这对该领域的进展将产生积极的影响。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://www.alessiofanelli.com/blog/llama2-isnt-open-source&quot;&gt;LLaMA2 isn’t “Open Source” - and why it doesn’t matter&lt;/a&gt;&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Meta为何选择开源Llama 2？</title>
    <link href="https://blog.aicoco.net/2023/07/22/why-did-meta-open-source-llama/"/>
    <id>https://blog.aicoco.net/2023/07/22/why-did-meta-open-source-llama/</id>
    <published>2023-07-22T00:40:27.000Z</published>
    <updated>2023-07-23T06:20:35.727Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://matt-rickard.com/why-did-meta-open-source-llama">Why Did Meta Open-Source Llama 2?</a></p><h4 id="对Meta开源Llama-2可能原因的一些猜想"><a href="#对Meta开源Llama-2可能原因的一些猜想" class="headerlink" title="对Meta开源Llama 2可能原因的一些猜想"></a>对Meta开源Llama 2可能原因的一些猜想</h4><h5 id="削弱竞争对手的优势"><a href="#削弱竞争对手的优势" class="headerlink" title="削弱竞争对手的优势"></a>削弱竞争对手的优势</h5><ul><li><p>Llama 2 对拥有专有模型的竞争对手，如Google和OpenAI(以及Microsoft的相关服务)，构成了挑战。通过开源大模型，可以<strong>削弱这些大公司在语言模型领域的优势</strong>。</p></li><li><p>作为身处服务栈需要<strong>持续吸引用户</strong>的公司，Meta坐拥数十亿固定用户，这使他们在竞争中占据优势。</p></li></ul><h5 id="市场推广策略的考虑"><a href="#市场推广策略的考虑" class="headerlink" title="市场推广策略的考虑"></a>市场推广策略的考虑</h5><ul><li><p>Llama 2提供不同参数大小的模型，包括7b、13b和70b。通过将较小的模型作为“免费版”自助选项，Meta可以吸引用户，并鼓励他们在未来<strong>选择Meta平台的更大或更新的版本</strong>。</p></li><li><p>Meta可能推出一系列与Llama 2<strong>相关的扩展产品</strong>，包括为Instagram、Threads或Facebook提供支持Llama的功能，特殊设计的硬件芯片和数据中心，以及在PyTorch中优化Llama衍生模型的机器学习框架。</p></li><li><p>此外，Meta未来可能推出<strong>托管服务作为商业化产品</strong>，其中将Llama 2作为基础组件。</p></li></ul><h5 id="市场营销和声誉建设"><a href="#市场营销和声誉建设" class="headerlink" title="市场营销和声誉建设"></a>市场营销和声誉建设</h5><ul><li>通过开源Llama 2并将自己定位为技术前沿的公司，Meta旨在<strong>建立强大的声誉</strong>。这样的声誉可以吸引开发者、用户和媒体的关注，类似于Google多年来的积极影响。</li></ul><p>总体而言，开源Llama 2的决定使得Meta可以挑战和削弱竞争对手，用自助免费模型吸引用户，在语言模型领域树立创新领先的公司形象。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://matt-rickard.com/why-did-meta-open-source-llama&quot;&gt;Why Did Meta Open-Source Llama 2?&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;对Meta开源Llama-2可能原因</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>沉迷工具不会让你成为大师</title>
    <link href="https://blog.aicoco.net/2023/07/10/Amateurs-obsess-over-tools-pros-over-mastery/"/>
    <id>https://blog.aicoco.net/2023/07/10/Amateurs-obsess-over-tools-pros-over-mastery/</id>
    <published>2023-07-10T03:21:09.000Z</published>
    <updated>2023-07-23T06:29:28.813Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://adamsinger.substack.com/p/amateurs-obsess-over-tools-pros-over">Amateurs obsess over tools, pros over mastery</a></p><h4 id="文章主要信息"><a href="#文章主要信息" class="headerlink" title="文章主要信息"></a>文章主要信息</h4><ul><li>文章强调了业余爱好者过度迷恋新工具，而专业人士更关注技艺的精湛。</li><li>专业人士知道工具并不能决定成就，真正重要的是个人运用它们的心态和技能。</li><li>文章通过例子阐述了技能的重要性，指出工具本身并非关键，个人的精湛运用才是决定成就的关键。</li><li>文章提倡专注于培养基本技能和永恒的原则，摒弃追求新奇工具的心态。</li></ul><h4 id="详细重点内容"><a href="#详细重点内容" class="headerlink" title="详细重点内容"></a>详细重点内容</h4><ol><li><p><strong>业余爱好者迷恋新工具</strong>：现代科技发展迅速，新的工具和应用层出不穷。很多人容易陷入追逐新工具的陷阱，相信使用这些工具会带来高效率、更多产出和成功。然而，过度迷恋新工具只是短暂的热情，很快就会被其他新工具取代，而这些追逐也是徒劳的。</p></li><li><p><strong>专业人士关注技艺</strong>：与业余爱好者不同，真正的专业人士知道工具并不能代表他们的成就。他们理解重要的是运用这些工具的个人心态和技能。类似于音乐家运用吉他演奏，吉他本身可能并不是最先进的工具，但在熟练的音乐家手中，它能产生动人的旋律和感人的和声。</p></li><li><p><strong>精湛的技艺超越工具</strong>：文章举例说明吉他手通过对技艺的深入研究和练习，使得吉他成为创作灵感的源泉。这表明真正的精湛技艺超越了工具本身，是在不受技术潮流影响的基础上持续存在的。</p></li><li><p><strong>专注培养基本技能</strong>：专业人士理解重要的是不断磨练自己的技艺，无论手中有怎样的工具。文章引用了武术家李小龙的名言，强调通过深入、持续的练习，掌握基本技能才是成为专家的关键。</p></li><li><p><strong>不盲目追求新工具</strong>：文章警示不要盲目追求新工具，而是专注于打磨自己的技能。新工具可能只是短暂的潮流，而真正持久的是对基本技能和原则的坚持。</p></li><li><p><strong>超越人工智能的创造力</strong>：文章提到人工智能的发展，但也指出真正的创造力和技艺仍然超越人工智能的能力。专注于个人的技艺将使人们摆脱对人工智能创造力的过度依赖，实现真正意义上的创新。</p></li><li><p><strong>反思和自省</strong>：文章最后呼吁读者在追逐新工具时停下来反思，问自己是否真正在提升自己的技艺，是否被他人的意见所左右，是否在做真正有意义的事情，而不是只是追逐表面的诱惑。</p></li></ol><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>文章强调了专业人士不是盲目追求新工具，而是注重培养自己的技艺和技能。真正的精湛技艺超越了工具本身，并且在不受技术潮流影响的基础上持续存在。专注于培养基本技能和原则，而不是追逐新奇工具，才能成为真正的专业人士。文章还提醒我们要超越人工智能的创造力，通过反思和自省，避免陷入盲目追逐的状态。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://adamsinger.substack.com/p/amateurs-obsess-over-tools-pros-over&quot;&gt;Amateurs obsess over tools, pros over mastery&lt;/a&gt;&lt;/p&gt;</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
  </entry>
  
  <entry>
    <title>AI权重开&quot;源&quot;怎么论</title>
    <link href="https://blog.aicoco.net/2023/07/06/AI_weights_are_not_open_source/"/>
    <id>https://blog.aicoco.net/2023/07/06/AI_weights_are_not_open_source/</id>
    <published>2023-07-06T11:17:38.000Z</published>
    <updated>2023-07-23T06:33:50.418Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://opencoreventures.com/blog/2023-06-27-ai-weights-are-not-open-source/">AI weights are not open “source”</a></p><h4 id="文章主要信息总结"><a href="#文章主要信息总结" class="headerlink" title="文章主要信息总结"></a>文章主要信息总结</h4><ul><li>AI的许可证复杂多样，不同于传统软件的开源或专有授权。</li><li>AI有多个组成部分，如源码、权重、数据等，每个部分的许可方式可能不同。</li><li>为了标准化对AI许可证的讨论，文章提出了一套许可证类型的分类，包括专有、合作者、可用、伦理和开源。</li><li>提倡区分AI的源代码和权重，认识到权重不是源代码，需要特定的许可证类型。</li><li>引用了“开放权重”（Open Weights）和“伦理权重”（Ethical Weights）等术语，以便更准确地描述不同类型的AI权重许可证，避免误用“开源”这个术语。</li></ul><h4 id="详细重点内容"><a href="#详细重点内容" class="headerlink" title="详细重点内容"></a>详细重点内容</h4><ol><li><p><strong>AI许可证复杂性</strong>：AI许可证不同于传统的软件许可证，因为AI有多个组成部分，如源码、权重、数据等，每个部分可能有不同的许可方式。此外，AI的使用还涉及到社会伦理层面的考量，需要更多的限制和约束。</p></li><li><p><strong>标准化许可证分类</strong>：为了更好地讨论AI许可证，文章提出了一套分类，包括专有、合作者、可用、伦理和开源。每个分类针对不同的用途和组成部分，以更清晰地描述许可证的属性。</p></li><li><p><strong>“开放权重”和“伦理权重”</strong>：文章指出权重和源代码是两个不同的概念，不能将权重称为“开源”，因为它们并不是源代码。为了避免混淆，文章提出了“开放权重”和“伦理权重”等术语，以更准确地描述权重的开放程度和许可限制。</p></li><li><p><strong>权重的真正开放性</strong>：许多标注为“开源”的AI权重实际上并不是真正的开源，因为它们不是源代码，而是特定的组件。文章强调使用正确的术语，如“开放权重”和“伦理权重”，有助于推动行业发展，并在不同类型的权重上建立标准。</p></li><li><p><strong>避免“开源洗白”</strong>：由于AI面临与传统软件不同的挑战，可能需要限制某些潜在的有害用途。文章提倡使用“伦理”来描述允许类似开源的自由使用的许可证，避免“开源洗白”，为用户提供更清晰的信息。</p></li></ol><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>文章着重强调AI许可证的复杂性，提出了一套分类以标准化对AI许可证的讨论。作者建议使用“开放权重”和“伦理权重”等术语，以更准确地描述不同类型的AI权重许可证，避免混淆和误用“开源”这个术语。同时，文章强调区分AI的源代码和权重，认识到权重不是源代码，需要特定的许可证类型。通过正确的命名和分类，有助于推动AI领域的标准化和发展。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://opencoreventures.com/blog/2023-06-27-ai-weights-are-not-open-source/&quot;&gt;AI weights are not open “source”&lt;/a&gt;&lt;/p&gt;
&lt;h4 id</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>AIGC时代程序员如何保持领先</title>
    <link href="https://blog.aicoco.net/2023/07/05/4_tips_for_programmers_to_stay_ahead_of_generative_AI/"/>
    <id>https://blog.aicoco.net/2023/07/05/4_tips_for_programmers_to_stay_ahead_of_generative_AI/</id>
    <published>2023-07-05T13:22:38.000Z</published>
    <updated>2023-07-23T06:37:27.873Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://spectrum.ieee.org/ai-programming">How Coders Can Survive—and Thrive—in a ChatGPT World &gt; 4 tips for programmers to stay ahead of generative AI</a></p><h4 id="文章主要信息总结"><a href="#文章主要信息总结" class="headerlink" title="文章主要信息总结"></a>文章主要信息总结</h4><ul><li>人工智能，特别是由大型语言模型（LLM）驱动的生成式人工智能，可能会改变许多编程人员的生计，但一些专家认为AI不会立即取代人类程序员。</li><li>软件开发人员可以通过遵循基本原则和最佳实践，以及找到适合自己需求的AI工具来在生成式AI时代生存和发展。</li><li>保持对编程基础和问题解决技能的重视，同时加强软件工程实践，规划系统设计和软件架构。</li><li>在使用AI编程助手时，清晰明确的交流是关键，需要详细说明需求，使用合适的提示工程方法，对AI生成的代码进行审查和验证。</li><li>开发人员应对大型语言模型的输出持批判态度，了解其潜在的风险和局限性，同时注意版权和安全问题。</li></ul><h4 id="详细重点内容"><a href="#详细重点内容" class="headerlink" title="详细重点内容"></a>详细重点内容</h4><ol><li><p><strong>保持基本原则和最佳实践</strong>：虽然AI辅助编码工具可以帮助完成代码和生成代码，但编程的基本原则和最佳实践依然重要，如阅读和理解自己和他人的代码，以及了解编写的代码如何适应更大的系统。</p></li><li><p><strong>问题解决技能</strong>：解决问题仍然是程序员最重要的技能之一。分析问题并找到优雅的解决方案在编程领域中依然备受推崇。</p></li><li><p><strong>选择适合自己需求的AI工具</strong>：找到合适的AI工具至关重要。不同的工具有不同的交互方式和整合方式，可以用于自动化单元测试的创建、生成测试数据、或编写文档等。</p></li><li><p><strong>清晰明确的交流</strong>：使用AI编程助手时，需要详细说明需求，对生成的代码进行审查和验证。合理的提示工程方法可以帮助与AI模型进行更有效的交流，以获取满足需求的代码。</p></li><li><p><strong>对AI输出持批判态度</strong>：大型语言模型往往会产生错误或不准确的代码，因此软件工程师需要对生成的代码进行审查和验证。了解模型的训练数据和版本等信息有助于理解结果并提供更多上下文。</p></li><li><p><strong>注意安全和版权问题</strong>：AI生成的代码可能存在漏洞，软件工程师需要注意安全问题，并采取代码审查和强有力的测试流程来防范风险。此外，版权问题也需要考虑，尤其是使用私有代码时。</p></li></ol><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>在生成式AI时代，软件开发人员需要认识到AI是一种工具，将其纳入工作流程，同时了解这些工具的机会和限制，并继续依靠自己的人类编码能力来取得成功。重视问题解决能力、保持软件工程实践和审查AI输出是软件开发人员在AI时代生存和发展的关键。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://spectrum.ieee.org/ai-programming&quot;&gt;How Coders Can Survive—and Thrive—in a ChatGPT World &amp;gt; 4 tips for programmers to</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="AIGC" scheme="https://blog.aicoco.net/tags/AIGC/"/>
    
  </entry>
  
  <entry>
    <title>科研数据分享最佳实践</title>
    <link href="https://blog.aicoco.net/2023/06/29/How-to-make-your-scientific-data-accessible-discoverable-and-useful/"/>
    <id>https://blog.aicoco.net/2023/06/29/How-to-make-your-scientific-data-accessible-discoverable-and-useful/</id>
    <published>2023-06-29T02:57:33.000Z</published>
    <updated>2023-07-23T06:39:21.029Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://www.nature.com/articles/d41586-023-01929-7">How to make your scientific data accessible, discoverable and useful</a></p><p>这篇文章讨论了在开放科学和可重复性的背景下发布可用和高质量数据的最佳实践。越来越多的研究人员被鼓励在发表论文的同时提交数据，但在处理来自不同来源和格式的数据时可能会遇到挑战。以下是一些数据科学家建议的关键做法：</p><ol><li><p><strong>制定元数据</strong>: 元数据描述数据，对于使数据符合FAIR（可找到、可访问、可互操作、可重用）原则至关重要。科学家应提供详细的数据收集、处理和变量信息，以及表格或文件之间相互关联的解释。</p></li><li><p><strong>多分享</strong>: 最好能同时分享原始数据和派生数据。原始数据允许其他研究人员测试假设和处理策略，而派生数据则是分析的基础。</p></li><li><p><strong>采纳标准</strong>: 科学家应该寻求更广泛社区的指导，了解数据存储库和文件格式。推荐使用开放、非专有的文件格式，如CSV，以确保数据长期可读。</p></li><li><p><strong>包含代码</strong>: 当数据分析涉及代码时，研究人员应将代码与数据一起分享。代码应有良好的文档记录，清除特定计算机元素，并经过可重现性测试。</p></li><li><p><strong>考虑可访问性</strong>: 考虑潜在数据用户的技术基础设施和要求。咨询相关组织，以获取有关数据标准和假设的反馈，并为不同条件下的用户开发低技术解决方案。</p></li><li><p><strong>迈出第一步</strong>: 开放科学不必是非此即彼的。即使分享部分数据也能增加价值和促进合作机会。</p></li></ol><p>通过遵循这些最佳实践，研究人员可以促进科学的发展，推动合作，并确保其工作的可重复性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://www.nature.com/articles/d41586-023-01929-7&quot;&gt;How to make your scientific data accessible, discoverable and useful&lt;/a&gt;&lt;</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Academic" scheme="https://blog.aicoco.net/tags/Academic/"/>
    
    <category term="Data" scheme="https://blog.aicoco.net/tags/Data/"/>
    
  </entry>
  
  <entry>
    <title>图解：私有知识库LLM聊天机器人</title>
    <link href="https://blog.aicoco.net/2023/06/28/LLM-based-Chatbot-to-query-Private-Knowledge-Base/"/>
    <id>https://blog.aicoco.net/2023/06/28/LLM-based-Chatbot-to-query-Private-Knowledge-Base/</id>
    <published>2023-06-28T01:18:02.000Z</published>
    <updated>2023-06-28T12:38:44.191Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://twitter.com/Aurimas_Gr/status/1673621343122726912">LLM based Chatbot to query Private Knowledge Base</a></p><div class="tag-plugin image"><div class="image-bg"><img src="https://s1.ax1x.com/2023/06/28/pCwP8te.jpg" alt="LLM based Chatbot to query Private Knowledge Base"/></div><div class="image-meta"><span class="image-caption center">LLM based Chatbot to query Private Knowledge Base</span></div></div><ol><li>将整个知识库的文本语料分割成多个块——每个块表示一个可查询的上下文片段，知识数据可以来自多个源；</li><li>用嵌入(Embedding)模型将每个块转换为一个向量；</li><li>将所有向量存储在向量数据库；</li><li>分别保存表示每个嵌入向量的文本，同时保存指向该向量的指针。</li><li>使用与嵌入知识库本身所使用的相同的嵌入模型，将要提问的问题&#x2F;查询进行嵌入，转换成向量；</li><li>使用生成的向量在向量数据库的索引中运行一个查询。选择要从向量数据库中检索多少个向量 - 这将等于您将要检索和最终用于回答查询问题的上下文数量；</li><li>向量数据库对所提供的向量执行近似最近邻(ANN)搜索，并返回之前选择的上下文向量的数量。该过程返回在给定的嵌入&#x2F;潜空间中最相似的向量；</li><li>将返回的向量嵌入映射到对应的文本块；</li><li>将问题与检索到的上下文文本块一起传给LLM(大语言模型)，通过提示指示LLM仅使用提供的上下文来回答给定的问题。这并不意味着不需要进行提示工程 - 需要确保LLM返回的答案符合预期的范围，例如，如果在检索到的上下文中没有可用的数据，则确保不提供虚构的答案。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://twitter.com/Aurimas_Gr/status/1673621343122726912&quot;&gt;LLM based Chatbot to query Private Knowledge Base&lt;/a&gt;&lt;/p&gt;
&lt;div cla</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="ChatGPT" scheme="https://blog.aicoco.net/tags/ChatGPT/"/>
    
    <category term="Chatbot" scheme="https://blog.aicoco.net/tags/Chatbot/"/>
    
  </entry>
  
  <entry>
    <title>LLM赋能的自主智能体(Agent)</title>
    <link href="https://blog.aicoco.net/2023/06/28/LLM-Powered-Autonomous-Agents/"/>
    <id>https://blog.aicoco.net/2023/06/28/LLM-Powered-Autonomous-Agents/</id>
    <published>2023-06-27T22:20:04.000Z</published>
    <updated>2023-06-28T12:39:06.328Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents</a></p><p>以LLM（大型语言模型）作为核心控制器构建智能体的概念很酷。一些概念验证演示，如AutoGPT、GPT-Engineer和BabAGI，都是令人鼓舞的示例。LLM的潜力不仅限于生成书面副本、故事、论文和程序，它可以被视为强大的通用问题求解器。LLM驱动的自主智能体系统概述：LLM作为智能体的大脑，配合几个关键组件：子目标规划和分解、反思和改进、短期记忆、长期记忆和工具使用。该系统还包括记忆类型、最大内积搜索(MIPS)算法、工具使用能力的案例研究，以及AutoGPT等概念验证示例。</p><p>要点：</p><ul><li>建立以大型语言模型(LLM)为核心控制器的智能体系统是一个很酷的概念。</li><li>LLM的潜力不仅限于生成文本、故事、论文和程序，它还可以被视为一个强大的通用问题求解器。</li><li>智能体系统的核心组成部分包括规划、反思、记忆和工具使用。</li><li>规划：将复杂任务分解为可管理的子目标，实现对复杂任务的高效处理。</li><li>反思和改进：智能体可以对过去的行动进行自我批评和反思，从错误中学习并改进，提高最终结果的质量。</li><li>记忆：短期记忆用于模型的上下文学习，长期记忆通过外部向量存储和快速检索提供了无限的信息存储和回忆能力。</li><li>工具使用：智能体学习调用外部API获取模型权重中缺失的额外信息，包括当前信息、代码执行能力、专有信息源等。</li><li>LLM+P方法利用外部经典规划器进行长程规划，通过PDDL语言描述规划问题，将规划步骤外包给外部工具。</li><li>自反思：智能体通过结合任务特定的离散动作和语言空间来进行自反思，从而改进决策和行动，提高推理能力。</li><li>CoH通过向模型提供过去输出的历史序列和反馈来改进模型的输出质量。</li><li>算法蒸馏利用交叉轮次历史训练神经网络，学习强化学习算法的过程而非任务特定的策略。</li><li>记忆分为感觉记忆、短期记忆和长期记忆，长期记忆可以通过外部向量存储和快速检索进行扩展。</li><li>外部记忆通常使用最大内积搜索(MIPS)算法进行快速检索，常用的算法包括LSH、ANNOY、HNSW、FAISS和ScaNN。</li><li>工具使用可以显著扩展模型的能力，如调用API、使用外部模块等。</li><li>MRKL是一个神经符号化架构，将LLM与多个专家模块结合起来，根据任务选择合适的模块。</li><li>HuggingGPT是一个使用LLM作为任务规划器的框架，根据模型描述选择合适的模型并提供执行结果的总结。</li><li>API-Bank是一个评估工具增强LLM性能的基准，包含常用API工具、完整的工具增强LLM流程和带有API调用的对话数据集。</li><li>ChemCrow是一个领域特定的示例，LLM与专家设计的工具结合，用于完成有机合成、药物发现和材料设计等任务。</li><li>Generative Agents是一个基于LLM的虚拟角色模拟实验，结合记忆、规划和反思机制，实现了智能体之间的互动行为。</li><li>AutoGPT是一个证明概念的示例，展示了将LLM作为主控制器的可能性，但在可靠性方面存在一些问题。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;&gt;LLM Powered Autonomous Agents&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;以LLM（大型语言模型）作为核心控制器构建智能体的概念很酷。</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="Agent" scheme="https://blog.aicoco.net/tags/Agent/"/>
    
    <category term="ChatGPT" scheme="https://blog.aicoco.net/tags/ChatGPT/"/>
    
  </entry>
  
  <entry>
    <title>学术界能否与资源雄厚的工业界相竞争？计算机图形学的历史观点</title>
    <link href="https://blog.aicoco.net/2023/06/28/Can-academia-compete-with-the-resources-of-industry/"/>
    <id>https://blog.aicoco.net/2023/06/28/Can-academia-compete-with-the-resources-of-industry/</id>
    <published>2023-06-27T22:07:38.000Z</published>
    <updated>2023-07-06T11:17:42.993Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://drive.google.com/file/d/1bsUrkwkaE2ZmGH8yBKRjDiqqpbvrnSLr/view">Can academia compete with the resources of industry? - A historical perspective from Computer Graphics</a></p><p>文章探讨了学术界与资源雄厚的工业界相竞争的问题。作者回顾了计算机图形学领域的发展历程，并指出工业界在数据规模、计算集群和工程团队等方面具有优势。然而，学术界并没有试图复制工业界的成就，而是专注于一些被工业界忽视的领域，如物理模拟、光照模拟、外观模型和机器学习等。随着时间的推移，学术界的研究成果逐渐影响了计算机图形学的渲染技术，并在图形硬件方面取得了一定的贡献。文章指出学术界的盲点和困境，同时强调了在学术研究中的重要原则，包括专注于长期目标、学习非主流技术、开放合作和保持乐趣等。总体而言，学术界在计算机图形学领域中扮演着重要的角色，并与工业界共同推动了该领域的发展。</p><p>要点：</p><ul><li>AI研究的规模让人们担心学术界在资源方面是否能与工业界竞争。</li><li>回顾计算机图形学领域，作者回忆起25年前作为博士生时的经历，当时工业界在资源方面具有明显优势。</li><li>工业界在规模方面具有优势，包括输入训练数据、计算集群和工程团队等方面，学术界无法与之匹敌。</li><li>学术界并没有试图复制工业界的成就，而是探索不同的方法，并专注于工业界认为不太重要的领域，如光线和动作的物理模拟。</li><li>学术界在许多领域进行了研究，包括光照模拟、基于图像的建模和光照、皮肤和头发的外观模型、衣物和流体等的物理模拟，以及动画的机器学习等创新领域。</li><li>随着时间推移，计算机图形学中的渲染技术在很大程度上依赖于学术界的研究，如蒙特卡洛路径追踪、材质外观模型、基于图像的光照和非真实感渲染。</li><li>学术界也在图形硬件方面做出了贡献，开创了灵活的硬件、光线追踪硬件和通用计算在GPU上的应用。</li><li>计算机图形学中的学术研究使该领域更加数学化，对离散微分几何、蒙特卡洛路径追踪、流体模拟等实际解决方案产生了影响。</li><li>作者提到自己参与了Halide的开发，这是一种用于高性能图像处理的编程语言和编译器，获得了工业界的成功采用。</li><li>学术界和工业界都对计算机图形学做出了重要贡献，而工业界通常在学术界广泛接受之前引领了具有范式变革意义的创新。</li><li>作者强调学术界存在盲点，并指出计算机图形学直到90年代才得到广泛认可。</li><li>从历史的角度得出的教训包括致力于不同的研究方向，学习非主流的技能和技术，探索现实世界的问题，质疑假设，专注于长期目标，开源和合作等。</li><li>工业界对学术思想进行改进是自然的顺序，学术界应专注于理论、理解和战略优势的发展。</li><li>强调小型、灵活的团队、系统思维以及在计算机图形学领域保持乐趣的重要性。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://drive.google.com/file/d/1bsUrkwkaE2ZmGH8yBKRjDiqqpbvrnSLr/view&quot;&gt;Can academia compete with the resources of industry? </summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="Research" scheme="https://blog.aicoco.net/tags/Research/"/>
    
  </entry>
  
  <entry>
    <title>Open LLM Leaderboard排名之谜</title>
    <link href="https://blog.aicoco.net/2023/06/27/What_s_going_on_with_the_Open_LLM_Leaderboard/"/>
    <id>https://blog.aicoco.net/2023/06/27/What_s_going_on_with_the_Open_LLM_Leaderboard/</id>
    <published>2023-06-27T02:15:00.000Z</published>
    <updated>2023-06-28T12:40:10.674Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard">What’s going on with the Open LLM Leaderboard?</a></p><p>本文讨论了Open LLM Leaderboard上MMLU评估的差异问题。不同的评估实现会给出不同的结果，并且可能改变模型在Leaderboard上的排名。作者强调了评估与实现细节密切相关，开放、标准化和可复现的基准测试对于改进LLM非常重要。介绍了三种不同的MMLU评估实现，即Harness实现、HELM实现和Original实现，并比较了它们的结果。最后，提到将更新EleutherAI Eval Harness，并更新完整的Leaderboard。</p><p>要点：  </p><ul><li>Twitter上发布了Falcon，并加入了Open LLM Leaderboard(开放排行榜)，引发了有趣的讨论。</li><li>讨论的焦点是排行榜上的四个评估之一：用于衡量”Massive Multitask Language Understanding(MMLU)”的基准。</li><li>当前排行榜上排名第一的LLaMA模型的MMLU评估数据明显低于LLaMa论文中的数据，这让社区感到惊讶。</li><li>为了弄清楚情况并解决问题，运行了三种不同的MMLU评估实现，并对模型进行了排名。</li><li>不同实现方式给出的评估结果差异很大，甚至改变了模型在排行榜上的顺序。</li><li>MMLU是一个多项选择题测试，评估方式有多种，其中包括模型生成的概率和生成的文本与预期答案的比较。</li><li>在评估过程中，不同实现方式在提示语、模型输出预测等方面存在细微差别。</li><li>模型在同一数据集上的得分和排名非常敏感，不同评估方法得出的结果不可比较。</li><li>标准化和可复现的评估基准对于比较不同模型和研究成果至关重要。</li><li>Open LLM Leaderboard决定采用社区维护的评估库，并更新了MMLU评估的实现以保持一致性。</li><li>正在更新完整的排行榜，使用更新后的评估库进行评估。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://huggingface.co/blog/evaluating-mmlu-leaderboard&quot;&gt;What’s going on with the Open LLM Leaderboard?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文讨论了Open </summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="Leaderboard" scheme="https://blog.aicoco.net/tags/Leaderboard/"/>
    
  </entry>
  
  <entry>
    <title>关于大型语言模型评估的思考</title>
    <link href="https://blog.aicoco.net/2023/06/27/The_Curious_Case_of_LLM_Evaluations/"/>
    <id>https://blog.aicoco.net/2023/06/27/The_Curious_Case_of_LLM_Evaluations/</id>
    <published>2023-06-27T02:00:41.000Z</published>
    <updated>2023-06-28T12:41:10.246Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://nlpurr.github.io/posts/case-of-llm-evals.html">The Curious Case of LLM Evaluations</a></p><p>本文讨论了在评估大型语言模型(LLM)时面临的一些复杂问题。评估和基准测试一直是具有挑战性的，而大型多用途模型的出现进一步增加了复杂性。数据泄漏、覆盖范围限制、虚假相关性和分区问题等问题困扰着我们的评估实践。此外，精度与召回的权衡、缺乏真实标准进一步增加了困扰。本文探讨了机器学习评估中的常见问题，并深入研究了LLM所面临的具体挑战。将评估媒介分为直接度量、辅助模型评估和模型驱动评估，并阐明了每种方法的细微差别。在审慎的前提下，探索了如何报告性能指标，并强调了细粒度的重要性。同时，对提示微调进行了审视，并提醒要考虑用户交互的现实性。在深入了解评估领域的同时，明白了对这些复杂性的全面理解对于有意义地评估LLMs至关重要。</p><p>要点：  </p><ul><li>我们的建模、扩展和泛化技术发展得比我们的基准能力快，导致评估不佳和被夸大的能力。  </li><li>评估和基准已经变得复杂，尤其是对于大型生成模型和长文本生成。  </li><li>评估中的常见问题包括数据泄漏、覆盖范围不足、虚假相关性、数据集分割和表述问题。  </li><li>神经网络输出通常与随机种子略有关联，单次推断结果可能会导致错误的结果。  </li><li>对于不同任务，准确率和召回率的权衡并不相同，需要根据任务的特点进行评估。  </li><li>在机器学习中存在许多关于数据选择和保留的决策，对于复制和验证研究结果以及论文审查和讨论都很重要。  </li><li>语言模型评估可以分为六个组成部分：评估数据集、模型输出、样本&#x2F;输出转换、评估媒介、性能报告和参考材料。  </li><li>评估数据集的构建和使用会引入一系列问题，如模糊性、数据泄漏、覆盖范围和偏见等。  </li><li>模型输出的评估受到提示和回答方式的影响，需要注意输出的可靠性和一致性。  </li><li>对模型输出进行转换的方法包括循环变换、链式变换、原子输出和约束输出等。  </li><li>在评估中使用的基准真实性需要谨慎处理，特别是在考虑到现实场景的情况下。  </li><li>评估媒介可以分为直接评估指标、间接或分解式模型评估和模型驱动评估。  </li><li>在报告性能指标时，需要考虑数据集划分和细微变化的影响，并保持适度的怀疑态度。  </li><li>需要对评估结果的可靠性和统计显著性进行正确解读，避免基于细微差异和单次推断结果宣称显著改进。  </li><li>对于直接面向用户的端到端模型，需要考虑最佳提示是否适用于所有用户，并理解评估中存在的复杂性。  </li><li>综合理解这些复杂性对于有意义地评估语言模型至关重要。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://nlpurr.github.io/posts/case-of-llm-evals.html&quot;&gt;The Curious Case of LLM Evaluations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文讨论了在评估大型语言模型(LLM)时面临的</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="LLM" scheme="https://blog.aicoco.net/tags/LLM/"/>
    
    <category term="ChatGPT" scheme="https://blog.aicoco.net/tags/ChatGPT/"/>
    
  </entry>
  
  <entry>
    <title>脚本(Script)与程序(Program)</title>
    <link href="https://blog.aicoco.net/2023/06/26/script-vs-program/"/>
    <id>https://blog.aicoco.net/2023/06/26/script-vs-program/</id>
    <published>2023-06-26T12:34:04.000Z</published>
    <updated>2023-06-28T12:42:11.416Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://bigthink.com/neuropsych/intelligent-people-slower-solve-hard-problems/">Failing to draw lines between ‘script’ and ‘program’</a></p><p>近期有关于“脚本”与“程序”之间界限的讨论，但人们对于这两者之间的区别并没有明确的界定。有人认为编译语言中的东西总是“程序”，大型或复杂的东西也总是“程序”，即使是解释型语言中的单文件。然而，脚本可能更多地依赖于外部Unix程序而非内部组件。对于扩展其他程序的代码，也存在对于它们是“脚本”还是“程序”的争议。总的来说，计算机术语的界定是一个有趣但也复杂的问题。</p><p>要点：</p><ul><li>在Fediverse上，有人进行了有关“脚本”与“程序”之间界限的调查。</li><li>有人认为，编译语言中的东西总是“程序”，而解释语言中的一个单文件的大型或复杂的东西也总是“程序”。</li><li>“脚本”可能更多地依赖于外部的Unix程序，而不是内部的东西。</li><li>对于个人而言，即使是小型的Python代码，也会被称为“程序”，而不是“脚本”。</li><li>对于某些功能强大的Unix实用程序，人们对其是否为“程序”还是“脚本”存在争议。</li><li>有些事物被认为是“脚本”，而不是“程序”，即使它们包含了一些相对复杂的逻辑。</li><li>有关扩展其他程序的代码的编写也是一个辩论的重要领域。</li><li>总的来说，计算机术语是有趣的，有一种特定的乐趣。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://bigthink.com/neuropsych/intelligent-people-slower-solve-hard-problems/&quot;&gt;Failing to draw lines between ‘script’ and ‘p</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="programming" scheme="https://blog.aicoco.net/tags/programming/"/>
    
  </entry>
  
  <entry>
    <title>AI工程师入门指南</title>
    <link href="https://blog.aicoco.net/2023/06/26/How-to-Break-into-AI-Engineering/"/>
    <id>https://blog.aicoco.net/2023/06/26/How-to-Break-into-AI-Engineering/</id>
    <published>2023-06-26T09:18:01.000Z</published>
    <updated>2023-06-28T12:42:45.899Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://news.ycombinator.com/item?id=36432598">Ask HN: How to Break into AI Engineering</a></p><p>文章提到了学习AI工程所需的技能和知识的一些资源，包括数学基础、统计学、Python编程、IBM数据科学专业证书、机器学习和深度学习专业课程等。还讨论了数学在软件工程师中的重要性以及AI工程的发展趋势。</p><p>要点：</p><ul><li>要成为AI工程师，需要有扎实的数学基础，尤其是微积分和线性代数。</li><li>需要掌握统计学的语言和基本概念。</li><li>学习Python编程语言，掌握PyTorch。</li><li>推荐学习资源包括IBM Data Science Professional Certificate、Oliver Theobald的《Machine Learning for Absolute Beginners》、Andrew Ng的Coursera课程（包括《Machine Learning Specialization》和《Deep Learning Specialization》）以及fast.ai课程。</li><li>在选择AI领域的方向时，可以考虑自然语言处理（NLProc）、视觉处理（Vision）、强化学习（RL）等。</li><li>如果想在大型科技公司找工作，需要准备Leetcode面试、学习系统设计和机器学习系统的设计，还可以参考Chip Huyen的相关书籍。</li><li>数学在软件工程师中可能变得更加重要，因为AI在软件开发生命周期中的应用越来越广泛。</li><li>AI工程师的角色可以分为专注于模型改进、优化模型性能以及将模型应用于大型应用程序的不同方面。</li><li>学习数据清洗、数据标注和训练算法的基本过程非常重要，这些内容常常被忽视和低估。</li><li>AI工程师可以选择从数据工程入门，然后再专注于AI领域的特定方向。</li><li>AI工程师不仅需要技术知识，还需要与科学、数学等领域的知识相结合，可以结合自己的专业背景和兴趣选择方向。</li><li>AI工程师的角色可以是数据工程师的一种特殊化，不一定需要成为数学专家，但需要扎实的工程和数据处理能力。</li><li>AI工程师可以选择从事机器学习工程、数据工程和运维方面的工作，也可以选择研究工作，但后者通常需要更高的学历和数学基础。</li><li>了解数据工程的发展历程可以对AI工程师的角色有所启示，从中可以看到AI工程师可能在软件工程和AI领域之间进行工作。</li><li>AI工程师需要不断学习和尝试，掌握最新的技术和工具，并将其应用于实际项目中。</li><li>可以借助ChatGPT等工具来获取学习和编码的帮助，但仍需要自己进行实践和验证。</li><li>AI工程师的发展可能因不同的公司和行业而异，需要根据实际情况进行选择和发展。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://news.ycombinator.com/item?id=36432598&quot;&gt;Ask HN: How to Break into AI Engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章提到了学习AI工程所需的技能和知识的一些资源，包</summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="AI" scheme="https://blog.aicoco.net/tags/AI/"/>
    
    <category term="Engineering" scheme="https://blog.aicoco.net/tags/Engineering/"/>
    
    <category term="Learning" scheme="https://blog.aicoco.net/tags/Learning/"/>
    
  </entry>
  
  <entry>
    <title>聪明人解决难题可能更慢</title>
    <link href="https://blog.aicoco.net/2023/06/26/Intelligent-people-take-longer-to-solve-hard-problems/"/>
    <id>https://blog.aicoco.net/2023/06/26/Intelligent-people-take-longer-to-solve-hard-problems/</id>
    <published>2023-06-26T08:06:17.000Z</published>
    <updated>2023-06-28T12:43:39.266Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://bigthink.com/neuropsych/intelligent-people-slower-solve-hard-problems/">Intelligent people take longer to solve hard problems</a></p><p>高智力者解决难题更慢？新研究揭示速度并非决定因素。智力与大脑连接和同步有关。速度和准确性之间存在权衡。</p><p>心理学家仍在努力定义智力，对智力测试的有效性存在质疑。高智力得分通常与更快的信息处理速度相关，但德国研究显示这可能不完全正确。研究表明，高智力得分者在解决复杂问题时需要更长时间，因为他们更不容易草率下结论。大脑连接和同步差异与问题解决能力相关。结果挑战了高智力与大脑更快有关的假设，速度和准确性之间存在权衡，慢而费力、更长时间整合信息的认知方式可能更适用于解决更困难的问题。</p><p>要点：</p><ul><li>心理学家仍在努力定义智力，对于测试智力的有效性也存在质疑。</li><li>高智力得分与更快的信息处理或“思维速度”有关，但一项德国研究表明，这可能不是完全正确的。</li><li>研究发现，高智力得分的人在解决复杂问题时需要更长的时间，因为他们更不容易草率下结论。</li><li>研究还将解决问题的能力与大脑连接性和不同脑区之间的同步联系起来。</li><li>大脑扫描研究表明，智力涉及前额顶叶和顶叶之间的网络，因此这些脑区之间的更高同步可能反映了一种调节顶叶处理的前额注意机制。</li><li>研究结果挑战了高智力是大脑更快的结果的假设，表明速度并不一定更好，在某些情况下，速度和准确性之间存在权衡，从而导致更好的决策。</li><li>对于简单任务，快速的“自动”思维足够做出决策，而在解决更困难的问题时，一种更缓慢、更费力的认知方式可能更好，它支持对相关信息的长时间整合。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原文：&lt;a href=&quot;https://bigthink.com/neuropsych/intelligent-people-slower-solve-hard-problems/&quot;&gt;Intelligent people take longer to solve hard </summary>
      
    
    
    
    <category term="文摘" scheme="https://blog.aicoco.net/categories/%E6%96%87%E6%91%98/"/>
    
    
    <category term="IQ" scheme="https://blog.aicoco.net/tags/IQ/"/>
    
    <category term="Intelligence" scheme="https://blog.aicoco.net/tags/Intelligence/"/>
    
  </entry>
  
</feed>
